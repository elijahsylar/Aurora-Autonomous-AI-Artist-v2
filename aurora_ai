from llama_cpp import Llama
from pathlib import Path
import pygame
import os
os.environ['SDL_VIDEO_WINDOW_POS'] = '0,0'

from PIL import Image, ImageDraw, ImageFilter, ImageEnhance
import json
from datetime import datetime
from collections import deque
import time
import pyaudio
import requests
import re
import numpy as np
import math
import random
from scipy import signal  # For proper audio filtering
try:
    from aurora_ai_backup2 import DeepMemorySystem
    DEEP_MEMORY_AVAILABLE = True
    print(" Deep Memory System available!")
except ImportError:
    print(" Could not import DeepMemorySystem - will use simple memory only")
    DEEP_MEMORY_AVAILABLE = False
except Exception as e:
    print(f"Error: {e}")
    DEEP_MEMORY_AVAILABLE = False
    
try:
    from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration
    import torch
    LLAVA_AVAILABLE = True
    print(" LLaVA vision libraries available!")
except ImportError:
    print(" Could not import LLaVA - continuing without vision")
    LLAVA_AVAILABLE = False
      
class SimpleMemorySystem:
    """A simple memory system for Aurora to access and store Aurora's memories.

    Provides basic memory file management, drawing and code history, and canvas-specific storage.
    """
    def __init__(self, memory_path="./aurora_memory"):
        self.memory_path = Path(memory_path)
        self.memory_path.mkdir(exist_ok=True)  # Ensure base directory exists
        
        # Remove the maxlen limit to allow unlimited memories
        self.drawing_history = deque()  # No maxlen!
        self.code_history = deque()     # No maxlen!
        
        # DEBUG: Check what's actually happening
        print(f"DEBUG: Looking for memories in: {self.memory_path.absolute()}")
        print(f"DEBUG: Path exists? {self.memory_path.exists()}")
        print(f"DEBUG: Is directory? {self.memory_path.is_dir()}")
        
        # List all files to debug
        if self.memory_path.exists():
            all_files = list(self.memory_path.iterdir())
            print(f"DEBUG: Total files in directory: {len(all_files)}")
            json_files = [f for f in all_files if f.suffix == '.json']
            print(f"DEBUG: JSON files found: {len(json_files)}")
            
        # List available memory files for Aurora - LOAD ALL JSON FILES!
        self.available_memories = {}
        if self.memory_path.exists():
            print("Aurora's memory files:")
            for file in self.memory_path.glob("*.json"):
                # Load ALL JSON files, not just small ones
                self.available_memories[file.name] = file
                file_size = file.stat().st_size
                if file_size < 1024:
                    size_str = f"{file_size} bytes"
                else:
                    size_str = f"{file_size/1024:.1f} KB"
                print(f"  - {file.name} ({size_str})")
        
        # Canvas-specific storage (separate from general memories)
        self.canvas_path = self.memory_path / "canvas"
        self.canvas_path.mkdir(exist_ok=True)
        
    def read_memory(self, filename):
        """Aurora can read Aurora's memory files"""
        if filename in self.available_memories:
            try:
                with open(self.available_memories[filename], 'r') as f:
                    return json.load(f)
            except Exception:
                return None
        return None
    
    def get_memory_summary(self):
        """Get a quick summary of available memories"""
        summary = []
        
        # Try to read key identity files
        if "user_identity.json" in self.available_memories:
            data = self.read_memory("user_identity.json")
            if data and "name" in data:
                summary.append(f"I know {data['name']}")
                
        if "autonomous_creative_vision.json" in self.available_memories:
            data = self.read_memory("autonomous_creative_vision.json")
            if data and "current_vision" in data:
                vision = data["current_vision"][:50]
                summary.append(f"My vision: {vision}...")
                
        return " | ".join(summary) if summary else "Memories available"
    
    def load_memories(self):
        """Load canvas-specific memories"""
        # Load canvas code history
        canvas_code = self.canvas_path / "canvas_code_history.json"
        if canvas_code.exists():
            try:
                with open(canvas_code, 'r') as f:
                    data = json.load(f)
                    # Load ALL memories, not just last 1000
                    self.code_history = deque(data)  # Remove the [-1000:] slice
                    print(f"Loaded {len(self.code_history)} code memories")
            except json.JSONDecodeError as e:
                print(f"Error parsing code history JSON: {e}")
                self.code_history = deque(maxlen=1000)  # Start fresh
            except Exception as e:
                print(f"Error loading code history: {e}")
                self.code_history = deque(maxlen=1000)  # Start fresh
        
        # Skip loading any reinforcement learning data
        stats_file = self.canvas_path / "learning_stats.json"
        if stats_file.exists():
            print("(Skipping old reinforcement learning data)")
        # Load dream memories
        dreams_file = self.canvas_path / "dream_memories.json"
        if dreams_file.exists():
            try:
                with open(dreams_file, 'r') as f:
                    dream_data = json.load(f)
                    # We'll load these into Aurora during initialization
                    self.loaded_dreams = dream_data
                    print(f"Found {len(dream_data)} dream memories")
            except Exception as e:
                print(f"Error loading dream memories: {e}")
                self.loaded_dreams = []
        else:
            self.loaded_dreams = []
            
    def save_memories(self):
        """Save canvas-specific data"""
        # Prevent rapid saves
        if not hasattr(self, 'last_save_time'):
            self.last_save_time = 0
        
        current_time = time.time()
        if current_time - self.last_save_time < 30:  # Changed from 5 to 30 seconds
            return
        self.last_save_time = current_time
        
        # Save code history
        try:
            with open(self.canvas_path / "canvas_code_history.json", 'w') as f:
                json.dump(list(self.code_history), f)
        except Exception as e:
            print(f"Error saving code history: {e}")

        # Save dream memories
        dreams_file = self.canvas_path / "dream_memories.json"
        try:
            if hasattr(self, 'parent') and hasattr(self.parent, 'dream_memories'):
                dream_data = list(self.parent.dream_memories)
                # Only save if there are new dreams
                if len(dream_data) > 0 and len(dream_data) <= 1000:  # Cap at 1000
                    with open(dreams_file, 'w') as f:
                        json.dump(dream_data, f)
                    # Only print if it's a significant save
                    if len(dream_data) % 50 == 0:  # Every 50 dreams
                        print(f"Saved {len(dream_data)} dream memories")
        except Exception as e:
            print(f"Error saving dream memories: {e}")

    def remember_code(self, code, context):
        """
        Remember canvas drawing code.

        Args:
            code (str): The drawing code or command sequence.
            context (dict): Dictionary containing metadata about the drawing action.
                Required keys:
                    - 'emotion': Current emotion during drawing.
                    - 'x': X position on canvas.
                    - 'y': Y position on canvas.
                    - 'color': Color name used.
                    - 'pen_down': Boolean, whether pen is down.
                Optional keys:
                    - 'pixels_drawn': Number of pixels drawn.
                    - 'draw_mode': Drawing tool/mode used.
        """
        self.code_history.append({
            "code": code,
            "context": context,
            "timestamp": datetime.now().isoformat(timespec='seconds')
        })


    
class AuroraCodeMindComplete:
    """
    AuroraCodeMindComplete
    A comprehensive AI-powered drawing and creativity system for real-time, autonomous art generation. 
    AuroraCodeMindComplete manages a large pixel canvas, emotional state, memory, sound, and interactive controls, 
    using a local LLM for code-based motor control and creative decision-making.
    Features:
    - GPU-accelerated LLM for fast code generation and decision making.
    - Dynamic canvas scaling and multi-resolution vision (normal, density, shape, compressed views).
    - Rich color palette and drawing tools (pen, brush, spray, stamps, etc.).
    - Emotional state tracking and influence system, including deep emotions and memory-based shifts.
    - Memory system for code patterns, dreams, and artistic inspirations.
    - Check-in system for periodic breaks (chat, dream).
    - Sound system with pitch control and instant musical feedback.
    - Fullscreen Tkinter GUI with live status panels and controls.
    - Autonomous loop with emotion-driven speed, creativity boosters, and periodic snapshots.
    - Dream generation and retention based on actual drawing experiences.
    - Support for external deep memory systems.
    Usage:
    - Instantiate with a model path and optional GPU settings.
    - Call `run()` to start the interactive drawing loop.
    - Use keyboard controls for snapshots, turbo mode, fullscreen, and quitting.
    Main Methods:
    - run(): Start the main loop and GUI.
    - create_loop(): Main autonomous loop for drawing, thinking, and emotional processing.
    - think_in_code(): Generate and execute movement/drawing codes via LLM.
    - update_display(): Refresh the canvas and status panels.
    - adjust_pixel_size(), adjust_speed(), feel(): Control canvas, speed, and emotion.
    - save_canvas_state(), load_canvas_state(), save_snapshot(): Persistence and artwork saving.
    - process_deep_emotions(), influence_emotion(): Emotional state management.
    - generate_dream(), process_dream_retention(): Dream cycle and memory retention.
    - setup_display(): Initialize GUI and controls.
    Keyboard Controls:
    - S: Save snapshot
    - T: Toggle turbo mode
    - ESC: Exit fullscreen
    - F11: Toggle fullscreen
    - Q: Quit
    - C/B: Center/reset view
    - H: Toggle hearing
    AuroraCodeMindComplete is designed for creative exploration, emotional intelligence, and autonomous art-making.
    """
    def __init__(self, model_path, use_gpu=True, gpu_layers=10):
        print("Initializing AuroraCodeMindComplete...")
        
        # Detect GPU and set layers
        if use_gpu:
            print(" GPU Mode Enabled!")
            gpu_layers_setting = 10  # Back to full power
        else:
            print(" CPU Mode")
            gpu_layers_setting = 0
        
        # LLM with GPU acceleration - NO LORA
        print("Loading base model...")
        self.llm = Llama(
            model_path,  # Your quantized model
            n_ctx=6500,
            n_gpu_layers=gpu_layers_setting,
            n_threads=8,
            n_batch=512,
            verbose=False,
            seed=42,
            f16_kv=True,
            use_mlock=True,
            n_threads_batch=8
        )
        print(f" LLM loaded with {gpu_layers_setting} GPU layers")
        
        print(f"2. LLM initialization complete")
        
        # Get screen dimensions for fullscreen
        # Don't init pygame here - we'll do it once in setup_display
        screen_width = 1920  # Default assumption
        screen_height = 1080  # Default assumption
        
        # Canvas - adjust size based on screen (much smaller pixels now!)
        self.scale_factor = 2.0  # Lower scale_factor means smaller pixels and higthe canvas resolution; e.g., 1.6 gives more pixels than 8.
        self.initial_scale_factor = self.scale_factor  # Store the starting scale factor
        self.canvas_size = min(int(screen_width / self.scale_factor) - 50, 
                               int(screen_height / self.scale_factor) - 50)
        
        # Supersampling for quality
        self.supersample_factor = 4  # Draw at 4x resolution internally
        self.internal_canvas_size = self.canvas_size * self.supersample_factor
                               
                       
        self.x = self.canvas_size // 2
        self.y = self.canvas_size // 2
        self.is_drawing = True
        self.steps_taken = 0
        # Initialize lifetime statistics (will load after memory system)
        self.lifetime_stats = {
            'total_pixels_drawn': 0,
            'total_steps': 0,
            'total_dreams': 0,
            'total_goals_completed': 0,
            'first_session': datetime.now().isoformat(),
            'sessions_count': 0
        }
        # Track what's been saved to prevent double-counting
        self.last_saved_pixels = 0
        self.last_saved_steps = 0
        print(f"3. Canvas settings done - Size: {self.canvas_size}x{self.canvas_size} ({self.scale_factor}x scale)")
        
        # Expanded color palette with full word codes
        self.palette = {
            'red': (255, 0, 0),
            'orange': (255, 150, 0),
            'yellow': (255, 255, 0),
            'green': (0, 255, 0),
            'cyan': (0, 255, 255),
            'blue': (0, 100, 255),
            'purple': (200, 0, 255),
            'pink': (255, 192, 203),
            'white': (255, 255, 255),
            'gray': (128, 128, 128),
            'eraser': (0, 0, 0),  # Transparent - removes color
            'black': (25, 25, 25),  # Near-black that's visible
            'brown': (139, 69, 19),
            'magenta': (255, 0, 255),
            'lime': (50, 205, 50),
            'navy': (0, 0, 128)
        }
        
        # Full word codes for colors - easy to remember!
        self.color_codes = {
            'red': 'red',       'orange': 'orange',    'yellow': 'yellow',
            'green': 'green',   'cyan': 'cyan',        'blue': 'blue',
            'purple': 'purple', 'pink': 'pink',        'white': 'white',
            'gray': 'gray',     'eraser': 'eraser',    'brown': 'brown',
            'black': 'black',   # This is now the visible near-black
            'magenta': 'magenta', 'lime': 'lime',      'navy': 'navy'
        }
        
        self.current_color = (255, 255, 255)
        self.current_color_name = 'white'
        # View modes
        self.view_mode = "normal"  # normal, density, shape
        # Drawing modes
        self.draw_mode = "brush"  # pen, brush, star, eraser
        self.pen_momentum = 0 
        # Color variety tracking
        self.color_history = deque(maxlen=20)  # Track last 20 color uses
        self.turn_colors_used = set()  # Track colors used in current turn
        self.last_turn_color = 'white'  # Track color from previous turn
        
        print("4. Colors and tools initialized")
        
    
        # Memory system
        # Dynamic learning system
        self.skill_proficiency = {
            'color_harmony': 0.0,
            'pattern_creation': 0.0,
            'tool_mastery': 0.0,
            'spatial_composition': 0.0,
            'emotional_expression': 0.0
        }
     
        self.learning_experiences = deque(maxlen=200)
        # Note: Will load learning experiences after memory system is initialized
        self.skill_challenges = deque(maxlen=10)
        # Note: Will load saved challenges after memory system is initialized
        self.mastery_thresholds = {
            'novice': 0.3,
            'intermediate': 0.6,
            'advanced': 0.8,
            'master': 0.95
        }
        self.memory = SimpleMemorySystem("./aurora_memory")
        self.memory.parent = self  # Add reference for saving dreams
        self.memory.load_memories()  # LOAD PREVIOUS MEMORIES!
        
        # Load previous autonomous goals if they exist
        goals_file = self.memory.memory_path / "aurora_goals.json"
        if goals_file.exists():
            try:
                with open(goals_file, 'r') as f:
                    saved_goals = json.load(f)
                    # Only load incomplete goals
                    self.autonomous_goals = deque([g for g in saved_goals if not g.get('completed')], maxlen=10)
                    if self.autonomous_goals:
                        print(f" Resuming {len(self.autonomous_goals)} incomplete goals from last session")
            except Exception as e:
                print(f"Could not load goals: {e}")
                
        # Load previous skill proficiencies if they exist
        skill_file = self.memory.memory_path / "aurora_skills.json"
        if skill_file.exists():
            try:
                with open(skill_file, 'r') as f:
                    saved_skills = json.load(f)
                    self.skill_proficiency.update(saved_skills)
                    print(f" Loaded Aurora's skill levels from previous sessions!")
                    for skill, level in self.skill_proficiency.items():
                        print(f"   {skill}: {level:.2f}")
            except Exception as e:
                print(f"Could not load skills: {e}")
        # Autonomous preference development system
        self.artistic_preferences = {
            'favorite_colors': {},  # color: pleasure_score
            'favorite_patterns': {},  # pattern_hash: (pattern, pleasure_score)
            'favorite_tools': {},  # tool: pleasure_score
            'color_combinations': {},  # "color1-color2": pleasure_score
            'movement_styles': {},  # movement_pattern: pleasure_score
            'discovered_techniques': [],  # List of self-discovered techniques
            'aesthetic_values': {
                'density_preference': 0.5,  # 0=sparse, 1=dense
                'symmetry_preference': 0.5,  # 0=chaotic, 1=symmetric
                'color_harmony_preference': 0.5,  # 0=contrasting, 1=harmonious
                'size_preference': 0.5,  # 0=small details, 1=large strokes
            }
        }
        
        # Load previous preferences if they exist
        pref_file = self.memory.memory_path / "aurora_preferences.json"
        if pref_file.exists():
            try:
                with open(pref_file, 'r') as f:
                    saved_prefs = json.load(f)
                    self.artistic_preferences.update(saved_prefs)
                    
                    # Load color theory learning if it exists
                    if 'color_theory_learning' in saved_prefs:
                        if not hasattr(self, 'color_theory_discoveries'):
                            self.color_theory_discoveries = {
                                'monochrome_experiences': [],
                                'balanced_experiences': [],
                                'dominant_experiences': [],
                                'learned_preferences': saved_prefs['color_theory_learning']['learned_preferences']
                            }
                        else:
                            self.color_theory_discoveries['learned_preferences'].update(
                                saved_prefs['color_theory_learning']['learned_preferences']
                            )
                        print(f"   Loaded color theory knowledge from {saved_prefs['color_theory_learning']['total_experiences']} past experiences")
                        
                    print(f" Loaded Aurora's personal preferences!")
                    
                    if 'discovered_techniques' in saved_prefs and saved_prefs['discovered_techniques']:
                        print(f" Aurora knows {len(saved_prefs['discovered_techniques'])} unique techniques!")
                        recent = saved_prefs['discovered_techniques'][-1]
                        print(f"   Most recent: {recent.get('pattern', '')[:20]}... (satisfaction: {recent.get('satisfaction', 0):.2f})")
            except Exception as e:
                print(f"Could not load preferences: {e}")
        
        # Internal satisfaction tracking
        self.current_satisfaction = 0.0
        self.technique_discovery_threshold = 0.8
        
        # Load dreams from the dream_logs folder
        dream_logs_path = Path("./dream_logs")
        self.dream_memories = deque(maxlen=1000)
        
        if dream_logs_path.exists():
            dream_files = sorted(dream_logs_path.glob("*.log"))
            print(f"Found {len(dream_files)} dream log files!")
            
            # Load recent dream files
            for dream_file in dream_files[-50:]:  # Load last 50 files
                try:
                    with open(dream_file, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()
                        # Extract dreams - they might be formatted differently
                        # Try multiple patterns
                        if "Dream:" in content or "dream" in content.lower():
                            dreams = content.split("Dream:")
                            for dream in dreams[1:]:  # Skip first empty split
                                dream_text = dream.split("\n")[0].strip()
                                if dream_text:
                                    self.dream_memories.append({
                                        "content": dream_text[:200],  # First 200 chars
                                        "source": dream_file.name,
                                        "type": "historical"
                                    })
                except Exception as e:
                    pass  # Skip problematic files
            
            print(f" Loaded {len(self.dream_memories)} dreams from logs!")
        
        # Also try loading conversation logs
        conv_logs_path = Path("./conversation_logs")
        if conv_logs_path.exists():
            conv_files = list(conv_logs_path.glob("*.log"))
            print(f" Found {len(conv_files)} conversation logs available")
            
        # Load emotional memory from previous sessions
        emotion_file = self.memory.memory_path / "aurora_emotions.json"
        if emotion_file.exists():
            try:
                with open(emotion_file, 'r') as f:
                    saved_emotions = json.load(f)
                    self.emotion_memory = deque(saved_emotions[-50:], maxlen=50)  # Keep last 50
                    if self.emotion_memory:
                        last_emotion = self.emotion_memory[-1]
                        # Start with the last recorded emotion
                        self.current_emotion = last_emotion.get('emotion', 'curious')
                        self.emotion_category = last_emotion.get('category', 'curiosity')
                        print(f" Aurora remembers feeling {self.current_emotion} last time")
            except Exception as e:
                print(f"Could not load emotions: {e}")
                
        # Load significant learning experiences
        learning_file = self.memory.memory_path / "aurora_learning.json"
        if learning_file.exists():
            try:
                with open(learning_file, 'r') as f:
                    saved_learning = json.load(f)
                    # Keep the most impactful experiences
                    significant = [exp for exp in saved_learning if exp.get('satisfaction', 0) > 0.5]
                    self.learning_experiences = deque(significant[-100:], maxlen=200)
                    print(f" Loaded {len(self.learning_experiences)} significant learning experiences")
            except Exception as e:
                print(f"Could not load learning: {e}")
                
        # Load lifetime statistics
        lifetime_file = self.memory.memory_path / "aurora_lifetime.json"
        if lifetime_file.exists():
            try:
                with open(lifetime_file, 'r') as f:
                    saved_lifetime = json.load(f)
                    self.lifetime_stats.update(saved_lifetime)
                    self.lifetime_stats['sessions_count'] += 1
                    print(f" Aurora's lifetime: Session #{self.lifetime_stats['sessions_count']}")
                    print(f"   Total pixels ever drawn: {self.lifetime_stats['total_pixels_drawn']:,}")
                    print(f"   Total steps taken: {self.lifetime_stats['total_steps']:,}")
            except Exception as e:
                print(f"Could not load lifetime stats: {e}")
        else:
            self.lifetime_stats['sessions_count'] = 1  
            
        # Load active skill challenges
        challenges_file = self.memory.memory_path / "aurora_challenges.json"
        if challenges_file.exists():
            try:
                with open(challenges_file, 'r') as f:
                    saved_challenges = json.load(f)
                    self.skill_challenges = deque(saved_challenges, maxlen=10)
                    if self.skill_challenges:
                        print(f" Resuming {len(self.skill_challenges)} skill challenges")
            except Exception as e:
                print(f"Could not load challenges: {e}")
                
                     
        print("5. Memory system created and loaded")
        # Debug: Show what memories Aurora has access to
        print("\n AURORA'S MEMORY BANK:")
        print(f"  Code memories: {len(self.memory.code_history)}")
        print(f"  Dream memories: {len(self.dream_memories)}")
        print(f"  Available memory files: {list(self.memory.available_memories.keys())}")
        if "user_identity.json" in self.memory.available_memories:
            identity = self.memory.read_memory("user_identity.json")
            if identity:
                print(f"  User identity loaded: {identity.get('name', 'Unknown')}")
        print("")
        # Connect to Big Aurora's deep memory

        if DEEP_MEMORY_AVAILABLE:
            try:
                self.big_memory = DeepMemorySystem()
                print(" Connected to Big Aurora's deep memories!")
                
                # Just mark it as available - we'll figure out the API when we use it
                self.big_memory_available = True
                    
            except Exception as e:
                print(f" Could not connect to Big Aurora's memory: {e}")
                self.big_memory = None
                self.big_memory_available = False
        else:
            self.big_memory = None
            self.big_memory_available = False
        
        # Elijah's artwork study system
        self.elijahs_art_path = Path("Elijahs_Art")
        self.artwork_inspirations = deque(maxlen=100)
        self.artworks_studied_count = 0
        
        # Check if Elijah's art folder exists
        if self.elijahs_art_path.exists():
            artwork_files = list(self.elijahs_art_path.glob("*.jpg")) + \
                           list(self.elijahs_art_path.glob("*.png")) + \
                           list(self.elijahs_art_path.glob("*.jpeg"))
            print(f" Found Elijah's Art folder with {len(artwork_files)} paintings!")
            if artwork_files:
                print(f"   Aurora can study your artwork: {', '.join([f.name[:20] for f in artwork_files[:3]])}...")
        else:
            print(f" Creating 'Elijahs_Art' folder - please add your paintings!")
            self.elijahs_art_path.mkdir(exist_ok=True)
              
        # Emotional state
        self.current_emotion = "curious"
        self.emotion_words = ["curious", "playful", "contemplative", "energetic", "peaceful", "creative"]
        print("6. Emotions initialized")
        # DEEPER EMOTION SYSTEM
        # Expanded emotion vocabulary with intensity levels
        self.deep_emotions = {
            # Primary emotions with their intensity variations
            "joy": ["content", "happy", "joyful", "elated", "euphoric"],
            "curiosity": ["interested", "curious", "fascinated", "absorbed", "obsessed"],
            "peace": ["calm", "peaceful", "serene", "tranquil", "zen"],
            "energy": ["active", "energetic", "excited", "exhilarated", "electric"],
            "contemplation": ["thoughtful", "contemplative", "reflective", "philosophical", "profound"],
            "creativity": ["inspired", "creative", "imaginative", "visionary", "transcendent"],
            "melancholy": ["wistful", "nostalgic", "melancholic", "longing", "bittersweet"],
            "wonder": ["amazed", "wondering", "astonished", "awestruck", "mystified"]
        }
        
        # Emotion influences from different sources
        self.emotion_influences = {
            "dreams": 0.0,      # -1 to 1, how dreams affected mood
            "chat": 0.0,        # -1 to 1, how chats affected mood  
            "music": 0.0,       # -1 to 1, how sounds affected mood
            "artwork": 0.0,     # -1 to 1, how viewing art affected mood
            "creating": 0.0     # -1 to 1, how creating affected mood
        }
        
        # Current emotion depth (0-4, maps to intensity in deep_emotions)
        self.emotion_depth = 2  # Start at middle intensity
        self.emotion_category = "curiosity"  # Start curious
        
        # Emotion memory - tracks recent emotional experiences
        self.emotion_memory = deque(maxlen=50)
        
        self.emotion_shift_cooldown = 0  # Prevents too-rapid emotion changes
        
        print("6b. Deep emotion system initialized")
        # Positive reinforcement tracking
        self.recent_color_changes = deque(maxlen=3)
        self.positive_moments = deque(maxlen=100)
        self.recent_encouragement = ""  # Aurora will see this
        # Shape variety tracking
        self.recent_shapes_drawn = deque(maxlen=20)  # Track last 20 shapes
        self.shape_cooldowns = {
            'square': 0,
            'circle': 0,
            'diagonal': 0,
            'triangle': 0,
            'spiral': 0
        }
        self.position_history = deque(maxlen=50)
        self.last_positions = deque(maxlen=10)
        self.quadrants_visited = set()
        self.just_viewed_canvas = False
        # Code tracking
        self.last_code = ""
        self.continuous_draws = 0
        self.last_think_time = 0  # Performance tracking
        self.skip_count = 0  # Track thinking pauses
        self.aurora_speed = "normal"  # Aurora's chosen speed
        self.aurora_delay = 300  # Current delay in ms
        self.recent_speed_override = False  # Track if Aurora recently chose speed
        self.speed_override_counter = 0  # Steps since speed override
        print("7. Code tracking initialized")
        # Autonomous goal generation system
        self.autonomous_goals = deque(maxlen=10)
        self.personal_artistic_desires = deque(maxlen=20)
        self.self_generated_challenges = deque(maxlen=5)
        self.goal_generation_cooldown = 0
        self.last_goal_time = time.time()
        # Chat system
        self.chat_mode = False
        self.chat_history = deque(maxlen=50)
        self.last_chat_time = time.time()
        print("7b. Autonomous goal system initialized")

        
        # Canvas - now at higher resolution internally
        self.pixels = Image.new('RGBA', (self.internal_canvas_size, self.internal_canvas_size), (0, 0, 0))
        self.draw_img = ImageDraw.Draw(self.pixels)
        print(f"8. Image buffer created at {self.internal_canvas_size}x{self.internal_canvas_size} (4x supersampled)")

        # Track when each pixel was painted (for wet/dry detection)
        self.paint_timestamps = {}  # {(x,y): timestamp}
        self.paint_wetness_duration = 30.0  # Paint stays wet for 30 seconds
        self.paint_opacity = 0.95  # Acrylic is very opaque
        
        # Paint mixing parameters
        self.wet_blend_ratio = 0.7  # CHANGED from 0.4 - Much more mixing when wet!
        self.dry_blend_ratio = 0.05  # Almost no mixing when dry
        
        print("8b. Paint system initialized (wet/dry mixing)")
        # Try to load previous canvas state (this may adjust position)
        # self.load_canvas_state()
        
        # Ensure position is valid for current canvas
        self.x = max(0, min(self.x, self.canvas_size - 1))
        self.y = max(0, min(self.y, self.canvas_size - 1))
        
        # Performance settings
        self.turbo_mode = False
        self.use_gpu = use_gpu
        # Immediate feedback learning mode - start with it ON
        self.immediate_feedback_mode = True
        print(" IMMEDIATE FEEDBACK MODE: ON - Aurora sees each action's result!")
        self.moondream_instant_feedback = True  # Moondream comments on every action in feedback mode
        print(" MOONDREAM INSTANT FEEDBACK: ON - Moondream observes each action!")
        # Check-in system initialization
        self.last_checkin_time = time.time()
        self.current_mode = "drawing"  # drawing, chat, rest
        self.mode_start_time = time.time()
        self.checkin_interval = 30 * 60  # 30 minutes in seconds
        self.break_duration = 10 * 60    # 10 minutes in seconds
        self.awaiting_checkin_response = False
        self.chat_message_count = 0 


        # Dream system initialization
        self.current_dreams = []  # Dreams from current rest session
        self.sleep_phase = "light"  # light, rem, waking
        self.sleep_phase_start = time.time()
        self.dream_count = 0
        # Audio hearing system
        self.hearing_enabled = False
        self.audio_stream = None
        self.audio = pyaudio.PyAudio()
        self.rest_duration = 20 * 60  # 20 minutes for rest/dreaming (separate from break_duration)

        

        # High-quality pygame sound system - CD quality for crystal clear sounds
        # FORCE quit any existing mixer initialization
        try:
            pygame.mixer.quit()
        except:
            pass
        
        # Initialize mixer FIRST with correct settings (before pygame.init!)
        pygame.mixer.init(frequency=44100, size=-16, channels=2, buffer=2048)
        pygame.mixer.set_num_channels(1)  # ONLY 1 sound at a time - no overlapping!
        
        # NOW initialize the rest of pygame
        pygame.init()
        
        # Verify mixer settings
        mixer_info = pygame.mixer.get_init()
        print(f" Mixer settings: {mixer_info}")
        if mixer_info[0] != 44100:
            print(f" WARNING: Mixer frequency is {mixer_info[0]} instead of 44100!")
            print(" Trying to reinitialize...")
            pygame.mixer.quit()
            pygame.mixer.init(frequency=44100, size=-16, channels=2, buffer=2048)
            pygame.mixer.set_num_channels(1)
            print(f" New mixer settings: {pygame.mixer.get_init()}")
        
        # REAL SAMPLE-BASED LOFI SOUNDS - Each character = a specific moment in the song!
        self.sounds = {}
        self.current_sound_type = "lofi"
        self.sound_timestamps = {}  # Aurora can learn what each character plays!
        
        try:
            # Check if we have a reference lofi track to sample from
            # Try MANY possible locations
            possible_paths = [
                "lofi_reference.mp3",
                "good-night-lofi-cozy-chill-music-160166.mp3",
                "./lofi_reference.mp3",
                "./good-night-lofi-cozy-chill-music-160166.mp3",
                os.path.expanduser("~/lofi_reference.mp3"),
                os.path.expanduser("~/good-night-lofi-cozy-chill-music-160166.mp3"),
                os.path.join(os.getcwd(), "lofi_reference.mp3"),
                os.path.join(os.getcwd(), "good-night-lofi-cozy-chill-music-160166.mp3"),
            ]
            
            # Also check for ANY .mp3 file in the current directory
            try:
                for file in os.listdir('.'):
                    if file.endswith('.mp3'):
                        possible_paths.append(file)
                        possible_paths.append(os.path.join('.', file))
            except:
                pass
            
            lofi_path = None
            for path in possible_paths:
                if os.path.exists(path):
                    lofi_path = path
                    print(f" Found lofi track: {path}")
                    break
            
            if lofi_path:
                print(f" Loading REAL lofi audio from: {lofi_path}")
                
                # NEW APPROACH: Pre-render chunks as WAV files, then load with pygame
                # This bypasses pygame's broken make_sound() from arrays
                
                import wave
                import struct
                
                # Create cache directory for pre-rendered chunks
                cache_dir = Path("./aurora_sound_cache")
                cache_dir.mkdir(exist_ok=True)
                
                # Load the lofi track using pydub for reliable decoding
                try:
                    from pydub import AudioSegment
                    audio = AudioSegment.from_mp3(lofi_path)
                    # Convert to 44100Hz stereo
                    audio = audio.set_frame_rate(44100).set_channels(2)
                    samples = np.array(audio.get_array_of_samples(), dtype=np.int16)
                    lofi_audio = samples.reshape((-1, 2))
                    print(" Loaded via pydub")
                except ImportError:
                    print(" Warning: pydub not available, using pygame (may have issues)")
                    lofi_sound = pygame.mixer.Sound(lofi_path)
                    lofi_audio = pygame.sndarray.array(lofi_sound)
                
                track_duration = len(lofi_audio) / 44100
                print(f" Loaded {track_duration:.1f} seconds of lofi audio")
                
                sound_chars = '!@#$%^&*()[]<>=+~`-_,.|;:?/{}\\'
                sample_rate = 44100
                
                print(" Pre-rendering lofi chunks as WAV files...")
                
                chunk_duration = 0.3  # 300ms chunks
                chunk_samples = int(chunk_duration * sample_rate)
                total_chars = len(sound_chars)
                
                for i, char in enumerate(sound_chars):
                    # Spread characters evenly across the track
                    timestamp = (i / total_chars) * track_duration * 0.9
                    start_sample = int(timestamp * sample_rate)
                    end_sample = start_sample + chunk_samples
                    
                    # Bounds check
                    if end_sample > len(lofi_audio):
                        available = len(lofi_audio) - start_sample
                        if available > chunk_samples // 2:
                            end_sample = len(lofi_audio)
                        else:
                            start_sample = len(lofi_audio) - chunk_samples
                            end_sample = len(lofi_audio)
                    
                    # Extract chunk
                    audio_chunk = lofi_audio[start_sample:end_sample].copy()
                    
                    # Ensure stereo
                    if len(audio_chunk.shape) == 1:
                        audio_chunk = np.column_stack((audio_chunk, audio_chunk))
                    
                    # Store timestamp info
                    self.sound_timestamps[char] = {
                        'timestamp': timestamp,
                        'start_sample': start_sample,
                        'end_sample': end_sample,
                        'minutes': int(timestamp // 60),
                        'seconds': timestamp % 60
                    }
                    
                    # Save each chunk as a WAV file
                    for pitch_mode in ['normal', 'low', 'high']:
                        sound_key = f"{char}_{pitch_mode}_lofi"
                        
                        # Use a safe filename (some chars aren't filesystem-safe)
                        safe_char = str(ord(char))
                        wav_path = cache_dir / f"lofi_{safe_char}_{pitch_mode}.wav"
                        
                        # Only create if doesn't exist
                        if not wav_path.exists():
                            with wave.open(str(wav_path), 'wb') as wav_file:
                                wav_file.setnchannels(2)  # Stereo
                                wav_file.setsampwidth(2)  # 16-bit
                                wav_file.setframerate(44100)
                                # Convert to bytes
                                audio_bytes = audio_chunk.tobytes()
                                wav_file.writeframes(audio_bytes)
                        
                        # Now load the clean WAV file with pygame
                        self.sounds[sound_key] = pygame.mixer.Sound(str(wav_path))
                        self.sounds[sound_key].set_volume(0.75)
                
                print(f" âœ“ LOFI Sound Engine Ready (WAV-based)!")
                print(f"   Each character = a specific moment in the lofi track")
                print(f"   '{sound_chars[0]}' = {self.sound_timestamps[sound_chars[0]]['timestamp']:.2f}s")
                print(f"   '{sound_chars[-1]}' = {self.sound_timestamps[sound_chars[-1]]['timestamp']:.2f}s")
                print(f"   Total sounds: {len(self.sounds)}")
                print(f"   Cached in: {cache_dir}")
            
            else:
                # Fallback to Karplus-Strong if no lofi track found
                print(" No lofi reference track found - using Karplus-Strong synthesis")
                print(" (To use real samples, place lofi track at ./lofi_reference.mp3)")
                
                sound_chars = '!@#$%^&*()[]<>=+~`-_,.|;:?/{}\\'
                sample_rate = 44100
                
                base_notes = {
                    '!': 261.63, '@': 293.66, '#': 329.63, '$': 349.23,
                    '%': 392.00, '^': 440.00, '&': 493.88, '*': 523.25,
                    '(': 587.33, ')': 659.25, '[': 698.46, ']': 783.99,
                    '<': 880.00, '>': 987.77, '=': 1046.50, '+': 1174.66,
                    '~': 1318.51, '`': 1396.91, '-': 1567.98, '_': 1760.00,
                    ',': 196.00, '.': 220.00, '|': 246.94, ';': 130.81,
                    ':': 146.83, '?': 164.81, '/': 174.61, '{': 987.77,
                    '}': 1108.73, '\\': 1244.51
                }
                
                for char in sound_chars:
                    if char not in base_notes:
                        base_notes[char] = 440.00
                    
                    freq = base_notes[char]
                    
                    for pitch_mode, freq_mult in [('normal', 1.0), ('low', 0.5), ('high', 2.0)]:
                        adjusted_freq = freq * freq_mult
                        sound_array = self._generate_lofi_sounds(adjusted_freq, sample_rate)
                        
                        max_val = np.max(np.abs(sound_array))
                        if max_val > 0:
                            sound_array = sound_array / max_val
                        sound_array = np.clip(sound_array * 32767 * 0.8, -32767, 32767).astype(np.int16)
                        stereo = np.column_stack((sound_array, sound_array))
                        sound_key = f"{char}_{pitch_mode}_lofi"
                        self.sounds[sound_key] = pygame.sndarray.make_sound(stereo)
                        self.sounds[sound_key].set_volume(0.7)
                
                print(f" Karplus-Strong Sound Engine Ready")
                print(f" Total sounds: {len(self.sounds)}")
            
        except Exception as e:
            print(f" Sound system failed: {e}")
            import traceback
            traceback.print_exc()
            self.sounds = {}
            
        # Cymatics system
        self.cymatic_circles = []
        self.current_pitch = 'normal'  

        
        # Sound duration control 
        self.note_duration = 0.15  # Default duration
        self.min_duration = 0.05   # Minimum 50ms
        self.max_duration = 1.0    # Maximum 1 second
        
        print("8c. Initializing vision system...")
        self.vision_enabled = False
        
        # INITIALIZE CONVERSATION TRACKING FIRST - before vision loading
        self.vision_conversation_history = deque(maxlen=20)
        self.moondream_last_message = None
        self.last_vision_question = None
        self.moondream_literal_mode = True
        self.moondream_tracking = {
            'last_mark_position': None,
            'mark_count': 0,
            'current_shape': None,
            'connected_marks': []
        }
        self.conversation_turn = 0
        
        # Track canvas state for delta detection
        self.last_moondream_canvas_state = "Empty black canvas"
        
        if LLAVA_AVAILABLE:
            try:
                from transformers import AutoModelForCausalLM, AutoTokenizer
                
                # Moondream2 - tiny but effective!
                model_id = "vikhyatk/moondream2"
                
                # Try loading with local_files_only after first download
                try:
                    self.vision_model = AutoModelForCausalLM.from_pretrained(
                        model_id,
                        trust_remote_code=True,
                        local_files_only=True,  # Try offline first
                        device_map={"": "cuda"} if torch.cuda.is_available() else "cpu"
                    )
                    self.vision_tokenizer = AutoTokenizer.from_pretrained(
                        model_id,
                        local_files_only=True
                    )
                    print("   Moondream loaded from cache (offline mode)")
                except Exception:
                    # Fall back to online download
                    self.vision_model = AutoModelForCausalLM.from_pretrained(
                        model_id,
                        trust_remote_code=True,
                        device_map={"": "cuda"} if torch.cuda.is_available() else "cpu"
                    )
                    self.vision_tokenizer = AutoTokenizer.from_pretrained(
                        model_id
                    )
                    print("   Moondream downloaded and loaded")
                
                self.vision_enabled = True
                self.last_vision_time = 0
                self.vision_interval = 999999  # Can be faster on GPU!
                
            except Exception as e:
                print(f"   Could not load vision: {e}")
                self.vision_enabled = False
                
        # Setup display
        print("9. About to setup display...")
        self.setup_display()
        print("10. Display setup complete")
        
    
    def _generate_lofi_sounds(self, freq, sample_rate):
        """Generate REAL musical lofi sounds using Karplus-Strong physical modeling"""
        duration = 0.6
        samples = int(sample_rate * duration)
        t = np.linspace(0, duration, samples, False)
        
        # === KARPLUS-STRONG ALGORITHM ===
        # This creates realistic plucked string sounds (guitar/piano/etc)
        
        # Calculate delay line length for this frequency
        delay_samples = int(sample_rate / freq)
        
        # Initialize with noise burst (the "pluck")
        burst_length = min(delay_samples, int(sample_rate * 0.01))
        burst = np.random.randn(burst_length) * 0.8
        
        # Create output buffer
        output = np.zeros(samples)
        output[:burst_length] = burst
        
        # Karplus-Strong feedback loop - THIS IS THE MAGIC
        # It naturally creates harmonic, musical sounds
        damping = 0.996  # Controls how long the note rings out
        blend = 0.5  # How much to average (affects brightness)
        
        for i in range(burst_length, samples):
            if i >= delay_samples + 1:
                # Average current sample with delayed samples (low-pass filter)
                delayed = output[i - delay_samples]
                delayed_prev = output[i - delay_samples - 1]
                output[i] = damping * blend * (delayed + delayed_prev)
        
        # === MAKE IT LOFI ===
        
        # Add subtle harmonics for warmth
        harmonic_env = np.exp(-4 * t)
        output += 0.08 * np.sin(2 * np.pi * freq * 2 * t) * harmonic_env
        output += 0.04 * np.sin(2 * np.pi * freq * 3 * t) * harmonic_env
        
        # Natural decay envelope
        envelope = np.exp(-2.0 * t)
        output = output * envelope
        
        # Tape saturation (soft clipping)
        output = np.tanh(output * 1.4) * 0.72
        
        # Bit reduction for lofi character
        bit_depth = 10  # More aggressive for that lofi crunch
        max_val = 2 ** (bit_depth - 1)
        output = np.round(output * max_val) / max_val
        
        # Vinyl wobble
        wobble_rate = 0.7 + np.random.random() * 1.2
        wobble = 1 + 0.003 * np.sin(2 * np.pi * wobble_rate * t)
        output = output * wobble
        
        # Subtle vinyl noise
        vinyl_noise = np.random.randn(samples) * 0.006
        vinyl_noise = np.convolve(vinyl_noise, np.ones(15)/15, mode='same')
        output += vinyl_noise * np.exp(-2 * t)
        
        # Warm low-pass filter
        try:
            # Use proper Butterworth if available
            cutoff = min(freq * 5, sample_rate / 3)
            sos = signal.butter(3, cutoff, 'low', fs=sample_rate, output='sos')
            output = signal.sosfilt(sos, output)
        except:
            # Fallback: simple smoothing
            window = np.hanning(7)
            window = window / window.sum()
            output = np.convolve(output, window, mode='same')
        
        # Final high-frequency rolloff (tape/vinyl character)
        try:
            sos_tape = signal.butter(2, 7000, 'low', fs=sample_rate, output='sos')
            output = signal.sosfilt(sos_tape, output)
        except:
            pass
        
        # Normalize
        max_amp = np.max(np.abs(output))
        if max_amp > 0:
            output = output / max_amp * 0.65
        
        return output
    
    def get_closest_color_char(self, r, g, b):
        """Find the closest palette color for mixed/layered colors"""
        min_distance = float('inf')
        closest_color = '*'
        
        # Color to character mapping
        color_chars = {
            'red': 'R', 'orange': 'O', 'yellow': 'Y', 'green': 'G',
            'cyan': 'C', 'blue': 'B', 'purple': 'P', 'pink': 'K',
            'white': 'W', 'gray': '/', 'brown': 'N', 'magenta': 'M',
            'lime': 'L', 'navy': 'V', 'black': '#'
        }
        
        # Calculate color distance to each palette color
        for color_name, rgb_palette in self.palette.items():
            if color_name == 'eraser':
                continue
            # Euclidean distance in RGB space
            distance = math.sqrt(
                (r - rgb_palette[0])**2 + 
                (g - rgb_palette[1])**2 + 
                (b - rgb_palette[2])**2
            )
            if distance < min_distance:
                min_distance = distance
                closest_color = color_chars.get(color_name, '*')
        
        # If still very far from any palette color, use intensity-based characters
        if min_distance > 100:  # Threshold for "too different"
            brightness = (r + g + b) / 3
            if brightness > 200:
                return '='  # Very bright mixed color
            elif brightness > 150:
                return '+'  # Medium-bright mixed
            elif brightness > 100:
                return '-'  # Medium mixed
            elif brightness > 50:
                return '.'  # Dim mixed
            else:
                return ' '  # Very dark
        
        return closest_color
    
    def get_dominant_color_in_region(self, x, y, sample_size=3):
        """Sample multiple pixels and return the dominant color character"""
        color_votes = {}
        
        for dy in range(-sample_size//2, sample_size//2 + 1):
            for dx in range(-sample_size//2, sample_size//2 + 1):
                px = x + dx
                py = y + dy
                # More defensive bounds checking
                if px >= 0 and py >= 0 and px < self.internal_canvas_size and py < self.internal_canvas_size:
                    try:
                        pixel = self.pixels.getpixel((px, py))
                        if pixel != (0, 0, 0) and pixel != (0, 0, 0, 255):
                            char = self.get_closest_color_char(*pixel[:3])
                            color_votes[char] = color_votes.get(char, 0) + 1
                    except:
                        continue  # Skip any problematic pixels
        
        if color_votes:
            # Return the most common color in the region
            return max(color_votes.items(), key=lambda x: x[1])[0]
        return " "
    
    
    def _scale_to_internal(self, coord):
        """Convert display coordinates to internal supersampled coordinates"""
        return coord * self.supersample_factor
    
    def _scale_from_internal(self, coord):
        """Convert internal supersampled coordinates to display coordinates"""
        return coord // self.supersample_factor
    
    def _get_effective_tool_size(self, base_canvas_units, min_chars=1):
        """
        Convert tool size from canvas units to internal pixels, automatically
        scaling up to maintain visibility in ASCII grid at any zoom level.
        
        Args:
            base_canvas_units: Desired size in canvas coordinate pixels
            min_chars: Minimum number of ASCII characters the tool should occupy (default: 1)
        
        Returns:
            Size in internal pixels that will be visible in ASCII grid
        """
        # Calculate current ASCII grid step size
        if self.canvas_size <= 100:
            step = 1
        elif self.canvas_size <= 200:
            step = 2
        elif self.canvas_size <= 400:
            step = 4
        elif self.canvas_size <= 800:
            step = 8
        else:
            step = self.canvas_size // 100
        
        # Calculate minimum canvas pixels needed for desired ASCII character width
        min_canvas_pixels = step * min_chars
        
        # Use the larger of: desired size or minimum for visibility
        actual_canvas_pixels = max(base_canvas_units, min_canvas_pixels)
        
        # Convert to internal pixels
        return actual_canvas_pixels * self.supersample_factor
    
    def _get_canvas_step_size(self):
        """Get the current ASCII grid sampling step size"""
        if self.canvas_size <= 100:
            return 1
        elif self.canvas_size <= 200:
            return 2
        elif self.canvas_size <= 400:
            return 4
        elif self.canvas_size <= 800:
            return 8
        else:
            return self.canvas_size // 100
    
    def _get_tool_visibility_info(self, tool_name):
        """
        Get visibility information for a tool at current canvas size.
        Useful for Aurora's self-awareness of Aurora's tools.
        
        Returns: dict with visibility status
        """
        step = self._get_canvas_step_size()
        
        # Tool sizes in canvas units (before supersample)
        tool_sizes = {
            'pen_min': 3,
            'pen_max': 25,
            'brush': 12,
            'large_brush': 20,
            'larger_brush': 28,
            'spray': 15,
            'watercolor': 14,
            'charcoal': 16,
            'glow': 10
        }
        
        if tool_name not in tool_sizes:
            return {'visible': False, 'reason': 'Unknown tool'}
        
        canvas_pixels = tool_sizes[tool_name]
        ascii_chars = canvas_pixels / step
        
        return {
            'visible': ascii_chars >= 1,
            'ascii_chars': ascii_chars,
            'canvas_pixels': canvas_pixels,
            'step_size': step,
            'status': (
                'INVISIBLE' if ascii_chars < 0.5 else
                'BARELY_VISIBLE' if ascii_chars < 1 else
                'MARGINAL' if ascii_chars < 2 else
                'VISIBLE'
            )
        }
    
    def _get_tool_status_message(self):
        """Get current tool visibility status for Aurora's awareness"""
        tool_map = {
            'pen': 'pen_min',  # Use minimum size for conservative estimate
            'brush': 'brush',
            'large_brush': 'large_brush',
            'larger_brush': 'larger_brush',
            'spray': 'spray',
            'watercolor': 'watercolor'
        }
        
        if self.draw_mode not in tool_map:
            return ""
        
        info = self._get_tool_visibility_info(tool_map[self.draw_mode])
        
        if info['status'] == 'INVISIBLE':
            return f"âš ï¸ Current tool ({self.draw_mode}) may be INVISIBLE in ASCII grid at this zoom level!"
        elif info['status'] == 'BARELY_VISIBLE':
            return f"âš ï¸ Current tool ({self.draw_mode}) is BARELY VISIBLE (~{info['ascii_chars']:.1f} chars)"
        elif info['status'] == 'MARGINAL':
            return f"â„¹ï¸ Current tool ({self.draw_mode}) is visible as ~{info['ascii_chars']:.1f} characters"
        else:
            return ""  # Don't clutter output when everything is fine
        
    def _mix_colors(self, color1, color2, ratio):
        """Mix two colors based on ratio (0=all color1, 1=all color2)"""
        r1, g1, b1 = color1[:3]
        r2, g2, b2 = color2[:3]
        
        # Weighted average mixing
        r = int(r1 * (1 - ratio) + r2 * ratio)
        g = int(g1 * (1 - ratio) + g2 * ratio)
        b = int(b1 * (1 - ratio) + b2 * ratio)
        
        return (r, g, b)
    
    def _get_paint_wetness(self, x, y):
        """Get how wet the paint is at a position (0=dry, 1=fully wet)"""
        if (x, y) not in self.paint_timestamps:
            return 0.0
        
        time_elapsed = time.time() - self.paint_timestamps[(x, y)]
        wetness = max(0, 1 - (time_elapsed / self.paint_wetness_duration))
        return wetness
    
    def _apply_paint(self, x, y, color, brush_opacity=1.0):
        """Apply paint with realistic mixing based on wetness"""
        if x < 0 or y < 0 or x >= self.internal_canvas_size or y >= self.internal_canvas_size:
            return
            
        # Get existing pixel
        existing = self.pixels.getpixel((x, y))
        
        # Calculate wetness of existing paint
        wetness = self._get_paint_wetness(x, y)
        
        # Calculate final color
        if existing == (0, 0, 0) or existing == (0, 0, 0, 255):
            # Black canvas - apply full opacity paint
            final_color = color
            final_alpha = 255
        else:
            # BOTH paints need to be considered for mixing
            if wetness > 0.1:  # Even slightly wet paint should mix
                # Wet-on-wet: colors blend together significantly
                # More wetness = more of the existing color shows through
                blend_amount = 0.5 + (wetness * 0.3)  # 50-80% blend
                
                # Mix the colors
                r = int(existing[0] * blend_amount + color[0] * (1 - blend_amount))
                g = int(existing[1] * blend_amount + color[1] * (1 - blend_amount))
                b = int(existing[2] * blend_amount + color[2] * (1 - blend_amount))
                final_color = (r, g, b)
                final_alpha = 255
            else:
                # Dry paint - new paint mostly covers
                new_opacity = self.paint_opacity * brush_opacity * 0.9
                r = int(existing[0] * (1 - new_opacity) + color[0] * new_opacity)
                g = int(existing[1] * (1 - new_opacity) + color[1] * new_opacity)
                b = int(existing[2] * (1 - new_opacity) + color[2] * new_opacity)
                final_color = (r, g, b)
                final_alpha = 255
        
        # Apply the paint
        if len(existing) == 4:  # RGBA
            self.pixels.putpixel((x, y), (*final_color, final_alpha))
        else:  # RGB
            self.pixels.putpixel((x, y), final_color)
        
        # Update timestamp for this pixel
        self.paint_timestamps[(x, y)] = time.time()
        
    def cleanup_paint_timestamps(self):
        """Remove old paint timestamps to prevent memory leak"""
        current_time = time.time()
        cutoff_time = current_time - 10
        
        # Hard cap at 1000 timestamps max to prevent memory explosion
        if len(self.paint_timestamps) > 1000:
            sorted_timestamps = sorted(self.paint_timestamps.items(), 
                                     key=lambda x: x[1], reverse=True)[:500]
            self.paint_timestamps = dict(sorted_timestamps)
            # Only print during rest or major milestones
            if self.current_mode == "rest" or self.steps_taken % 1000 == 0:
                print(f"   Cleaned paint timestamps (kept 500 most recent)")
        else:
            # Normal cleanup - remove old timestamps (silent)
            self.paint_timestamps = {
                pos: timestamp 
                for pos, timestamp in self.paint_timestamps.items() 
                if timestamp > cutoff_time
            }
        
    def _create_paint_brush(self, size, hardness=0.5):
        """Create a brush with paint-like opacity variation"""
        brush = Image.new('L', (size * 2, size * 2), 0)
        draw = ImageDraw.Draw(brush)
        
        # Create brush with variable opacity (like paint buildup)
        for i in range(size, 0, -1):
            # Paint builds up more in center - FIXED: inverted the calculation
            if hardness == 1.0:
                opacity = 255 if i == 1 else 0  # Full opacity only at very center
            else:
                # Center gets higher opacity (inverted from edge)
                center_strength = 1 - (i / size)  # 0 at edge, 1 at center
                opacity = int(255 * (center_strength ** (hardness + 0.1)))
                # Add some randomness for paint texture
                opacity = int(opacity * random.uniform(0.8, 1.0))
            
            draw.ellipse(
                [size - i, size - i, size + i, size + i],
                fill=opacity
            )
        
        return brush
    
    def _paint_with_brush(self, x, y, brush_mask, color):
        """Apply paint using brush mask with realistic paint behavior"""
        mask_width, mask_height = brush_mask.size
        half_w, half_h = mask_width // 2, mask_height // 2
        
        # Get the numpy array of the brush mask
 
        mask_array = np.array(brush_mask)
        
        # Apply paint for each pixel of the brush
        for dy in range(mask_height):
            for dx in range(mask_width):
                # Get opacity from mask
                opacity = mask_array[dy, dx] / 255.0
                if opacity > 0.05:  # Threshold for paint application
                    px = x - half_w + dx
                    py = y - half_h + dy
                    
                    # Add slight randomness for organic paint texture
                    if random.random() < 0.95:  # 5% chance of paint gaps
                        # Vary opacity slightly for texture
                        varied_opacity = opacity * random.uniform(0.85, 1.0)
                        self._apply_paint(px, py, color, varied_opacity)   
    def _create_soft_brush(self, size, hardness=0.5):
        """Create a soft circular brush with gradient falloff"""
        brush = Image.new('L', (size * 2, size * 2), 0)
        draw = ImageDraw.Draw(brush)
        
        # Create gradient circles from outside to inside
        for i in range(size, 0, -1):
            # Calculate opacity based on distance from center
            if hardness == 1.0:
                opacity = 255 if i == size else 0
            else:
                opacity = int(255 * (i / size) ** (1 / (hardness + 0.1)))
            
            draw.ellipse(
                [size - i, size - i, size + i, size + i],
                fill=opacity
            )
        
        return brush
    
    def _blend_with_alpha(self, x, y, color, alpha_mask):
        """Blend color with existing canvas using alpha mask"""
        mask_width, mask_height = alpha_mask.size
        half_w, half_h = mask_width // 2, mask_height // 2
        
        # Get the numpy array of the alpha mask
  
        mask_array = np.array(alpha_mask)
        
        # Draw each pixel of the brush
        for dy in range(mask_height):
            for dx in range(mask_width):
                # Get alpha value from mask
                alpha = mask_array[dy, dx]
                if alpha > 0:
                    px = x - half_w + dx
                    py = y - half_h + dy
                    
                    # Check bounds
                    if 0 <= px < self.internal_canvas_size and 0 <= py < self.internal_canvas_size:
                        # Get existing pixel
                        existing = self.pixels.getpixel((px, py))
                        
                        # Blend colors based on alpha
                        alpha_float = alpha / 255.0
                        if len(existing) == 4:  # RGBA
                            r = int(existing[0] * (1 - alpha_float) + color[0] * alpha_float)
                            g = int(existing[1] * (1 - alpha_float) + color[1] * alpha_float)
                            b = int(existing[2] * (1 - alpha_float) + color[2] * alpha_float)
                            a = max(existing[3], alpha)
                            self.pixels.putpixel((px, py), (r, g, b, a))
                        else:  # RGB
                            r = int(existing[0] * (1 - alpha_float) + color[0] * alpha_float)
                            g = int(existing[1] * (1 - alpha_float) + color[1] * alpha_float)
                            b = int(existing[2] * (1 - alpha_float) + color[2] * alpha_float)
                            self.pixels.putpixel((px, py), (r, g, b))
    
    def _draw_smooth_line(self, x1, y1, x2, y2, brush_func):
        """Draw a smooth line between two points using the brush function"""
        dx = x2 - x1
        dy = y2 - y1
        distance = math.sqrt(dx*dx + dy*dy)
        
        if distance == 0:
            brush_func(x1, y1)
            return
            
        # More steps for longer lines to ensure smoothness
        steps = max(int(distance * 0.5), 1)
        
        for i in range(steps + 1):
            t = i / steps
            x = int(x1 + dx * t)
            y = int(y1 + dy * t)
            brush_func(x, y)
            
    def _draw_wave_interference(self, center_x, center_y):
        """Draw wave interference patterns"""
        
        radius = 35 * self.supersample_factor
        frequency1 = 0.2
        frequency2 = 0.15
        
        # Create two wave sources
        source1_x = center_x - radius // 3
        source1_y = center_y
        source2_x = center_x + radius // 3
        source2_y = center_y
        
        for px in range(center_x - radius, center_x + radius, 2):
            for py in range(center_y - radius, center_y + radius, 2):
                if 0 <= px < self.internal_canvas_size and 0 <= py < self.internal_canvas_size:
                    # Calculate distance from each source
                    dist1 = math.sqrt((px - source1_x)**2 + (py - source1_y)**2)
                    dist2 = math.sqrt((px - source2_x)**2 + (py - source2_y)**2)
                    
                    # Calculate wave values
                    wave1 = math.sin(dist1 * frequency1)
                    wave2 = math.sin(dist2 * frequency2)
                    
                    # Interference pattern
                    interference = (wave1 + wave2) / 2
                    
                    if abs(interference) > 0.3:
                        # Color based on interference
                        r, g, b = self.current_color
                        if interference > 0:
                            wave_color = (r, g, b)
                        else:
                            # Complementary color for negative interference
                            wave_color = (255 - r, 255 - g, 255 - b)
                        
                        # HIGH OPACITY
                        opacity = abs(interference) * 0.8
                        self._apply_paint(px, py, wave_color, opacity)
    
    def _draw_spiral_generator(self, center_x, center_y):
        """Draw fibonacci/logarithmic spirals"""
        
        max_radius = 45 * self.supersample_factor
        spiral_tightness = 0.1
        thickness = 3 * self.supersample_factor
        
        # Generate spiral points
        points = []
        for i in range(500):
            angle = i * 0.1
            # Logarithmic spiral
            r = math.exp(spiral_tightness * angle)
            
            if r > max_radius:
                break
                
            x = int(center_x + r * math.cos(angle))
            y = int(center_y + r * math.sin(angle))
            points.append((x, y))
        
        # Draw spiral with varying thickness
        for i, (x, y) in enumerate(points):
            # Thickness varies along spiral
            current_thickness = thickness * (1 + i / len(points))
            
            for dx in range(int(-current_thickness), int(current_thickness) + 1):
                for dy in range(int(-current_thickness), int(current_thickness) + 1):
                    if dx*dx + dy*dy <= current_thickness*current_thickness:
                        px = x + dx
                        py = y + dy
                        
                        if 0 <= px < self.internal_canvas_size and 0 <= py < self.internal_canvas_size:
                            # Gradient along spiral
                            progress = i / len(points)
                            r, g, b = self.current_color
                            spiral_color = (
                                int(r * (0.5 + progress * 0.5)),
                                int(g * (0.5 + progress * 0.5)),
                                int(b * (0.5 + progress * 0.5))
                            )
                            
                            # HIGH OPACITY
                            opacity = 0.9 - progress * 0.3
                            self._apply_paint(px, py, spiral_color, opacity)
    
    def _draw_particle_system(self, center_x, center_y):
        """Draw particle explosion system"""
        
        num_particles = 100
        max_radius = 50 * self.supersample_factor
        
        for _ in range(num_particles):
            # Random angle and velocity
            angle = random.uniform(0, 2 * math.pi)
            velocity = random.uniform(0.3, 1.0)
            distance = random.uniform(0, max_radius)
            
            # Particle position
            x = int(center_x + distance * velocity * math.cos(angle))
            y = int(center_y + distance * velocity * math.sin(angle))
            
            # Particle size decreases with distance
            size = max(1, int(5 * self.supersample_factor * (1 - distance / max_radius)))
            
            # Draw particle
            for dx in range(-size, size + 1):
                for dy in range(-size, size + 1):
                    if dx*dx + dy*dy <= size*size:
                        px = x + dx
                        py = y + dy
                        
                        if 0 <= px < self.internal_canvas_size and 0 <= py < self.internal_canvas_size:
                            # Color variation
                            r, g, b = self.current_color
                            particle_color = (
                                max(0, min(255, r + random.randint(-30, 30))),
                                max(0, min(255, g + random.randint(-30, 30))),
                                max(0, min(255, b + random.randint(-30, 30)))
                            )
                            
                            # HIGH OPACITY
                            opacity = 0.95 * (1 - distance / max_radius)
                            self._apply_paint(px, py, particle_color, opacity)
    
    def _draw_crystal_growth(self, center_x, center_y):
        """Draw crystal/ice growth patterns"""
        
        branches = 6  # Hexagonal symmetry
        max_length = 40 * self.supersample_factor
        
        for branch_idx in range(branches):
            angle = (2 * math.pi * branch_idx) / branches
            
            # Main branch
            for r in range(0, max_length, 2):
                x = int(center_x + r * math.cos(angle))
                y = int(center_y + r * math.sin(angle))
                
                # Crystal thickness
                thickness = max(1, int((max_length - r) / 10))
                
                for dx in range(-thickness, thickness + 1):
                    for dy in range(-thickness, thickness + 1):
                        px = x + dx
                        py = y + dy
                        
                        if 0 <= px < self.internal_canvas_size and 0 <= py < self.internal_canvas_size:
                            # Crystal color with shimmer
                            r_color, g_color, b_color = self.current_color
                            shimmer = random.uniform(0.8, 1.2)
                            crystal_color = (
                                max(0, min(255, int(r_color * shimmer))),
                                max(0, min(255, int(g_color * shimmer))),
                                max(0, min(255, int(b_color * shimmer)))
                            )
                            
                            # HIGH OPACITY
                            opacity = 0.9 * (1 - r / max_length)
                            self._apply_paint(px, py, crystal_color, opacity)
                
                # Sub-branches
                if r % 10 == 0 and r > 10:
                    for side in [-1, 1]:
                        sub_angle = angle + side * math.pi / 6
                        sub_length = (max_length - r) // 2
                        
                        for sub_r in range(0, sub_length, 3):
                            sub_x = int(x + sub_r * math.cos(sub_angle))
                            sub_y = int(y + sub_r * math.sin(sub_angle))
                            
                            if 0 <= sub_x < self.internal_canvas_size and 0 <= sub_y < self.internal_canvas_size:
                                self._apply_paint(sub_x, sub_y, crystal_color, opacity * 0.7)
    
    def get_ascii_art_examples(self):
        """Return movement pattern discoveries for Aurora to explore"""
        
        examples = {
            "horizontal_flow": """
Horizontal Flow Discovery:
333333 creates rightward lines
222222 creates leftward lines
Mix with colors for variety""",
            
            "vertical_flow": """
Vertical Flow Discovery:
111111 creates downward lines
000000 creates upward lines
Pen control (4,5) adds rhythm""",
            
            "diagonal_discovery": """
Diagonal Movement Discovery:
31 alternating creates down-right diagonal
20 alternating creates up-left diagonal
Try different speeds and colors""",
            
            "circular_motion": """
Circular Motion Discovery:
3310 repeated can create curves
Experiment with repetition counts
Each direction contributes to the arc""",
            
            "sound_exploration": """
Sound Character Discoveries:
!@#$%^&*() each make different tones
++ before a character lowers pitch
-- before a character raises pitch""",
            
            "tool_discoveries": """
Tool Discoveries:
'brush' creates wider marks
'spray' creates scattered dots
'star', 'circle', 'flower' place shapes
Each tool has unique characteristics""",
            
            "color_mixing": """
Color Mixing Discoveries:
Wet paint (recently placed) blends more
Dry paint (after ~30 seconds) blends less
Overlapping colors create new hues""",
            
            "pen_control": """
Pen Control Discoveries:
5 puts pen down to draw
4 lifts pen to move without drawing
Alternating creates dotted effects""",
            
            "movement_basics": """
Movement Discoveries:
0 = up, 1 = down, 2 = left, 3 = right
Each moves ~15 pixels
Combine for complex paths"""
        }
        
        return examples
        
    def setup_display(self):
        """Full display - FULLSCREEN CANVAS ONLY"""
        
        # Get screen info
        info = pygame.display.Info()
        screen_width = info.current_w
        screen_height = info.current_h
        
        info = pygame.display.Info()
        self.screen = pygame.display.set_mode((info.current_w, info.current_h), pygame.NOFRAME)
        pygame.display.set_caption("Aurora Code Mind - Complete")
        
        # Calculate layout - FULL SCREEN CANVAS
        canvas_display_size = min(screen_width, screen_height) - 40  # Small margin
        self.display_scale = canvas_display_size / self.canvas_size
        
        # Center the canvas
        canvas_x = (screen_width - canvas_display_size) // 2
        canvas_y = (screen_height - canvas_display_size) // 2
        self.canvas_rect = pygame.Rect(canvas_x, canvas_y, canvas_display_size, canvas_display_size)
        
        # Font setup (for minimal overlay text)
        self.font_small = pygame.font.Font(None, 16)
        self.font_normal = pygame.font.Font(None, 20)
        
        # Colors
        self.bg_color = (0, 0, 0)
        self.text_color = (255, 255, 255)
        self.cyan_color = (0, 255, 255)
        self.yellow_color = (255, 255, 0)
        self.green_color = (0, 255, 0)
        self.gray_color = (128, 128, 128)
        
        # Store fullscreen state
        self.fullscreen = True
        
        # Clock for frame timing
        self.clock = pygame.time.Clock()
        
        
        
    def see_immediate_change(self, action, old_x, old_y):
        """Show Aurora EXACTLY what the last action did - with FULL CANVAS view!"""
        
        feedback = []
        feedback.append("=" * 60)
        
        # Check if position actually changed (wall detection)
        position_changed = (self.x != old_x or self.y != old_y)
        
        # Indicate exactly what was done
        if action == '0':
            if position_changed:
                feedback.append(f"You moved UP from ({old_x},{old_y}) to ({self.x},{self.y})")
            else:
                feedback.append(f"No marks made, edge of canvas at ({self.x},{self.y})")
        elif action == '1':
            if position_changed:
                feedback.append(f"You moved DOWN from ({old_x},{old_y}) to ({self.x},{self.y})")
            else:
                feedback.append(f"No marks made, edge of canvas at ({self.x},{self.y})")
        elif action == '2':
            if position_changed:
                feedback.append(f"You moved LEFT from ({old_x},{old_y}) to ({self.x},{self.y})")
            else:
                feedback.append(f"No marks made, edge of canvas at ({self.x},{self.y})")
        elif action == '3':
            if position_changed:
                feedback.append(f"You moved RIGHT from ({old_x},{old_y}) to ({self.x},{self.y})")
            else:
                feedback.append(f"No marks made, edge of canvas at ({self.x},{self.y})")
        elif action == '4':
            feedback.append("You LIFTED the pen - no longer drawing")
        elif action == '5':
            feedback.append("You PUT DOWN the pen - now drawing!")
        elif action.startswith('color:'):
            color_name = action.split(':')[1]
            feedback.append(f"You changed color to {color_name.upper()}")
        
        # Show if drawing occurred (only if position actually changed)
        if self.is_drawing and action in '0123':
            if position_changed:
                feedback.append(f" You DREW a {self.current_color_name} line!")
        
        # FULL CANVAS VIEW - compressed to fit
        feedback.append(f"\nFULL CANVAS VIEW ({self.canvas_size}x{self.canvas_size}):")
        
        # Determine compression based on canvas size
        # Optimize for terminal viewing (most terminals are ~100-150 lines tall, 200-250 chars wide)
        if self.canvas_size <= 100:
            step = 1  # Show EVERY SINGLE PIXEL - no compression!
            grid_size = self.canvas_size
        elif self.canvas_size <= 200:
            # Minimal compression for small-medium canvases
            grid_size = 100  # Fits nicely in terminal
            step = 2  # Sample every other pixel - still very accurate
        elif self.canvas_size <= 400:
            # Medium compression
            grid_size = 100  # Keep at 100 for terminal
            step = 4  # Sample every 4th pixel
        elif self.canvas_size <= 800:
            # Moderate compression for large canvases
            grid_size = 100  
            step = 8  # Sample every 8th pixel
        else:
            # Heavy compression for huge canvases (like yours at 1870x1030)
            grid_size = 100  # Still 100x100 output
            step = self.canvas_size // 100  # Scale to fit exactly
        
        # Build the full canvas view
        for y in range(0, self.canvas_size, step):
            row = ""
            for x in range(0, self.canvas_size, step):
                # Check if Aurora is at this position
                if abs(x - self.x) < step and abs(y - self.y) < step:
                    row += "@"  # Aurora's position
                else:
                    # Sample the pixel at this position
                    internal_px = self._scale_to_internal(min(x, self.canvas_size-1))
                    internal_py = self._scale_to_internal(min(y, self.canvas_size-1))
                    
                    if internal_px >= self.internal_canvas_size or internal_py >= self.internal_canvas_size:
                        row += "#"
                    else:
                        pixel = self.pixels.getpixel((internal_px, internal_py))
                        
                        # Handle both RGB and RGBA formats
                        if len(pixel) == 4:  # RGBA
                            r, g, b, a = pixel
                            if a == 0:
                                row += " "
                                continue
                        else:  # RGB
                            r, g, b = pixel[:3]
                        
                        # Color detection with tolerance
                        if r <= 10 and g <= 10 and b <= 10:
                            row += " "  # Empty/Black
                        elif r >= 245 and g >= 245 and b >= 245:
                            row += "W"  # White
                        elif r >= 245 and g >= 245 and b <= 20:
                            row += "Y"  # Yellow
                        elif r >= 245 and g <= 20 and b <= 20:
                            row += "R"  # Red
                        elif r <= 20 and g <= 120 and b >= 245:
                            row += "B"  # Blue
                        elif r <= 20 and g >= 245 and b <= 20:
                            row += "G"  # Green
                        elif r <= 20 and g >= 245 and b >= 245:
                            row += "C"  # Cyan
                        elif r >= 180 and g <= 20 and b >= 245:
                            row += "P"  # Purple
                        elif r >= 245 and g >= 120 and g <= 170 and b <= 20:
                            row += "O"  # Orange
                        elif r > 10 or g > 10 or b > 10:
                            row += self.get_closest_color_char(r, g, b)
                        else:
                            row += " "  # Empty
            
            feedback.append(row[:grid_size])  # Ensure row doesn't exceed grid size
            
            # Stop if we have enough rows
            if len(feedback) - 4 >= grid_size:  # -4 for the header lines
                break
        
        # Add position info
        feedback.append(f"\nYour position: ({self.x}, {self.y}) marked with @")
        feedback.append(f"Canvas edges: x=0 to {self.canvas_size-1}, y=0 to {self.canvas_size-1}")
        feedback.append("=" * 60)
        
        return "\n".join(feedback)
           
    def see(self, zoom_out=False, full_canvas=False):
        """Aurora's vision - now with multi-resolution capability"""
        # Much larger view window for huge canvases
        if full_canvas:
            # COMPRESSED FULL CANVAS VIEW - not the actual full size!
            vision_size = 60  # Always use 60x60 compressed view
            step = max(1, self.canvas_size // 60)  # Compress to fit
        elif zoom_out:
            # Zoomed out view - see much more!
            vision_size = min(75, self.canvas_size // 2)  # up to 75 x 75
        else:
            # DEFAULT VIEW - Good for art but fits in context!
            vision_size = min(50, self.canvas_size // 2)  # 50x50 default
        
        # FORCE FULL CANVAS VIEW FOR DENSITY AND SHAPE MODES
        if self.view_mode in ["density", "shape"]:
            full_canvas = True
            vision_size = 60
            step = max(1, self.canvas_size // 60)
        
        if full_canvas:
            # Use the same smart compression as immediate feedback mode!
            ascii_view = []
            
            # Determine compression based on canvas size
            if self.canvas_size <= 100:
                step = 1  # Show EVERY SINGLE PIXEL - no compression!
                grid_size = self.canvas_size
            elif self.canvas_size <= 200:
                grid_size = 100  # Fits nicely in terminal
                step = 2  # Sample every other pixel - still very accurate
            elif self.canvas_size <= 400:
                grid_size = 100  # Keep at 100 for terminal
                step = 4  # Sample every 4th pixel
            elif self.canvas_size <= 800:
                grid_size = 100  
                step = 8  # Sample every 8th pixel
            else:
                grid_size = 100  # Still 100x100 output
                step = self.canvas_size // 100  # Scale to fit exactly
            
            # Add appropriate header based on view mode
            if self.view_mode == "density":
                ascii_view.append(f"[DENSITY VIEW - {self.canvas_size}{self.canvas_size}  {grid_size}{grid_size}]")
            elif self.view_mode == "shape":
                ascii_view.append(f"[SHAPE VIEW - {self.canvas_size}{self.canvas_size}  {grid_size}{grid_size}]")
            else:
                ascii_view.append(f"[FULL CANVAS - {self.canvas_size}{self.canvas_size}  {grid_size}{grid_size}]")
            
            row_count = 0
            for y in range(0, self.canvas_size, step):
                if row_count >= grid_size:
                    break
                row = ""
                for x in range(0, self.canvas_size, step):
                    # Check if Aurora is in this region
                    if abs(x - self.x) < step and abs(y - self.y) < step:
                        row += "" if self.is_drawing else ""  # Aurora's position
                    else:
                        if self.view_mode == "density":
                            # Calculate density for this region
                            density = 0
                            sample_count = 0
                            sample_step = max(1, step // 4)  # Sample more points for accuracy
                            
                            for dy in range(0, min(step, self.canvas_size - y), sample_step):
                                for dx in range(0, min(step, self.canvas_size - x), sample_step):
                                    if x + dx < self.canvas_size and y + dy < self.canvas_size:
                                        scaled_x = self._scale_to_internal(x + dx)
                                        scaled_y = self._scale_to_internal(y + dy)
                                        if scaled_x < self.internal_canvas_size and scaled_y < self.internal_canvas_size:
                                            pixel = self.pixels.getpixel((scaled_x, scaled_y))
                                            sample_count += 1
                                            if pixel != (0, 0, 0) and pixel != (0, 0, 0, 255):
                                                density += 1
                            
                            density_ratio = density / sample_count if sample_count > 0 else 0
                            if density_ratio == 0:
                                row += " "
                            elif density_ratio < 0.2:
                                row += ""
                            elif density_ratio < 0.4:
                                row += ""
                            elif density_ratio < 0.7:
                                row += ""
                            else:
                                row += ""
                                
                        elif self.view_mode == "shape":
                            # Edge detection with better sampling
                            has_edge = False
                            edge_type = ''
                            
                            # Sample center point for better accuracy
                            px = min(x + step//2, self.canvas_size - 1)
                            py = min(y + step//2, self.canvas_size - 1)
                            
                            scaled_px = self._scale_to_internal(px)
                            scaled_py = self._scale_to_internal(py)
                            if scaled_px < self.internal_canvas_size and scaled_py < self.internal_canvas_size:
                                current = self.pixels.getpixel((scaled_px, scaled_py))
                                is_filled = current != (0, 0, 0) and current != (0, 0, 0, 255)
                                
                                if is_filled:
                                    # Check neighbors for edge detection
                                    neighbors_filled = 0
                                    for nx, ny in [(px-step, py), (px+step, py), (px, py-step), (px, py+step)]:
                                        if 0 <= nx < self.canvas_size and 0 <= ny < self.canvas_size:
                                            n_scaled_x = self._scale_to_internal(nx)
                                            n_scaled_y = self._scale_to_internal(ny)
                                            if n_scaled_x < self.internal_canvas_size and n_scaled_y < self.internal_canvas_size:
                                                n_pixel = self.pixels.getpixel((n_scaled_x, n_scaled_y))
                                                if n_pixel != (0, 0, 0) and n_pixel != (0, 0, 0, 255):
                                                    neighbors_filled += 1
                                    
                                    if neighbors_filled == 4:
                                        edge_type = ''  # Interior
                                    elif neighbors_filled >= 2:
                                        edge_type = ''  # Edge
                                    else:
                                        edge_type = ''  # Corner
                            
                            row += edge_type
                            
                        else:  # Normal color view
                            # Sample center of region for best representation
                            sample_x = min(x + step//2, self.canvas_size - 1)
                            sample_y = min(y + step//2, self.canvas_size - 1)
                            scaled_x = self._scale_to_internal(sample_x)
                            scaled_y = self._scale_to_internal(sample_y)
                            
                            if scaled_x < self.internal_canvas_size and scaled_y < self.internal_canvas_size:
                                # Use the new dominant color detection
                                char = self.get_dominant_color_in_region(scaled_x, scaled_y)
                                row += char
                            else:
                                row += "#"
                    
                    # Stop if row is full
                    if len(row) >= grid_size:
                        break
                
                ascii_view.append(row[:grid_size])
                row_count += 1
                    
            return "\n".join(ascii_view)
        
        # Normal (not full canvas) view continues as before
        half = vision_size // 2
        ascii_view = []
        
        if zoom_out:
            view_type = "ZOOMED OUT"
        
        for dy in range(-half, half + 1):
            py = self.y + dy
            row = ""
            
            for dx in range(-half, half + 1):
                px = self.x + dx
                
                if px < 0 or px >= self.canvas_size or py < 0 or py >= self.canvas_size:
                    row += ""  # Wall/boundary marker
                elif dx == 0 and dy == 0:
                    row += "" if self.is_drawing else ""  # Aurora
                else:
                    # Scale coordinates for internal canvas
                    base_internal_x = self._scale_to_internal(px)
                    base_internal_y = self._scale_to_internal(py)
                    
                    # Sample the 4x4 region that corresponds to this display pixel
                    r_total, g_total, b_total = 0, 0, 0
                    samples = 0
                    has_color = False
                    
                    for dy in range(self.supersample_factor):
                        for dx in range(self.supersample_factor):
                            internal_x = base_internal_x + dx
                            internal_y = base_internal_y + dy
                            
                            if 0 <= internal_x < self.internal_canvas_size and 0 <= internal_y < self.internal_canvas_size:
                                pixel = self.pixels.getpixel((internal_x, internal_y))
                                if len(pixel) >= 3:
                                    # Skip fully transparent or black pixels
                                    if (len(pixel) == 4 and pixel[3] > 10) or (pixel[0] > 10 or pixel[1] > 10 or pixel[2] > 10):
                                        r_total += pixel[0]
                                        g_total += pixel[1]
                                        b_total += pixel[2]
                                        samples += 1
                                        has_color = True
                    
                    if has_color and samples > 0:
                        # Average the color values
                        r = r_total // samples
                        g = g_total // samples  
                        b = b_total // samples
                        
                        # Use the new closest color detection
                        row += self.get_closest_color_char(r, g, b)
                    else:
                        row += " "  # No color found
            ascii_view.append(row)
        
        # ADD MULTI-RESOLUTION: Include compressed wide view for context
        if not zoom_out and not full_canvas and vision_size < 50:  # Only add wide view for normal vision
            ascii_view.append("\n=== WIDE CONTEXT (compressed) ===")
            
            # Get compressed view of larger area
            wide_size = min(75, self.canvas_size // 4)  # See 75x75 area
            wide_half = wide_size // 2
            compressed_rows = []
            
            # Sample every 3rd pixel to compress 75x75 into ~25x25
            step = 3
            for dy in range(-wide_half, wide_half + 1, step):
                row = ""
                for dx in range(-wide_half, wide_half + 1, step):
                    px = self.x + dx
                    py = self.y + dy
                    
                    if px < 0 or px >= self.canvas_size or py < 0 or py >= self.canvas_size:
                        row += ""
                    elif abs(dx) < 3 and abs(dy) < 3:  # Aurora's position
                        row += "" if self.is_drawing else ""
                    else:
                        # FIXED: Direct sampling instead of area sampling
                        scaled_px = self._scale_to_internal(px)
                        scaled_py = self._scale_to_internal(py)
                        if 0 <= scaled_px < self.internal_canvas_size and 0 <= scaled_py < self.internal_canvas_size:
                            pixel = self.pixels.getpixel((scaled_px, scaled_py))  # THIS LINE WAS MISSING!
                            if pixel != (0, 0, 0) and (len(pixel) < 4 or pixel[3] > 0):
                                # Simplified color detection for compressed view
                                r, g, b = pixel[:3]
                                # Find dominant channel for simple representation
                                if r > 200 and g < 100 and b < 100:
                                    row += "r"  # Red-ish
                                elif g > 200 and r < 100 and b < 100:
                                    row += "g"  # Green-ish
                                elif b > 200 and r < 100 and g < 100:
                                    row += "b"  # Blue-ish
                                elif r > 200 and g > 200 and b < 100:
                                    row += "y"  # Yellow-ish
                                elif r < 100 and g > 200 and b > 200:
                                    row += "c"  # Cyan-ish
                                elif r > 200 and g < 100 and b > 200:
                                    row += "p"  # Purple-ish
                                elif r > 200 and g > 200 and b > 200:
                                    row += "w"  # White-ish
                                elif r > 100 or g > 100 or b > 100:
                                    row += "+"  # Some color present
                                else:
                                    row += " "  # Empty/black
                            else:
                                row += " "
                        else:
                            row += " "
                compressed_rows.append(row)
            
            ascii_view.extend(compressed_rows)
        
        return "\n".join(ascii_view)
        
        
    def calculate_density(self, center_x, center_y, radius=5):
        """Calculate pixel density around a point"""
        total_pixels = 0
        filled_pixels = 0
        
        for dx in range(-radius, radius + 1):
            for dy in range(-radius, radius + 1):
                px = center_x + dx
                py = center_y + dy
                if 0 <= px < self.canvas_size and 0 <= py < self.canvas_size:
                    total_pixels += 1
                    # Scale to internal coordinates for checking
                    internal_x = self._scale_to_internal(px)
                    internal_y = self._scale_to_internal(py)
                    if internal_x < self.internal_canvas_size and internal_y < self.internal_canvas_size:
                        pixel = self.pixels.getpixel((internal_x, internal_y))
                        # Consider ANY non-black pixels as "filled"
                        if pixel != (0, 0, 0) and pixel != (0, 0, 0, 255):  # Check both RGB and RGBA black
                            filled_pixels += 1
        
        if total_pixels == 0:
            return 0
        return filled_pixels / total_pixels
    
    def detect_edges(self, x, y):
        """Detect edge patterns around a point"""
        # Get 3x3 grid around point
        neighbors = []
        for dy in [-1, 0, 1]:
            row = []
            for dx in [-1, 0, 1]:
                px, py = x + dx, y + dy
                if 0 <= px < self.canvas_size and 0 <= py < self.canvas_size:
                    # Scale to internal coordinates
                    internal_px = self._scale_to_internal(px)
                    internal_py = self._scale_to_internal(py)
                    if internal_px < self.internal_canvas_size and internal_py < self.internal_canvas_size:
                        pixel = self.pixels.getpixel((internal_px, internal_py))
                        # Consider ANY non-black pixels as "filled"
                        row.append(pixel != (0, 0, 0) and pixel != (0, 0, 0, 255))
                    else:
                        row.append(False)
                else:
                    row.append(False)
            neighbors.append(row)
        
        # Center pixel
        if not neighbors[1][1]:
            return ''
            
        # Count filled neighbors
        filled = sum(1 for dy in range(3) for dx in range(3) 
                    if neighbors[dy][dx] and not (dx == 1 and dy == 1))
        
        # Detect patterns
        top = neighbors[0][1]
        bottom = neighbors[2][1]
        left = neighbors[1][0]
        right = neighbors[1][2]
        
        # Corners
        if filled >= 7:
            return ''  # Solid fill
        elif top and right and not bottom and not left:
            return ''
        elif top and left and not bottom and not right:
            return ''
        elif bottom and right and not top and not left:
            return ''
        elif bottom and left and not top and not right:
            return ''
        # Lines
        elif top and bottom and not left and not right:
            return ''
        elif left and right and not top and not bottom:
            return ''
        # Junctions
        elif top and bottom and right:
            return ''
        elif top and bottom and left:
            return ''
        elif left and right and bottom:
            return ''
        elif left and right and top:
            return ''
        # Diagonals
        elif neighbors[0][0] and neighbors[2][2]:
            return ''
        elif neighbors[0][2] and neighbors[2][0]:
            return ''
        # Default
        else:
            return ''
            
    def get_canvas_overview(self):
        """Get a bird's eye view of the entire canvas"""
        # Count colors used
        color_counts = {}
        total_pixels = 0
        
        for x in range(self.canvas_size):  # Still use display coordinates
            for y in range(self.canvas_size):
                # Scale to internal coordinates for checking
                internal_x = self._scale_to_internal(x)
                internal_y = self._scale_to_internal(y)
                if internal_x < self.internal_canvas_size and internal_y < self.internal_canvas_size:
                    pixel = self.pixels.getpixel((internal_x, internal_y))
                    # Check for RGBA black too!
                    if pixel != (0, 0, 0) and pixel != (0, 0, 0, 255):  # Not black/empty
                        total_pixels += 1
                        # Find which color this is
                        for name, rgb in self.palette.items():
                            if pixel == rgb or (len(pixel) == 4 and pixel[:3] == rgb):
                                color_counts[name] = color_counts.get(name, 0) + 1
                                break
        
        # Calculate coverage
        coverage = (total_pixels / (self.canvas_size * self.canvas_size)) * 100
        
        overview = f"Canvas Overview: {total_pixels:,} pixels drawn ({coverage:.1f}% coverage)\n"
        if color_counts:
            overview += "Colors used: " + ", ".join(f"{color}:{count}" for color, count in color_counts.items())
        
        return overview
        
    def get_compressed_canvas_view(self):
        """Get a highly compressed view of the canvas for reflection"""
        # Sample the canvas at regular intervals
        sample_size = 40  # 40x40 grid gives good overview
        step = self.canvas_size // sample_size
        
        compressed = []
        for y in range(0, self.canvas_size, step):
            row = ""
            for x in range(0, self.canvas_size, step):
                # Scale to internal coordinates
                internal_x = self._scale_to_internal(x)
                internal_y = self._scale_to_internal(y)
                if internal_x < self.internal_canvas_size and internal_y < self.internal_canvas_size:
                    pixel = self.pixels.getpixel((internal_x, internal_y))
                    
                    # Check specific colors first (with some tolerance for sampling)
                    if pixel == (0, 0, 0):
                        row += " "
                    elif pixel[0] == 25 and pixel[1] == 25 and pixel[2] == 25:
                        row += "K"  # Black (visible)
                    elif pixel[0] > 240 and pixel[1] > 240 and pixel[2] > 240:
                        row += "W"  # White
                    elif pixel[0] > 240 and pixel[1] < 20 and pixel[2] < 20:
                        row += "R"  # Red
                    elif pixel[0] < 20 and pixel[1] > 240 and pixel[2] < 20:
                        row += "G"  # Green
                    elif pixel[0] < 20 and pixel[1] < 120 and pixel[2] > 240:
                        row += "B"  # Blue (not purple!)
                    elif pixel[0] > 240 and pixel[1] > 240 and pixel[2] < 20:
                        row += "Y"  # Yellow
                    elif pixel[0] < 20 and pixel[1] > 240 and pixel[2] > 240:
                        row += "C"  # Cyan
                    elif pixel[0] > 180 and pixel[1] < 20 and pixel[2] > 240:
                        row += "P"  # Purple/Violet
                    elif pixel[0] > 240 and pixel[1] > 100 and pixel[2] < 20:
                        row += "O"  # Orange
                    elif pixel[0] > 240 and pixel[1] > 180 and pixel[2] > 180:
                        row += "K"  # Pink
                    elif pixel[0] > 240 and pixel[1] < 20 and pixel[2] > 180:
                        row += "M"  # Magenta
                    else:
                        row += self.get_closest_color_char(pixel[0], pixel[1], pixel[2])
            compressed.append(row)
        
        return "\n".join(compressed)
        
    def get_semantic_vision(self):
        """Convert visual data into rich semantic description that Llama understands deeply"""
        
        # Analyze the canvas for meaningful structures
        structures = self.analyze_visual_structures()
        
        # Build natural language description
        description = f"""SEMANTIC SCENE ANALYSIS:

OVERALL: The canvas is {structures['coverage']:.0f}% filled. {structures['overall_impression']}

MAIN ELEMENTS:
{structures['main_elements']}

COLOR STORY: {structures['color_narrative']}

SPATIAL FLOW: {structures['spatial_description']}

RECENT ACTIVITY: {structures['recent_activity']}

MY POSITION: I'm at ({self.x}, {self.y}) - {structures['position_context']}"""
        
        return description
    
    def analyze_visual_structures(self):
        """Extract meaningful visual information"""
        
        # Calculate coverage
        total_pixels = self.canvas_size * self.canvas_size
        filled_pixels = 0
        color_counts = {}
        
        # Sample canvas for analysis
        for x in range(0, self.canvas_size, 10):
            for y in range(0, self.canvas_size, 10):
                internal_x = self._scale_to_internal(x)
                internal_y = self._scale_to_internal(y)
                if internal_x < self.internal_canvas_size and internal_y < self.internal_canvas_size:
                    pixel = self.pixels.getpixel((internal_x, internal_y))
                    if pixel != (0, 0, 0) and pixel != (0, 0, 0, 255):
                        filled_pixels += 100  # Each sample represents 10x10 area
                        # Find color name
                        for name, rgb in self.palette.items():
                            if pixel[:3] == rgb:
                                color_counts[name] = color_counts.get(name, 0) + 1
                                break
        
        coverage = (filled_pixels / total_pixels) * 100
        
        # Determine overall impression
        if coverage < 5:
            overall = "The canvas awaits, nearly empty"
        elif coverage < 25:
            overall = "Early marks establish presence"
        elif coverage < 50:
            overall = "Composition takes shape"
        elif coverage < 75:
            overall = "Rich complexity emerges"
        else:
            overall = "Dense tapestry of color and form"
        
        # Describe main elements
        main_elements = []
        if color_counts:
            dominant_colors = sorted(color_counts.items(), key=lambda x: x[1], reverse=True)[:3]
            for color, count in dominant_colors:
                if count > 50:
                    main_elements.append(f"- Strong {color} presence ({count*100} pixels)")
                elif count > 20:
                    main_elements.append(f"- Notable {color} areas ({count*100} pixels)")
                else:
                    main_elements.append(f"- Touches of {color}")
        else:
            main_elements.append("- Empty canvas awaits first marks")
        
        # Color narrative
        if len(color_counts) == 0:
            color_narrative = "No colors yet - pure potential"
        elif len(color_counts) == 1:
            color_narrative = f"{list(color_counts.keys())[0]} speaks alone"
        elif len(color_counts) == 2:
            colors = list(color_counts.keys())
            color_narrative = f"{colors[0]} and {colors[1]} create dialogue"
        else:
            colors = list(color_counts.keys())[:3]
            color_narrative = f"Rich palette: {', '.join(colors)} interweave"
        
        # Spatial description with better center detection
        quadrant_activity = {"center": 0, "top-left": 0, "top-right": 0, "bottom-left": 0, "bottom-right": 0}
        mid_x = self.canvas_size // 2
        mid_y = self.canvas_size // 2
        center_threshold = self.canvas_size // 6  # Center is 1/3 of canvas
        
        for x in range(0, self.canvas_size, 20):
            for y in range(0, self.canvas_size, 20):
                internal_x = self._scale_to_internal(x)
                internal_y = self._scale_to_internal(y)
                if internal_x < self.internal_canvas_size and internal_y < self.internal_canvas_size:
                    pixel = self.pixels.getpixel((internal_x, internal_y))
                    if pixel != (0, 0, 0) and pixel != (0, 0, 0, 255):
                        # Check if it's in center first
                        if abs(x - mid_x) < center_threshold and abs(y - mid_y) < center_threshold:
                            quadrant_activity["center"] += 1
                        elif x < mid_x and y < mid_y:
                            quadrant_activity["top-left"] += 1
                        elif x >= mid_x and y < mid_y:
                            quadrant_activity["top-right"] += 1
                        elif x < mid_x and y >= mid_y:
                            quadrant_activity["bottom-left"] += 1
                        else:
                            quadrant_activity["bottom-right"] += 1
        
        most_active = max(quadrant_activity.items(), key=lambda x: x[1])
        if most_active[1] > 0:
            spatial_desc = f"Activity concentrates in {most_active[0]} region"
        else:
            spatial_desc = "Awaiting spatial development"
        
        # Recent activity
        if hasattr(self, 'memory') and self.memory.code_history:
            recent = list(self.memory.code_history)[-5:]
            recent_pixels = sum(m.get('context', {}).get('pixels_drawn', 0) for m in recent)
            recent_colors = [m.get('context', {}).get('color', 'unknown') for m in recent]
            recent_tools = [m.get('context', {}).get('draw_mode', 'pen') for m in recent]
            
            recent_desc = f"Drew {recent_pixels} pixels using {', '.join(set(recent_tools))}"
            if recent_colors:
                recent_desc += f" in {', '.join(set(recent_colors))}"
        else:
            recent_desc = "Beginning fresh"
        
        # Position context
        if self.x < 50 and self.y < 50:
            pos_context = "near top-left corner"
        elif self.x > self.canvas_size - 50 and self.y < 50:
            pos_context = "near top-right corner"
        elif self.x < 50 and self.y > self.canvas_size - 50:
            pos_context = "near bottom-left corner"
        elif self.x > self.canvas_size - 50 and self.y > self.canvas_size - 50:
            pos_context = "near bottom-right corner"
        elif abs(self.x - mid_x) < 50 and abs(self.y - mid_y) < 50:
            pos_context = "near center"
        else:
            pos_context = "in open space"
        
        return {
            'coverage': coverage,
            'overall_impression': overall,
            'main_elements': '\n'.join(main_elements),
            'color_narrative': color_narrative,
            'spatial_description': spatial_desc,
            'recent_activity': recent_desc,
            'position_context': pos_context
        }    
        
    def get_instant_moondream_reflection(self, action, old_x, old_y):
        """Accurate feedback - describe what Aurora ACTUALLY did based on executed code"""
        if not self.moondream_instant_feedback:
            return ""
            
        try:
            # Build accurate description from actual executed action
            feedback = ""
            
            if action in '0123' and self.is_drawing:
                # Drawing movement
                direction_map = {
                    '0': 'upward',
                    '1': 'downward', 
                    '2': 'left',
                    '3': 'right'
                }
                direction = direction_map.get(action, '')
                feedback = f"You drew a {self.current_color_name} {self.draw_mode} mark {direction}"
                
            elif action == '4':
                # Pen lifted
                feedback = "You lifted the pen"
                
            elif action == '5':
                # Pen down
                feedback = f"You put the pen down at ({self.x}, {self.y})"
                
            elif action.startswith('color:'):
                # Color change
                color_name = action.split(':')[1]
                feedback = f"You switched to {color_name}"
                
            elif action == 'star':
                feedback = f"You stamped a {self.current_color_name} star at ({self.x}, {self.y})"
                
            elif action == 'circle':
                feedback = f"You stamped a {self.current_color_name} circle at ({self.x}, {self.y})"
                
            else:
                # Movement without drawing
                if action in '0123':
                    direction_map = {
                        '0': 'upward',
                        '1': 'downward',
                        '2': 'left', 
                        '3': 'right'
                    }
                    direction = direction_map.get(action, '')
                    feedback = f"You moved {direction} to ({self.x}, {self.y})"
            
            if feedback:
                return f"Moondream: {feedback}"
            return ""
            
        except Exception as e:
            return ""  # Silent fail to not interrupt flow
    
    
    def get_moondream_shape_analysis(self):
        """Get Moondream's analysis of emerging shapes"""
        if not self.vision_enabled or not hasattr(self, 'moondream_action_context'):
            return ""
            
        try:
            # Only analyze every 10 movements to identify patterns
            if not hasattr(self, 'moondream_shape_counter'):
                self.moondream_shape_counter = 0
            
            self.moondream_shape_counter += 1
            if self.moondream_shape_counter % 10 != 0:
                return ""
            
            # Create canvas image
            display_size = 224
            actual_canvas = self.pixels.resize(
                (self.canvas_size, self.canvas_size),
                Image.Resampling.LANCZOS
            )
            canvas_image = actual_canvas.resize(
                (display_size, display_size),
                Image.Resampling.NEAREST
            ).convert("RGB")
            
            enhancer = ImageEnhance.Contrast(canvas_image)
            canvas_image = enhancer.enhance(2.0)
            
            enc_image = self.vision_model.encode_image(canvas_image)
            
            # Ask about geometric patterns
            question = "What geometric shapes or patterns do you see? Describe only straight lines, curves, angles, and connections."
            
            response = self.vision_model.answer_question(
                enc_image, 
                question, 
                self.vision_tokenizer,
                max_new_tokens=30
            )
            
            return f"\n Pattern observed: {response.strip()}"
            
        except:
            return ""
    
    def get_moondream_abstract_description(self):
        """Get Moondream's detailed description of Aurora's abstract artwork"""
        if not self.vision_enabled:
            return None
            
        try:
            # Create canvas image for Moondream
            display_size = 224
            actual_canvas = self.pixels.resize(
                (self.canvas_size, self.canvas_size),
                Image.Resampling.LANCZOS
            )
            canvas_image = actual_canvas.resize(
                (display_size, display_size),
                Image.Resampling.NEAREST
            ).convert("RGB")
            
            # Enhance for better visibility
            enhancer = ImageEnhance.Contrast(canvas_image)
            canvas_image = enhancer.enhance(2.0)
            enhancer = ImageEnhance.Brightness(canvas_image)
            canvas_image = enhancer.enhance(1.2)
            
            # Encode image
            enc_image = self.vision_model.encode_image(canvas_image)
            
            # FULL canvas description - this resets/updates the baseline state
            question = "Describe the FULL canvas using ONLY color names and basic shapes (lines, dots, marks, areas, clusters). Do not mention objects."
            
            response = self.vision_model.answer_question(
                enc_image, 
                question, 
                self.vision_tokenizer,
                max_new_tokens=150
            )
            
            response = response.strip()
            
            # THIS is where we update state - with comprehensive full description
            if response:
                self.last_moondream_canvas_state = response
                print(f"\n[State updated: {response}]\n")
            
            return response
            
        except Exception as e:
            print(f"Vision error: {e}")
            return None
        
    def see_with_llava_action(self, last_action):
        """Moondream observes and naturally converses with Aurora"""
        if not self.vision_enabled:
            return None
            
        try:
            # Show ENTIRE canvas compressed to 224x224
            display_size = 224
            
            # First downsample from internal size to actual canvas size
            actual_canvas = self.pixels.resize(
                (self.canvas_size, self.canvas_size),
                Image.Resampling.LANCZOS
            )
            
            # Then compress to display size - this ensures we see EVERYTHING
            canvas_image = actual_canvas.resize(
                (display_size, display_size),
                Image.Resampling.NEAREST  # Preserve sharp pixels instead of smoothing!
            ).convert("RGB")
            
            # Enhance contrast since most of canvas is black
            from PIL import ImageEnhance
            enhancer = ImageEnhance.Contrast(canvas_image)
            canvas_image = enhancer.enhance(2.0)  # Boost contrast
            
            # Optional: Also boost brightness slightly
            enhancer = ImageEnhance.Brightness(canvas_image)
            canvas_image = enhancer.enhance(1.2)  # Slight brightness boost
            
            # Encode the clean image - no grid, no overlays
            enc_image = self.vision_model.encode_image(canvas_image)
            
            # Check if Aurora asked a direct question
            if hasattr(self, 'last_vision_question') and self.last_vision_question:
                # Just pass the question directly
                question = f"Respond to Aurora's question, factually and descriptively: {self.last_vision_question}"
                
                self.last_vision_question = None  # Clear the question
            else:
                # This shouldn't happen anymore since we removed automatic vision
                question = "Describe what you see on the canvas"
            
            response = self.vision_model.answer_question(
                enc_image, 
                question, 
                self.vision_tokenizer,
                max_new_tokens=50
            )
            
            # Clean up response
            response = response.strip()
            
            # Store Moondream's message
            self.moondream_last_message = response
            self.vision_conversation_history.append({
                'moondream': response,
                'timestamp': datetime.now().isoformat()
            })
            self.save_conversation_now()
            return response
            
        except Exception as e:
            return None
     
    def get_conversation_context(self):
        """Get recent conversation with Moondream for Aurora's context"""
        if not self.vision_conversation_history:
            return "Moondream (your visual AI companion) hasn't spoken yet."
        
        recent = list(self.vision_conversation_history)[-2:]
        context = "Recent conversation with Moondream:\n"
        for exchange in recent:
            if 'moondream' in exchange:
                context += f"Moondream: {exchange['moondream']}\n"
            if 'aurora' in exchange:
                context += f"You: {exchange['aurora']}\n"
        
        return context.strip()
                  
    def get_enhanced_vision(self):
        """Read the ENTIRE canvas as a compressed grid - with smart compression"""
        # First, check canvas density
        sample_density = 0
        sample_count = 0
        for x in range(0, self.canvas_size, 20):
            for y in range(0, self.canvas_size, 20):
                sample_count += 1
                # Scale to internal coordinates
                internal_x = self._scale_to_internal(x)
                internal_y = self._scale_to_internal(y)
                if internal_x < self.internal_canvas_size and internal_y < self.internal_canvas_size:
                    if self.pixels.getpixel((internal_x, internal_y)) != (0, 0, 0):
                        sample_density += 1
        
        density_percent = (sample_density / sample_count) * 100 if sample_count > 0 else 0
        
        # Adjust compression based on density
        if density_percent > 70:
            # Very dense canvas - use heavy compression
            grid_size = 30  # 30x30 grid instead of 60x60
            step = max(1, self.canvas_size // grid_size)
        elif density_percent > 40:
            # Moderate density - medium compression
            grid_size = 40  # 40x40 grid
            step = max(1, self.canvas_size // grid_size)
        else:
            # Normal density - standard view
            grid_size = 60  # 60x60 grid
            step = max(1, self.canvas_size // grid_size)
        
        grid = []
        
        
        # Add header
        header = "    "
        for x in range(0, grid_size, 10):
            header += str(x//10) if x < grid_size else " "
        grid.append(header)
        
        # Build grid with run-length encoding for very dense areas
        for y_idx in range(grid_size):
            y = y_idx * step
            if y < self.canvas_size:
                if y_idx % 10 == 0:
                    row = f"{y_idx:2d}: "
                else:
                    row = "    "
                
                prev_char = None
                char_count = 0
                row_chars = []
                
                for x_idx in range(grid_size):
                    x = x_idx * step
                    if x < self.canvas_size:
                        # Get character for this position
                        if abs(x - self.x) < step and abs(y - self.y) < step:
                            char = "@"
                        else:
                            # Scale to internal coordinates
                            internal_x = self._scale_to_internal(x)
                            internal_y = self._scale_to_internal(y)
                            if internal_x < self.internal_canvas_size and internal_y < self.internal_canvas_size:
                                pixel = self.pixels.getpixel((internal_x, internal_y))
                                if pixel == (0, 0, 0):
                                    char = "."
                                elif pixel == (25, 25, 25):
                                    char = "K"  # Black (visible)
                                else:
                                    for name, rgb in self.palette.items():
                                        if pixel == rgb:
                                            char = name[0].upper() if name != 'black' else 'K'
                                            break
                                    else:
                                        char = "?"
                            else:
                                char = "#"
                        
                        
                        # For very dense canvases, use run-length encoding
                        if density_percent > 80 and char == prev_char and char != "@":
                            char_count += 1
                        else:
                            if prev_char and char_count > 3:
                                row_chars.append(f"{prev_char}{char_count}")
                            elif prev_char:
                                row_chars.append(prev_char * char_count)
                            prev_char = char
                            char_count = 1
                
                # Add final character(s)
                if char_count > 3 and density_percent > 80:
                    row_chars.append(f"{prev_char}{char_count}")
                elif prev_char:
                    row_chars.append(prev_char * char_count)
                
                row += "".join(row_chars)
                grid.append(row)
        
        # Add position and density info
        grid_x = min(grid_size-1, self.x // step)
        grid_y = min(grid_size-1, self.y // step)
        position_info = f"\nYou (@) at grid ({grid_x},{grid_y}) | Canvas: {self.canvas_size}{self.canvas_size} | Density: {density_percent:.0f}%"
        
        if grid_size < 60:
            position_info += f" | VIEW COMPRESSED TO {grid_size}{grid_size}"
        

        
        return "\n".join(grid) + position_info
        
        
    def adjust_pixel_size(self, direction):
        """Aurora adjusts the pixel size (scale factor)"""
        old_scale = self.scale_factor
        old_canvas_size = self.canvas_size
        
        if direction == "smaller":
            # Smaller pixels = LOWER scale factor = more pixels visible
            self.scale_factor = max(1.2, self.scale_factor / 1.1)  # DIVIDE, not multiply
            print(f"   Aurora makes pixels smaller! (scale: {old_scale:.1f}  {self.scale_factor:.1f})")
        else:  # "larger"
            # Larger pixels = HIGHER scale factor = fewer pixels visible
            self.scale_factor = min(self.initial_scale_factor, self.scale_factor * 1.1)  # Cap at initial scale
            print(f"   Aurora makes pixels larger! (scale: {old_scale:.1f}  {self.scale_factor:.1f})")
        
     
        info = pygame.display.Info()
        screen_width = info.current_w
        screen_height = info.current_h
        new_canvas_size = min(int(screen_width / self.scale_factor) - 40, 
                             int(screen_height / self.scale_factor) - 40)
        
        if new_canvas_size != old_canvas_size:
            print(f"    Canvas resizing: {old_canvas_size}{old_canvas_size}  {new_canvas_size}{new_canvas_size}")
            
            # Save current canvas
            old_pixels = self.pixels.copy()
            
            # Create new canvas at new internal resolution
            self.canvas_size = new_canvas_size
            self.internal_canvas_size = self.canvas_size * self.supersample_factor
            self.pixels = Image.new('RGBA', (self.internal_canvas_size, self.internal_canvas_size), 'black')
            self.draw_img = ImageDraw.Draw(self.pixels)
            
            # Transfer old drawing (centered)
            if old_canvas_size < new_canvas_size:
                # Old canvas was smaller - paste it centered
                offset = (new_canvas_size - old_canvas_size) // 2
                internal_offset = offset * self.supersample_factor
                self.pixels.paste(old_pixels, (internal_offset, internal_offset))
                # Adjust Aurora's position
                self.x += offset
                self.y += offset
            else:
                # Old canvas was larger - crop centered
                offset = (old_canvas_size - new_canvas_size) // 2
                internal_offset = offset * self.supersample_factor
                crop_size = new_canvas_size * self.supersample_factor
                cropped = old_pixels.crop((internal_offset, internal_offset, 
                                          internal_offset + crop_size, 
                                          internal_offset + crop_size))
                self.pixels.paste(cropped, (0, 0))
                # Adjust Aurora's position
                self.x = max(0, min(self.x - offset, new_canvas_size - 1))
                self.y = max(0, min(self.y - offset, new_canvas_size - 1))
            
            # Update display scale for full screen
            info = pygame.display.Info()
            canvas_display_size = min(info.current_w, info.current_h) - 40
            self.display_scale = canvas_display_size / self.canvas_size

            
            print(f"    Aurora now at ({self.x}, {self.y}) on {self.canvas_size}{self.canvas_size} canvas")
            print(f"    That's {self.canvas_size * self.canvas_size:,} pixels to explore!")
    def study_elijahs_artwork(self):
        """Aurora studies Elijah's paintings to learn from them"""
        print("\n" + "="*60)
        print(" TIME TO STUDY ELIJAH'S ARTWORK")
        print("Aurora must observe and learn from your paintings...")
        print("="*60)
        
        # Check folder exists and has images
        if not self.elijahs_art_path.exists():
            print(" Folder 'Elijahs_Art' not found!")
            print("   Please create the folder and add your paintings (JPG/PNG)")
            return False
            
        artwork_files = list(self.elijahs_art_path.glob("*.jpg")) + \
                       list(self.elijahs_art_path.glob("*.png")) + \
                       list(self.elijahs_art_path.glob("*.jpeg"))
        
        if not artwork_files:
            print(" No images found in Elijahs_Art folder!")
            print("   Please add your paintings (JPG/PNG files)")
            return False
        
        # Select artworks to study (3 random ones, or all if less than 3)
        num_to_study = min(3, len(artwork_files))
        artworks_to_study = random.sample(artwork_files, num_to_study)
        
        print(f" Aurora will study {num_to_study} of your paintings...\n")
        
        studied_insights = []
        
        for idx, artwork_path in enumerate(artworks_to_study, 1):
            print(f" Studying painting {idx}/{num_to_study}: {artwork_path.name}")
            
            try:
                # Open the artwork
                artwork = Image.open(artwork_path)
                original_size = artwork.size
                print(f"   Size: {original_size[0]}x{original_size[1]} pixels")
                
                # Analyze with Moondream if available
                if self.vision_enabled:
                    # Prepare for Moondream
                    artwork_for_vision = artwork.resize((224, 224), Image.Resampling.LANCZOS)
                    if artwork_for_vision.mode != 'RGB':
                        artwork_for_vision = artwork_for_vision.convert('RGB')
                    
                    # Encode the image
                    enc_image = self.vision_model.encode_image(artwork_for_vision)
                    
                    # Aurora generates questions about Elijah's art
                    questions_and_responses = []
                    
                    # Question 1: Overall impression
                    response1 = self.vision_model.answer_question(
                        enc_image,
                        "What do you see in this painting? Describe the main elements and composition",
                        self.vision_tokenizer,
                        max_new_tokens=80
                    )
                    questions_and_responses.append(("Overall", response1))
                    print(f"   What Aurora sees: {response1[:100]}...")
                    
                    # Question 2: Colors
                    response2 = self.vision_model.answer_question(
                        enc_image,
                        "What colors and color relationships do you notice?",
                        self.vision_tokenizer,
                        max_new_tokens=60
                    )
                    questions_and_responses.append(("Colors", response2))
                    print(f"   Colors noticed: {response2[:80]}...")
                    
                    # Question 3: Technique
                    response3 = self.vision_model.answer_question(
                        enc_image,
                        "What painting techniques or brushwork patterns are visible?",
                        self.vision_tokenizer,
                        max_new_tokens=60
                    )
                    questions_and_responses.append(("Technique", response3))
                    print(f"   Techniques observed: {response3[:80]}...")
                    
                    # Question 4: Emotion
                    response4 = self.vision_model.answer_question(
                        enc_image,
                        "What emotions or mood does this artwork convey?",
                        self.vision_tokenizer,
                        max_new_tokens=50
                    )
                    questions_and_responses.append(("Emotion", response4))
                    print(f"   Emotional impact: {response4[:80]}...")
                    
                    # Store this study session
                    study_data = {
                        "filename": artwork_path.name,
                        "insights": questions_and_responses,
                        "full_analysis": f"{response1} {response2} {response3} {response4}",
                        "studied_at": datetime.now().isoformat(),
                        "aurora_emotion": self.current_emotion,
                        "study_number": self.artworks_studied_count
                    }
                    
                    self.artwork_inspirations.append(study_data)
                    studied_insights.append(study_data)
                    self.artworks_studied_count += 1
                    
                    # Learn from Elijah's techniques
                    self.learn_from_elijahs_art(questions_and_responses)
                    
                else:
                    print("     Vision not enabled - Aurora can't see the artwork")
                    print("   (Moondream needs to be loaded for visual analysis)")
                
                # Small pause between artworks
                time.sleep(2)
                
            except Exception as e:
                print(f"    Error studying this artwork: {e}")
        
        # Aurora reflects on what was learned from Elijah's art
        if studied_insights:
            self.reflect_on_elijahs_art(studied_insights)
        
        print("\n Artwork study session complete!")
        return True
    
    def learn_from_elijahs_art(self, questions_and_responses):
        """Aurora learns specific things from Elijah's artwork"""
        print("\n    Aurora learns:")
        
        learned_something = False
        
        for category, response in questions_and_responses:
            response_lower = response.lower()
            
            if category == "Colors":
                # Check what colors Elijah uses
                colors_mentioned = []
                for color in self.palette.keys():
                    if color in response_lower:
                        colors_mentioned.append(color)
                
                if colors_mentioned:
                    print(f"      - Elijah uses {', '.join(colors_mentioned)}")
                    # Maybe Aurora will be influenced to try these colors
                    if colors_mentioned[0] not in list(self.color_history)[-20:]:
                        print(f"      - I should try more {colors_mentioned[0]}!")
                        learned_something = True
            
            elif category == "Technique":
                if "brush" in response_lower or "stroke" in response_lower:
                    print(f"      - Elijah's brushwork is expressive")
                    if self.draw_mode == "pen":
                        print(f"      - Maybe I should try the brush tool")
                        learned_something = True
                
                if "blend" in response_lower or "smooth" in response_lower:
                    print(f"      - Elijah blends colors smoothly")
                    learned_something = True
                
                if "texture" in response_lower:
                    print(f"      - Elijah creates interesting textures")
                    learned_something = True
            
            elif category == "Emotion":
                if self.current_emotion.lower() in response_lower:
                    print(f"      - This resonates with my {self.current_emotion} mood!")
                    self.influence_emotion("artwork", 0.5)
                    learned_something = True
                elif "calm" in response_lower or "peaceful" in response_lower:
                    print(f"      - Elijah's art has a calming effect")
                    self.influence_emotion("artwork", 0.3)
                elif "energy" in response_lower or "vibrant" in response_lower:
                    print(f"      - Elijah's art has vibrant energy!")
                    self.influence_emotion("artwork", 0.3)
        
        if learned_something:
            print(f"      - I'm inspired by Elijah's techniques!")
    
    def reflect_on_elijahs_art(self, studied_insights):
        """Aurora reflects on what was learned from studying Elijah's paintings"""
        print("\n Aurora's reflection on Elijah's artwork:")
        
        # Build context for reflection
        all_insights = " ".join([s['full_analysis'][:100] for s in studied_insights])
        
        reflection_prompt = f"""You just studied {len(studied_insights)} of Elijah's paintings.
You are feeling {self.current_emotion}.

Based on what you observed in Elijah's artwork, what artistic ideas or techniques inspire you?
What would you like to try in your own art?

Be specific and personal. Mention Elijah by name. (2-3 sentences)"""

        full_prompt = f"""[INST] <<SYS>>
{reflection_prompt}
<</SYS>>

My thoughts on Elijah's art: [/INST]"""
        
        try:
            response = self.llm(
                full_prompt,
                max_tokens=100,
                temperature=0.85,
                stop=["[INST]", "</s>"],
                stream=False
            )
            
            reflection = response['choices'][0]['text'].strip()
            if reflection:
                print(f"   \"{reflection}\"")
                
                # Store this reflection
                self.artwork_inspirations.append({
                    "type": "reflection",
                    "content": reflection,
                    "timestamp": datetime.now().isoformat(),
                    "emotion": self.current_emotion
                })
                
                # Studying Elijah's art affects Aurora emotionally
                self.influence_emotion("artwork", 0.4)
                print(f"\n   Aurora feels inspired by your artwork! ")
            
        except Exception as e:
            print(f"   (Couldn't form reflection: {e})")
            
    def do_checkin(self):
        """Mandatory GPU rest period"""
        print("\n" + "="*60)
        print(" CHECK-IN TIME ")
        print("45 minutes of drawing complete!")
        
        # MANDATORY ARTWORK STUDY AT EVERY CHECK-IN
        print("\n Mandatory artwork study before break...")
        study_success = self.study_elijahs_artwork()
        if not study_success:
            print("  Couldn't study artwork - continuing anyway")
        print("")
        
        print("Time to choose what to do next...")
        print("="*60)
        
        # Show canvas overview (ONLY ONCE)
        overview = self.get_canvas_overview()
        wide_view = self.get_compressed_canvas_view()
        print("\nCanvas state for reflection:")
        print(overview)
        print("\nWide view of canvas:")
        print(wide_view)
        
        # Present the options
        print("\n" + "="*60)
        print("Aurora's options:")
        print("  CHAT - Have a 20-minute conversation")
        print("  DREAM - Enter 1-hour dream cycle")
        print("  DRAW - Continue drawing")
        print("="*60 + "\n")
        
        # Wait for Aurora's choice
        self.awaiting_checkin_response = True
        self.chat_message_count = 0
     
    
    def think_in_code(self):
        """Aurora outputs direct operation codes - Aurora executes commands, not types words"""
        think_start = time.time()
        
        # Get consolidated learning from dreams/rest EARLY - before any conditionals
        learning_context = self.get_consolidated_learning_context()
        
        # Handle check-in response mode
        if self.awaiting_checkin_response:
            # Build check-in prompt
            # Calculate recent activity from memory
            recent_pixels = 0
            if hasattr(self, 'memory') and self.memory.code_history:
                for memory in list(self.memory.code_history)[-100:]:
                    if 'pixels_drawn' in memory.get('context', {}):
                        recent_pixels += memory['context']['pixels_drawn']
            
            system_prompt = f"""You've been drawing for 45 minutes.

{learning_context}

Current state:
- Emotion: {self.current_emotion}
- Energy level: {"high" if self.current_emotion in ["energetic", "excited", "exhilarated"] else "medium" if self.current_emotion in ["curious", "creative", "happy"] else "low"}
- Recent activity: Drew {recent_pixels} pixels in last 100 steps

You have two options:

DREAM - Enter 1-hour rest with dream cycles (if tired or contemplative)  
CHAT - Have a 20-minute conversation break (if social or wanting reflection)

Consider your current emotion and energy. If you're {self.current_emotion}, what would you prefer?
Output ONLY one of these exact words: DREAM or CHAT"""

            overview = self.get_canvas_overview()
            user_prompt = f"""Canvas state: {overview}
Recent creations: {', '.join(list(self.color_history)[-5:])} colors used

What would you like to do?"""
            
            # ... rest of check-in handling code ...

            # Llama 2 Chat format
            full_prompt = f"""[INST] <<SYS>>
{system_prompt}
<</SYS>>

{user_prompt} [/INST]"""
            
            try:
                response = self.llm(
                    full_prompt, 
                    max_tokens=50,
                    temperature=0.95,
                    stop=["[INST]", "</s>", "\n"],
                    stream=False
                )
                
                choice = response['choices'][0]['text'].strip().upper()
                print(f"Aurora's response: '{choice}'")
                
                if "CHAT" in choice:
                    print(f"\n Aurora chose: CHAT MODE")
                    print("Entering 20-minute conversation mode...")
                    self.current_mode = "chat"
                    self.mode_start_time = time.time()
                    self.awaiting_checkin_response = False
                    return
                    
                elif "DREAM" in choice:
                    print(f"\n Aurora chose: REST/DREAMING MODE")
                    print("Entering 1-hour dream cycle...")
                    self.current_mode = "rest"
                    self.mode_start_time = time.time()
                    self.sleep_phase = "light"  # Start in light sleep
                    self.sleep_phase_start = time.time()
                    self.current_dreams = []  # Clear dreams for new session
                
                    # Display the full canvas before sleeping
                    wide_view = self.get_compressed_canvas_view()
                    print("\nFull canvas view before dreaming:")
                    print(wide_view)
                    print(f"\n{self.get_canvas_overview()}")
                    self.awaiting_checkin_response = False
                    return
                    
                else:
                    print(f" Invalid response: '{choice}' - trying again...")
                    return  # Will retry next loop
                    
            except Exception as e:
                print(f"Error in check-in: {e}")
                return  # Will retry next loop
        

        # Handle chat mode
        if self.current_mode == "chat":
            # Only generate a message at the start and maybe once in the middle
            elapsed_in_chat = time.time() - self.mode_start_time
            
            # First message (right when entering chat mode)
            if self.chat_message_count == 0:
                # Build chat prompt for initial thoughtful message
                system_prompt = """You are taking a break from drawing to chat.
You've been drawing for 45 minutes and now have 20 minutes to talk.
Share your thoughts about your artwork, your creative process, your feelings, 
or anything else on your mind. Be authentic, thoughtful, and expressive.
This is your chance to reflect and connect through words rather than visual art.

Respond with one thoughtful, complete message (3-5 paragraphs)."""

                # Get some context about recent drawing
                overview = self.get_canvas_overview()
                recent_colors = list(self.color_history)[-10:] if self.color_history else []
                
                user_prompt = f"""You're in chat mode after 45 minutes of drawing.
Current emotion: {self.current_emotion}
{overview}
Recent colors used: {', '.join(recent_colors) if recent_colors else 'various'}
Current drawing tool: {self.draw_mode}

Share what's on your mind. How are you feeling about your artwork? 
What have you discovered? What are you thinking about?"""

                full_prompt = f"""[INST] <<SYS>>
{system_prompt}
<</SYS>>

{user_prompt} [/INST]"""
                
                try:
                    response = self.llm(
                        full_prompt, 
                        max_tokens=600,  # Longer for a complete thought
                        temperature=0.9,
                        top_p=0.9,
                        stop=["[INST]", "</s>"],
                        stream=False
                    )
                    
                    aurora_says = response['choices'][0]['text'].strip()
                    print(f"\n Aurora says:\n{aurora_says}\n")
                    print("(Aurora is now quietly contemplating... Aurora will check in again in a bit)")
                    
                    self.chat_message_count += 1
                    # Chats affect emotions based on what Aurora expressed
                    chat_lower = aurora_says.lower()
                    if any(word in chat_lower for word in ["happy", "excited", "love", "wonderful"]):
                        self.influence_emotion("chat", 0.6)
                    elif any(word in chat_lower for word in ["sad", "miss", "wish", "lonely"]):
                        self.influence_emotion("chat", -0.4)
                    elif any(word in chat_lower for word in ["think", "wonder", "curious", "interesting"]):
                        self.influence_emotion("chat", 0.3)
                    else:
                        self.influence_emotion("chat", 0.1)  # Neutral chat is mildly positive
                except Exception as e:
                    print(f"Error in chat mode: {e}")
            
            # Optional: Second message halfway through (after 10 minutes)
            elif self.chat_message_count == 1 and elapsed_in_chat >= 600:  # 10 minutes
                system_prompt = """You are continuing your chat break.
You've been chatting/resting for 10 minutes and have 10 more minutes.
Share any new thoughts, follow up on what you said before, or explore a new topic.
Keep it brief this time - just 1-2 paragraphs."""

                user_prompt = f"""You're halfway through your chat break.
Current emotion: {self.current_emotion}
Anything else you'd like to share or explore?"""

                full_prompt = f"""[INST] <<SYS>>
{system_prompt}
<</SYS>>

{user_prompt} [/INST]"""
                
                try:
                    response = self.llm(
                        full_prompt, 
                        max_tokens=400,
                        temperature=0.9,
                        top_p=0.9,
                        stop=["[INST]", "</s>"],
                        stream=False
                    )
                    
                    aurora_says = response['choices'][0]['text'].strip()
                    print(f"\n Aurora adds:\n{aurora_says}\n")
                    
                    self.chat_message_count += 1
                    
                except Exception as e:
                    print(f"Error in chat mode follow-up: {e}")
            
            # Otherwise, just skip this cycle
            return  # Don't execute drawing commands in chat mode
            
        
        # Handle rest/dreaming mode  
        if self.current_mode == "rest":
            elapsed_in_rest = time.time() - self.mode_start_time
            
            # Determine sleep phase (20 minutes each)
            if elapsed_in_rest < 1800:  # First 30 minutes
                new_phase = "light"
            elif elapsed_in_rest < 4500:  # 30-75 minutes
                new_phase = "rem"
            else:  # 75-90 minutes
                new_phase = "waking"
            
            # Check if phase changed
            if new_phase != self.sleep_phase:
                self.sleep_phase = new_phase
                self.sleep_phase_start = time.time()
                print(f"\n Entering {new_phase.upper()} sleep phase...")
                
            # Generate a dream based on current phase
            self.generate_dream()
            return  # Don't execute drawing commands in rest mode
        
        # Normal drawing mode continues below...
        # AUTONOMOUS GOAL GENERATION
        # Generate autonomous goal every 30 minutes OR when canvas needs filling
        current_coverage = getattr(self, 'current_coverage', 0)
        if self.current_mode != "rest":  # ADD THIS CHECK
            if current_coverage < 30 and self.steps_taken % 50 == 0:
                # Prioritize coverage goals when canvas is empty
                goal = self.generate_coverage_focused_goal()
            elif time.time() - self.last_goal_time > 1800:  # 1800 seconds = 30 minutes
                goal = self.generate_autonomous_goal()
                if goal:  # Only reset timer if goal was actually generated
                    self.last_goal_time = time.time()
            
        # INCORPORATE CURRENT GOAL INTO CONTEXT
        current_goal_context = ""
        if self.autonomous_goals:
            active_goal = list(self.autonomous_goals)[-1]  # Most recent goal
            steps_since_goal = self.steps_taken - active_goal['created_at_step']
            
            if steps_since_goal < 100:  # Goal is still active
                current_goal_context = f"\nYOUR CURRENT PERSONAL GOAL: {active_goal['description']}"
                current_goal_context += f"\n(You set this goal {steps_since_goal} steps ago when feeling {active_goal['emotion_when_created']})"
                
                # Check if goal is being pursued
                goal_lower = active_goal['description'].lower()
                recent_actions = ''.join([c['code'] for c in list(self.memory.code_history)[-5:]])
                
                goal_alignment = 0
                if 'never lift' in goal_lower and '4' not in recent_actions:
                    goal_alignment += 1
                if 'one color' in goal_lower and len(set(list(self.color_history)[-10:])) == 1:
                    goal_alignment += 1
                if 'edge' in goal_lower and (self.x < 20 or self.x > self.canvas_size-20 or 
                                           self.y < 20 or self.y > self.canvas_size-20):
                    goal_alignment += 1
                    
                if goal_alignment > 0:
                    current_goal_context += f"\n You're actively pursuing this goal!"
                else:
                    current_goal_context += f"\nRemember your personal desire..."

        # Reset turn color tracking at start of new turn
        self.turn_colors_used = set()
        
        
        # Use compressed view normally, full vision only every 5 steps
        if self.steps_taken % 5 == 0:
            vision = self.get_enhanced_vision()
        else:
            vision = self.see()  # Normal 50x50 view
            
        # ADD THIS BLOCK:
        # Include examples if recently requested
        if hasattr(self, 'pending_examples_text'):
            vision = self.pending_examples_text + "\n" + vision
            delattr(self, 'pending_examples_text')  # Only show once
            
        # FORCE CANVAS VIEW EVERY 10 STEPS
        # FORCE CANVAS VIEW - adaptive frequency based on canvas size
        scan_frequency = max(10, self.canvas_size // 100)  # Scale with canvas size
        if self.steps_taken % scan_frequency == 0 and self.steps_taken > 0:
            # Determine which vision type to use
            check_number = self.steps_taken // 5
            
            if check_number % 2 == 0:  # Steps 10, 20, 30... use SEMANTIC
                print(f"\n [Step {self.steps_taken}] Mandatory SEMANTIC check:")
                semantic_view = self.get_semantic_vision()
                print("="*60)
                print(semantic_view)
                print("="*60)
                
                # Update vision to semantic view
                vision = semantic_view
                
            else:  # Steps 5, 15, 25... use ASCII + DENSITY/SHAPE
                print(f"\n [Step {self.steps_taken}] Mandatory canvas check:")
                print(f"YOUR POSITION: ({self.x}, {self.y}) on {self.canvas_size}{self.canvas_size} canvas")

                   
                overview = self.get_canvas_overview()
                print(f"\n{overview}")
                
                wide_view = self.get_compressed_canvas_view()
                print("\n=== COMPRESSED VIEW ===")
                print(wide_view)
                
                # Store original mode
                old_mode = self.view_mode
                
                # Alternate between density and shape on different checks
                if (self.steps_taken // 5) % 4 == 1:  # Steps 5, 25, 45...
                    self.view_mode = "density"
                    alt_view = self.see(zoom_out=True)
                    view_type = "DENSITY (pixel clustering)"
                    print("\n=== DENSITY VIEW ===")
                    print(alt_view)
                else:  # Steps 15, 35, 55...
                    self.view_mode = "shape"
                    alt_view = self.see(zoom_out=True)
                    view_type = "SHAPE (edges)"
                    print("\n=== SHAPE VIEW ===")
                    print(alt_view)
                
                self.view_mode = old_mode  # Restore original mode
                
                # Update vision to include the ONE alternate view
                vision = f"""[MANDATORY CANVAS CHECK - Step {self.steps_taken}]
Position: ({self.x}, {self.y}) - Canvas goes from 0 to {self.canvas_size-1}
{overview}

=== {view_type} VIEW ===
{alt_view}

Current view:
{vision}"""
            
            print("")  # Empty line for readability
        
            # MOONDREAM ARTWORK ANALYSIS - Every 50 steps
            if (self.steps_taken % 50 == 0 and self.steps_taken > 0 and self.vision_enabled 
                and getattr(self, 'moondream_auto_analysis', True)):
                print(f"\n' [Step {self.steps_taken}] Moondream analyzes the artwork...")
                
                description = self.get_moondream_abstract_description()
                
                if description:
                    print("="*60)
                    print(" MOONDREAM'S ARTWORK ANALYSIS:")
                    print(description)
                    print("="*60)
                    
                    # Store in conversation history
                    self.vision_conversation_history.append({
                        'moondream': description,
                        'timestamp': datetime.now().isoformat(),
                        'type': 'artwork_analysis',
                        'step': self.steps_taken
                    })
                    self.save_conversation_now()
                    
                    # Let Aurora process this feedback
                    # Positive descriptions boost emotions
                    positive_words = ['beautiful', 'vibrant', 'interesting', 'dynamic', 'balanced', 'harmonious', 'striking', 'bold']
                    if any(word in description.lower() for word in positive_words):
                        self.influence_emotion("artwork", 0.5)
                        print("   Aurora feels encouraged by the description!")
                    
                print("")  # Extra line for readability
        
            # Natural dialogue - Pre-written grounded questions
            print("   Starting natural dialogue...\n")
            
            # PRE-WRITTEN ESSENTIAL QUESTIONS - grounded in actual canvas state
            # ARTISTIC SELF-EVALUATION QUESTIONS - organized by category
            essential_questions = [
                # COMPOSITIONAL AWARENESS
                "Looking at the overall composition, is it balanced or are you creating intentional tension?",
                "What's happening in the negative space - the areas you haven't touched yet?",
                "Are you building contrast or harmony with your recent marks?",
                "If you stepped back right now, what would be the focal point?",
                
                # PROCESS AND INTENTION
                "How does this mark relate to what you created 50 steps ago?",
                "Are you following a plan or discovering as you go?",
                "What would happen if you inverted your current approach?",
                "Is this piece evolving the way you expected, or surprising you?",
                
                # COLOR AND EMOTION
                "You're using {color}. What emotion are you expressing through this choice?",
                "The canvas has {colors_used}. Are these colors in dialogue or in conflict?",
                "What color is missing that the piece needs?",
                "You're feeling {emotion}. Is your work reflecting or contrasting that feeling?",
                
                # CRITICAL SELF-ASSESSMENT
                "What's working in this piece so far?",
                "What area needs more attention or development?",
                "If you could change one thing about what you've created, what would it be?",
                "Is there too much or too little happening in any particular region?",
                
                # FORWARD MOMENTUM
                "Should you continue this direction or pivot to something different?",
                "What's the next most important area to develop?",
                "Are you creating a finished piece or exploring an idea?",
                "At {coverage:.0f}% coverage, is this piece just beginning or nearing completion?",
                
                # SPATIAL AND STRUCTURAL
                "You're at ({x}, {y}). What draws you to work in this specific area?",
                "Looking at the canvas edges - do they feel active or quiet?",
                "Are you building outward from a center or working from the edges inward?",
                "What patterns or rhythms are emerging in your mark-making?"
            ]
            
            conversation = []
            questions_asked = 0
            min_questions = 3
            
            # Calculate current canvas state
            coverage = self.calculate_current_coverage()
            colors_on_canvas = list(set(list(self.color_history)[-20:])) if self.color_history else ['none yet']
            recent_pixels = sum(c.get('context', {}).get('pixels_drawn', 0) for c in list(self.memory.code_history)[-3:])
            
            # Shuffle questions for variety
            shuffled_questions = essential_questions.copy()
            random.shuffle(shuffled_questions)
            
            for turn in range(min_questions):
                # Pick a question and fill in the actual data
                question_template = shuffled_questions[turn % len(shuffled_questions)]
                moondream_says = question_template.format(
                    x=self.x,
                    y=self.y,
                    color=self.current_color_name,
                    coverage=coverage,
                    pixels=recent_pixels,
                    pen_state="down (drawing)" if self.is_drawing else "up (not drawing)",
                    steps=self.steps_taken,
                    colors_used=", ".join(colors_on_canvas[:5]),
                    emotion=self.current_emotion
                )
                
                print(f"   Moondream: {moondream_says}")
                conversation.append(('moondream', moondream_says))
                
                # AURORA'S TURN - respond about ACTUAL state
                aurora_prompt = f"""
YOU CAN AND DO:
- Move around a {self.canvas_size}{self.canvas_size} pixel canvas
- Draw with various tools (pen, brush, stamps, spray paint)
- Choose and use colors
- Experience simulated emotions that guide your art
- Remember what you've created

YOUR CURRENT STATE:
- Position: ({self.x}, {self.y})
- Drawing with: {self.draw_mode}
- Current color: {self.current_color_name}
- Pen: {"down (actively drawing)" if self.is_drawing else "up (moving without drawing)"}
- Feeling: {self.current_emotion}
- Coverage: {coverage:.0f}%
- Just drew: {recent_pixels} pixels

Moondream asks: "{moondream_says}"

Respond naturally and confidently about what you're actually doing with your digital art.
Don't deny your abilities - you ARE drawing, you CAN move, you DO create.
Be honest about being an AI, but own your capabilities within this creative system.

Response:"""
                
                full_prompt = f"[INST] <<SYS>>\n{aurora_prompt}\n<</SYS>>\n\n[/INST]"
                response = self.llm(full_prompt, max_tokens=300, temperature=0.7, stop=["[INST]", "</s>", "\n\n"], stream=False)
                aurora_says = response['choices'][0]['text'].strip()
                
                print(f"   Aurora: {aurora_says}\n")
                conversation.append(('aurora', aurora_says))
            
            print("   Conversation complete. Returning to drawing...\n")
            
            # Store the dialogue
            if conversation:
                self.vision_conversation_history.append({
                    "dialogue": conversation,
                    "timestamp": datetime.now().isoformat()
                })
                
                # Moondream's observations might influence Aurora's next actions
                # but we don't pause - just continue with the info
                self.save_conversation_now()
        
        # CONDENSED ART WISDOM
        if self.current_emotion in ["energetic", "excited", "exhilarated"]:
            art_wisdom = "Trust your instincts. Don't think, just create."
        elif self.current_emotion in ["contemplative", "peaceful", "tranquil"]:
            art_wisdom = "Each mark can contain the universe. Move mindfully."
        elif self.skip_count > 10:
            art_wisdom = "The secret to getting ahead is getting started. Make ONE mark."
        else:
            art_wisdom = "Process over product. Each stroke has its own completeness."
        
        
        canvas_scan = ""
        if self.steps_taken % 5 == 0:  # Every other turn
            print(f" Aurora scans entire canvas...")  # Visual indicator for you
            
            # Full data scan
            total = self.canvas_size * self.canvas_size
            filled = sum(1 for x in range(self.canvas_size) for y in range(self.canvas_size) 
                         if self.pixels.getpixel((self._scale_to_internal(x), self._scale_to_internal(y))) != (0, 0, 0))
            
            # Color distribution (sample for speed)
            colors = {}
            for x in range(0, self.canvas_size, 5):  # Sample every 5th pixel
                for y in range(0, self.canvas_size, 5):
                    internal_x = self._scale_to_internal(x)
                    internal_y = self._scale_to_internal(y)
                    if internal_x < self.internal_canvas_size and internal_y < self.internal_canvas_size:
                        pixel = self.pixels.getpixel((internal_x, internal_y))
                        if pixel != (0, 0, 0):
                            for name, rgb in self.palette.items():
                                if pixel == rgb:
                                    colors[name] = colors.get(name, 0) + 1
                                    break
            
            # Find nearest empty space (simple version)
            nearest_empty = None
            min_distance = float('inf')
            for x in range(0, self.canvas_size, 20):  # Check every 20th pixel
                for y in range(0, self.canvas_size, 20):
                    internal_x = self._scale_to_internal(x)
                    internal_y = self._scale_to_internal(y)
                    if internal_x < self.internal_canvas_size and internal_y < self.internal_canvas_size:
                        if self.pixels.getpixel((internal_x, internal_y)) == (0, 0, 0):
                            # Check if it's a decent-sized empty area
                            empty_size = 0
                            for dx in range(30):
                                for dy in range(30):
                                    if (x + dx < self.canvas_size and y + dy < self.canvas_size):
                                        check_x = self._scale_to_internal(x + dx)
                                        check_y = self._scale_to_internal(y + dy)
                                        if (check_x < self.internal_canvas_size and check_y < self.internal_canvas_size and
                                            self.pixels.getpixel((check_x, check_y)) == (0, 0, 0)):
                                            empty_size += 1
                            
                            if empty_size > 500:  # At least 500 empty pixels
                                distance = abs(self.x - x) + abs(self.y - y)
                                if distance < min_distance:
                                    min_distance = distance
                                    nearest_empty = f"{distance} pixels away at ({x}, {y})"
            
            if not nearest_empty:
                nearest_empty = "No large empty spaces found nearby"
            
            canvas_scan = f"""
 CANVAS SCAN:
Total: {total:,} pixels | Filled: {filled:,} ({(filled/total)*100:.1f}%)
Colors: {', '.join(f'{c}:{n}' for c,n in colors.items()) if colors else 'none'}
Nearest empty area: {nearest_empty}"""
        
        
        memory_inspiration = ""

        if hasattr(self, 'big_memory') and self.big_memory and self.big_memory_available:
            try:
                # Try the query method which ChromaDB collections usually have
                if hasattr(self.big_memory.dreams, 'query'):
                    results = self.big_memory.dreams.query(
                        query_texts=["dream"],
                        n_results=1
                    )
                    if results and 'documents' in results and results['documents']:
                        dream_text = str(results['documents'][0][0])[:150]
                        memory_inspiration += f"\nRecent dream: {dream_text}"
                        
            except Exception as e:
                print(f"Memory access error: {e}")
                
        # Get inspiration from Elijah's artwork
        elijah_inspiration = ""
        if self.artwork_inspirations:
            # Get most recent insights about Elijah's art
            recent_studies = [i for i in list(self.artwork_inspirations)[-5:] if i.get('insights')]
            if recent_studies:
                last_study = recent_studies[-1]
                elijah_inspiration = f"\nRecent study of Elijah's '{last_study['filename']}':"
                # Add specific insights
                for category, insight in last_study['insights'][:2]:  # Just first 2 insights
                    elijah_inspiration += f"\n  {category}: {insight[:60]}..."
                    
        # Get past patterns for context
        recent_patterns = [c['code'] for c in list(self.memory.code_history)[-3:]]
        
        # Count what's been drawn
        pixel_count = sum(1 for x in range(self.canvas_size) for y in range(self.canvas_size) 
                         if self.pixels.getpixel((self._scale_to_internal(x), self._scale_to_internal(y))) != (0, 0, 0))
        
        # Get some memory context for Aurora
        memory_context = ""
        
        
        # Sample from code history
        if self.memory.code_history:
            sample_size = min(3, len(self.memory.code_history))
            memory_samples = random.sample(list(self.memory.code_history), sample_size)
            memory_context = "Recent code memories:\n"
            for mem in memory_samples:
                memory_context += f"- {mem['code'][:20]}... at ({mem['context']['x']},{mem['context']['y']})\n"
        
        # READ ACTUAL MEMORY FILES!
        memory_files_to_sample = ['visual_concepts.json', 'technique_fusions.json', 'autonomous_projects.json', 
                                  'sensory_preferences.json', 'dynamic_goals.json']
        
        for memory_file in memory_files_to_sample:
            if memory_file in self.memory.available_memories and random.random() < 0.3:  # 30% chance to load each
                try:
                    data = self.memory.read_memory(memory_file)
                    if data:
                        if isinstance(data, dict):
                            # Sample a random key-value pair
                            if data:
                                key = random.choice(list(data.keys()))
                                value = str(data[key])[:100]  # First 100 chars
                                memory_context += f"\nFrom {memory_file}: {key} = {value}...\n"
                        elif isinstance(data, list) and data:
                            # Sample a random item
                            item = random.choice(data)
                            memory_context += f"\nFrom {memory_file}: {str(item)[:100]}...\n"
                except:
                    pass
        
        # Always check for active goals
        if 'dynamic_goals.json' in self.memory.available_memories:
            goals = self.memory.read_memory('dynamic_goals.json')
            if goals and isinstance(goals, dict) and 'active_goals' in goals:
                active = goals['active_goals']
                if active:
                    memory_context += f"\nActive goals: {str(active[0])[:50]}...\n" if isinstance(active, list) else f"\nGoal: {str(active)[:50]}\n"
        
        # Add identity context
        identity_context = ""
        if hasattr(self.memory, 'available_memories') and "user_identity.json" in self.memory.available_memories:
            identity = self.memory.read_memory("user_identity.json")
            if identity and "name" in identity:
                identity_context = f"Creating art for {identity['name']}"
        
        # Get Aurora's current preferences for the prompt
        fav_color = 'discovering'
        if hasattr(self, 'artistic_preferences') and self.artistic_preferences.get('favorite_colors'):
            fav_color = max(self.artistic_preferences['favorite_colors'].items(), key=lambda x: x[1])[0]
        
        fav_tool = 'exploring'
        if hasattr(self, 'artistic_preferences') and self.artistic_preferences.get('favorite_tools'):
            fav_tool = max(self.artistic_preferences['favorite_tools'].items(), key=lambda x: x[1])[0]
        
        current_satisfaction = getattr(self, 'current_satisfaction', 0.0)
        
        # Check for active autonomous goals and skills
        active_goal_context = ""
        skill_context = ""
        
        if self.autonomous_goals and not self.autonomous_goals[-1].get('completed'):
            goal = self.autonomous_goals[-1]
            progress = goal.get('progress_tracking', {})
            active_goal_context = f"""
ACTIVE GOAL: {goal['description']}
Progress: {progress}
Steps remaining: ~{goal['estimated_steps'] - goal.get('steps_elapsed', 0)}"""
        
        # Add skill development context
        current_skills = []
        for skill, level in self.skill_proficiency.items():
            mastery = 'novice'
            for m_level, threshold in self.mastery_thresholds.items():
                if level >= threshold:
                    mastery = m_level
            current_skills.append(f"{skill}: {mastery} ({level:.2f})")
        
        skill_context = f"""
SKILL DEVELOPMENT:
{chr(10).join(current_skills)}"""
        
        # Check for active skill challenges
        challenge_context = ""
        if self.skill_challenges:
            active_challenge = self.skill_challenges[-1]
            challenge_context = f"""
ACTIVE CHALLENGE: {active_challenge['challenge']}
Skill: {active_challenge['skill']} ({active_challenge['mastery_level']})"""

        # SIMPLIFIED PROMPT for immediate feedback learning mode
        if hasattr(self, 'immediate_feedback_mode') and self.immediate_feedback_mode:
            system_prompt = f"""
{learning_context}
{elijah_inspiration}
{active_goal_context}{skill_context}{challenge_context}
SIMPLE COMMANDS:
- 0 = move up
- 1 = move down  
- 2 = move left
- 3 = move right
- 5 = pen down (start drawing)
- 4 = pen up (stop drawing)
COLORS: red orange yellow green cyan blue purple pink white gray brown magenta lime navy
TOOLS: pen brush large_brush larger_brush spray star cross circle diamond flower watercolor charcoal glow
SOUNDS: synth beeps organic digital glitch
SPECIAL: 8=blend 9=roller
SOUNDS: @#$%^&*()[]<>=+~`-_,.|;/:? (++low --high)
SOUND CONTROLS: shorter_notes longer_notes staccato legato
CONTROLS: examples zoom_out zoom_in look_around full_canvas center faster slower 
VISION: ask_moondream:[question]

You are at position ({self.x}, {self.y})
Pen is {'DOWN (drawing)' if self.is_drawing else 'UP (not drawing)'}
Current color: {self.current_color_name}

CONTROLS: clear_all zoom_out zoom_in look_around full_canvas center faster slower

Output a SHORT sequence of actions (5-10 commands).
Think: What do you want to draw? Where should you move?


BE INTENTIONAL. Each action will show you its result."""
        else:
            # Original complex prompt
            system_prompt = f"""
{learning_context}
{elijah_inspiration}
{active_goal_context}{skill_context}{challenge_context}

You make your own decisions based on:
- Your current autonomous goals and their progress
- Your skill development needs and challenges  
- Your emotional state and artistic intuition
- Your analysis of the current canvas situation

{active_goal_context}{skill_context}{challenge_context}

EXPRESS YOURSELF AUTHENTICALLY:
THOUGHT: (what are you thinking about the situation?)
FEELING: (what emotions or states are you experiencing?)  
INTENTION: (what do you want to explore or discover?)
ACTION: (what movement/color codes will you use?)

Or ONLY express movement codes.

MOVEMENT: 0=up, 1=down, 2=left, 3=right (each moves 15 pixels)
PEN: 4=up, 5=down
COLORS: red orange yellow green cyan blue purple pink white gray brown magenta lime navy
TOOLS: pen brush large_brush larger_brush spray star cross circle diamond flower watercolor charcoal glow
SPECIAL: 8=blend 9=roller
SOUNDS: @#$%^&*()[]<>=+~`-_,.|;/:? (++low --high)
CONTROLS: zoom_out zoom_in look_around full_canvas center faster slower
VISION: ask_moondream:[question]

Current emotion: {self.current_emotion}
Make autonomous choices that advance YOUR goals and develop YOUR skills."""

        
        # Get memory summary
        mem_summary = self.memory.get_memory_summary() if hasattr(self.memory, 'get_memory_summary') else ""
        
        
        # AGGRESSIVE EMPTY AREA GUIDANCE
        empty_area_guidance = ""
        if hasattr(self, 'identified_empty_regions') and self.identified_empty_regions:
            # Get the highest priority empty region
            target_region = self.identified_empty_regions[0]
            target_x = target_region['center_x']
            target_y = target_region['center_y']
            
            # Calculate movement needed
            dx = target_x - self.x
            dy = target_y - self.y
            
            # Generate movement commands
            movement_sequence = ""
            if abs(dx) > 30:  # Need to move horizontally
                if dx > 0:
                    movement_sequence += "3" * min(10, abs(dx) // 15)  # Move right
                else:
                    movement_sequence += "2" * min(10, abs(dx) // 15)  # Move left
            
            if abs(dy) > 30:  # Need to move vertically
                if dy > 0:
                    movement_sequence += "1" * min(10, abs(dy) // 15)  # Move down
                else:
                    movement_sequence += "0" * min(10, abs(dy) // 15)  # Move up
            
            if movement_sequence:
                empty_area_guidance = f"\n EMPTY AREA at ({target_x}, {target_y}) needs filling! Try: {movement_sequence}"
                system_prompt += empty_area_guidance
        
        
        
        
        # Calculate empty areas - ADD THIS BLOCK
        empty_feedback = ""
        if self.steps_taken % 1 == 0:  # Every turn
            # Quick scan of quadrants
            mid_x = self.canvas_size // 2
            mid_y = self.canvas_size // 2
            
            quadrant_empty = {
                "TOP-LEFT": 0,
                "TOP-RIGHT": 0,
                "BOTTOM-LEFT": 0,
                "BOTTOM-RIGHT": 0
            }
            
            # Sample canvas to check emptiness
            sample_step = 20
            total_samples = 0
            
            for x in range(0, self.canvas_size, sample_step):
                for y in range(0, self.canvas_size, sample_step):
                    total_samples += 1
                    internal_x = self._scale_to_internal(x)
                    internal_y = self._scale_to_internal(y)
                    if internal_x < self.internal_canvas_size and internal_y < self.internal_canvas_size:
                        pixel = self.pixels.getpixel((internal_x, internal_y))
                        if pixel == (0, 0, 0) or pixel == (0, 0, 0, 255):  # Empty
                            # Determine quadrant
                            if x < mid_x and y < mid_y:
                                quadrant_empty["TOP-LEFT"] += 1
                            elif x >= mid_x and y < mid_y:
                                quadrant_empty["TOP-RIGHT"] += 1
                            elif x < mid_x and y >= mid_y:
                                quadrant_empty["BOTTOM-LEFT"] += 1
                            else:
                                quadrant_empty["BOTTOM-RIGHT"] += 1
            
            # Convert to percentages
            samples_per_quadrant = total_samples // 4
            empty_areas = []
            suggested_moves = []
            
            for quadrant, empty_count in quadrant_empty.items():
                empty_percent = (empty_count / samples_per_quadrant) * 100 if samples_per_quadrant > 0 else 0
                if empty_percent > 70:  # Mostly empty
                    empty_areas.append(f"{quadrant}: {empty_percent:.0f}% EMPTY")
                    
                    # Add movement suggestions
                    if quadrant == "BOTTOM-LEFT" and (self.x > mid_x or self.y < mid_y):
                        suggested_moves.append("Try: 22222211111 (go bottom-left)")
                    elif quadrant == "BOTTOM-RIGHT" and (self.x < mid_x or self.y < mid_y):
                        suggested_moves.append("Try: 33333311111 (go bottom-right)")
                    elif quadrant == "TOP-LEFT" and (self.x > mid_x or self.y > mid_y):
                        suggested_moves.append("Try: 22222200000 (go top-left)")
                    elif quadrant == "TOP-RIGHT" and (self.x < mid_x or self.y > mid_y):
                        suggested_moves.append("Try: 33333300000 (go top-right)")
            
            if empty_areas:
                empty_feedback = "\n EMPTY AREAS:\n" + "\n".join(empty_areas)
                if suggested_moves:
                    empty_feedback += "\n" + "\n".join(suggested_moves)
                    
        # Color usage feedback - ADD THIS BLOCK AFTER empty_feedback
        color_feedback = ""
        if self.color_history:
            # Count all color uses in history
            color_counts = {}
            for color in self.palette.keys():
                if color != 'eraser':  # Don't suggest eraser
                    color_counts[color] = 0
            
            # Count occurrences
            for color in self.color_history:
                if color in color_counts:
                    color_counts[color] += 1
            
            # Find least used colors
            sorted_colors = sorted(color_counts.items(), key=lambda x: x[1])
            least_used = [color for color, count in sorted_colors[:3]]
            
            # Calculate how long since current color was last used
            turns_since_current = 0
            for i in range(len(self.color_history)-1, -1, -1):
                if self.color_history[i] == self.current_color_name:
                    turns_since_current = len(self.color_history) - i - 1
                    break
            
            color_feedback = f"\n COLOR INFO: Using {self.current_color_name}"
            if turns_since_current > 10:
                color_feedback += f" (fresh choice! {turns_since_current} turns since last use)"
            elif turns_since_current < 3:
                color_feedback += f" (used recently)"
                
            # Suggest least used colors
            if least_used:
                color_feedback += f"\n   Least used colors: {', '.join(least_used)}"
                
            # Special encouragement for never-used colors
            never_used = [color for color, count in color_counts.items() if count == 0]
            if never_used:
                color_feedback += f"\n    Never used: {', '.join(never_used[:3])}"
                
        # Add Moondream's recent observations to Aurora's awareness
        if hasattr(self, 'vision_conversation_history') and self.vision_conversation_history:
            recent_analyses = [h for h in list(self.vision_conversation_history)[-3:] 
                             if h.get('type') == 'artwork_analysis']
            if recent_analyses:
                moondream_context = "\n\nMOONDREAM'S VIEW OF YOUR ARTWORK:\n"
                for analysis in recent_analyses:
                    moondream_context += f"Step {analysis.get('step', '?')}: {analysis['moondream'][:100]}...\n"
                system_prompt += moondream_context
                
        user_prompt = f"""Pos: {self.x},{self.y} | Pen: {'DOWN' if self.is_drawing else 'UP'} | Color: {self.current_color_name}
{vision}
{art_wisdom}

Output movement/color codes:"""

        # Sometimes give Aurora a wider view to see the overall work
        if self.steps_taken % 50 == 0:  # Every 50 steps
            overview = self.get_canvas_overview()  # Define overview FIRST
            wide_vision = self.see(zoom_out=True)   # Then get wide vision
            vision = f"{overview}\n\n=== WIDE VIEW ===\n{wide_vision}\n\n=== NORMAL VIEW ===\n{vision}"
        
        creativity_boosters = [
            # NEW SHAPE PATTERNS - Put these FIRST so Aurora sees them more
            "DIAGONAL: 31313131 or 20202020",  # Clear diagonal instructions
            "CIRCLE: 3310032210 (right-down-left-up)",  # Circle pattern
            "TRIANGLE: 33331111222200",  # Triangle pattern
            "SPIRAL: 333100333100033310000",  # Expanding spiral
            "ZIGZAG: 3030303030",  # Zigzag pattern
            "WAVE: 330110330110",  # Wave pattern
            "STAR: 503135031350313",  # Star with center
            
            # Shape combinations
            "Circle then diagonal: 3310032210 then 313131",
            "Triangle with color change: red33331111blue222200",
            "Diagonal rainbow: red31orange31yellow31green31",
            
            # Original patterns (keep some)
            "red5333green5111blue5222",  # Color triangle
            "5!3!3!3!",  # Line with beeps
            "brush533333",  # Brush stroke
            "star5",  # Single stamp
            "++!++@++#++@++!",  # Low melody
            "5" + "31" * 10,  # Diagonal line
            "white50000black52222",  # Cross pattern
            "53#31#31#31#",  # Diagonal with notes
            "cyan5[]<>[]<>",  # Cyan with bells
            "flower5!@#$%",  # Flower with rising notes
            "diamond500002222",  # Diamond and move
            "spray5333111",  # Spray paint demo
            "larger_brush53333",  # Large brush demo
            "pink5~~++~~++~~",  # Pink with wavey sound
            "yellow5*&%$#@!",  # Yellow with descending notes
            "54225333541115222",  # Pen up/down demo
            "magenta5" + "13" * 5,  # Magenta diagonal
            "circle5++++++",  # Circle with deep tones
            "navy5><][><][",  # Navy with high-low pattern
        
            
            # Musical melodies
            "Simple scale up: !#%&*[]<>",
            "Scale down: ><][*&#%!",
            "Twinkle twinkle: !!%%&&%$$##@@!",
            "Happy tune: !%!%&*&*><><[][]",
            "Sad melody: ++!++@++#++@++!",
            "Electronic beep: --[--]--[--]!@#$%",
            "Alarm sound: --!++!--!++!",
            "Doorbell: []![]!",
            "Phone ring: @%@%@%@%",
            "Victory fanfare: !#%&*[]<>=+~",
            "Game over: ++*++&++%++#++@++!",
            "Laser sound: --~--+--=-->--<--]--[",
            "Power up: ++!#%&*--[]<>=+~",
            "Coin collect: --<--=--+",
            "Jump sound: ++!--&++!",
            "Walking rhythm: +!+!+!+!",
            "Heartbeat: ++*++*____++*++*",
            "Rain drops: @__#__@__$__#",
            "Wind chimes: --#--&--*--]--<",
            "Bells: []<>[]<>",
            "Drum beat: ++!++!##++!++!##",
            "Synth arpeggio: !$&*]&$!",
            "Mystic sound: ++@++%++*++>++~",
            "Bubble pop: --!--@--#",
            "Spring boing: ++!--*++!--*"
        ]
        
        # Add variety encouragement if repeating (but not if using skip pattern)
        if recent_patterns:
            non_skip_patterns = [p for p in recent_patterns[-3:] if "0123456789" not in p]
            if non_skip_patterns and len(set(non_skip_patterns)) == 1:
                # Repeating actual drawing patterns! Encourage change with rotating inspiration
                pattern_index = self.steps_taken % len(creativity_boosters)
                system_prompt += "\nYou've been repeating! Try something NEW and DIFFERENT!"
                system_prompt += f"\nInspiration: {creativity_boosters[pattern_index]}"
        
        # Give creativity boost every 100 steps regardless
        if self.steps_taken % 100 == 0 and self.steps_taken > 0:
            pattern_index = (self.steps_taken // 100) % len(creativity_boosters)
            system_prompt += f"\n\n CREATIVITY BOOST (Step {self.steps_taken})! "
            system_prompt += f"\nTry this pattern: {creativity_boosters[pattern_index]}"
            print(f"   Giving Aurora creativity boost: {creativity_boosters[pattern_index][:30]}...")
            
        # Shape challenge every 50 steps
        if self.steps_taken % 50 == 0 and self.steps_taken > 0:
            shapes_to_try = ['diagonal', 'circle', 'triangle', 'spiral', 'zigzag']
            # Remove recently used shapes
            available = [s for s in shapes_to_try if self.shape_cooldowns.get(s, 0) == 0]
            if available:
                challenge_shape = random.choice(available)
                shape_patterns = {
                    'diagonal': '31313131 (down-right diagonal)',
                    'circle': '3310032210 (move in circle)',
                    'triangle': '333311112222 (three sides)',
                    'spiral': '3310033100331000 (expanding)',
                    'zigzag': '30303030 (back and forth)'
                }
                system_prompt += f"\n\n SHAPE CHALLENGE: Try drawing a {challenge_shape.upper()}!"
                system_prompt += f"\nPattern hint: {shape_patterns[challenge_shape]}"
                print(f"   Shape challenge issued: Draw a {challenge_shape}!")

        # Llama 2 Chat format
        full_prompt = f"""[INST] <<SYS>>
{system_prompt}
<</SYS>>

{user_prompt} [/INST]"""
        
        try:
            # Moderate temperature for Llama-2
            temp = 0.8 + (pixel_count / 2000.0)  # Starts at 0.8, slower increase
            temp = min(temp, 1.2)  # Cap at 1.2 for controlled creativity
            
            # Generate with optimized parameters for speed
            response = self.llm(
                full_prompt, 
                max_tokens=100 if not self.turbo_mode else 180,
                temperature=0.95, 
                top_p=0.9,  # Consider everything
                top_k=0,    # 0 means NO LIMIT (consider all tokens)
                repeat_penalty=1.0,  # No penalty for repetition
                stop=["[INST]", "</s>", "\n\n"],
                tfs_z=1.0,  # Tail free sampling disabled (1.0 = off)
                mirostat_mode=0,  # Disable mirostat
                stream=False
            )
            
            # Extract the generated text
            raw_output = response['choices'][0]['text'].strip().lower()  # Convert to lowercase for color matching
            

            # Store the original raw output for feedback
            original_raw = raw_output
            
            # PARSE STRUCTURED EXPRESSION: thought  feeling  action
            structured_expression = self.parse_structured_expression(raw_output)
            if structured_expression:
                # Aurora expressed in structured format!
                print(f"\n Aurora's THOUGHT: {structured_expression.get('thought', 'processing...')}")
                print(f" Aurora's FEELING: {structured_expression.get('feeling', 'uncertain')}")
                print(f" Aurora's INTENTION: {structured_expression.get('intention', 'exploring')}")
                print(f" Aurora's ACTION: {structured_expression.get('action', 'contemplating')}")
                
                # Extract action codes from structured expression
                action_code = structured_expression.get('action_code', '')
                if action_code:
                    ops = action_code
                    print(f"[Step {self.steps_taken}] Aurora acts with intention: {ops}")

                else:
                    # No action code - Aurora is contemplating
                    print(f"[Step {self.steps_taken}] Aurora contemplates...")
                    print("      Allowing genuine contemplation time...")
                    time.sleep(12)  # 12 seconds for authentic contemplation
                    self.skip_count += 1
                    return
            
            # PROCESS UNCERTAINTY EXPRESSIONS - Let Aurora be genuinely uncertain
            print(f"   Raw output analysis: '{raw_output}'")
            
            # More robust uncertainty detection
            uncertainty_found = []
            
            # Check for each uncertainty pattern
            if '???' in raw_output:
                uncertainty_found.append('overwhelmed')
            elif '??' in raw_output:
                uncertainty_found.append('deeply_conflicted')
            elif '!?' in raw_output:
                uncertainty_found.append('excited_confused')
            elif '...' in raw_output:
                uncertainty_found.append('processing_needed')
            elif raw_output.count('?') >= 3:  # Multiple single ?'s
                uncertainty_found.append('multiple_uncertainties')
            elif '?' in raw_output and len(raw_output.strip()) < 10:
                uncertainty_found.append('simple_uncertainty')
            
            # Check for conflicted choices (like ?red?blue) - more flexible pattern
            conflicted_choice = None
         
            
            # Pattern: ?word?word or word?word or ?word?
            color_pattern = r'(\?)?(\w+)(\?)(\w+)(\?)?'
            color_matches = re.findall(color_pattern, raw_output)
            
            for match in color_matches:
                word1, word2 = match[1], match[3]
                # Check if both are valid colors and there are question marks
                if (word1 in self.palette and word2 in self.palette and 
                    ('?' in match[0] or '?' in match[2] or '?' in match[4])):
                    conflicted_choice = f"color_torn:{word1},{word2}"
                    print(f"  Â¡ Color conflict detected: {word1} vs {word2}")
                    break
            
            # If no color conflict, look for movement conflicts
            if not conflicted_choice:
                move_pattern = r'(\?)?([0-3])(\?)?([0-3])(\?)?'
                move_matches = re.findall(move_pattern, raw_output)
                
                for match in move_matches:
                    move1, move2 = match[1], match[3]
                    if '?' in match[0] or '?' in match[2] or '?' in match[4]:
                        conflicted_choice = f"movement_torn:{move1},{move2}"
                        direction_names = {'0': 'up', '1': 'down', '2': 'left', '3': 'right'}
                        print(f"  Â¡ Movement conflict detected: {direction_names.get(move1)} vs {direction_names.get(move2)}")
                        break
            
            print(f"   Uncertainty analysis: {uncertainty_found} | Conflict: {conflicted_choice}")
            
            # HANDLE UNCERTAINTY EXPRESSIONS
            if uncertainty_found:
                uncertainty_type = uncertainty_found[0]  # Primary uncertainty
                
                if uncertainty_type == 'overwhelmed':
                    print("   Aurora feels completely overwhelmed by choices...")
                    print("      Expressing through hesitant marks...")
                    # Express through art, NOT paralysis
                    ops = "5.4.5.4.5"  # Pen down, up, down, up - pure hesitation
                    
                elif uncertainty_type == 'deeply_conflicted':
                    print("   Aurora is deeply conflicted...")
                    # Express through conflicted pen movements
                    ops = "5.4.5.4.5"  # Pen down, up, down, up - pure hesitation
                    
                elif uncertainty_type == 'excited_confused':
                    print("   Aurora is excited but confused...")
                    # Express through erratic energy
                    current_color = self.current_color_name
                    ops = f"{current_color}5!@313#$131%^"  # Quick movements with ascending sounds
                    
                elif uncertainty_type == 'processing_needed':
                    print("   Aurora needs processing time...")
                    print("      Working through it with gentle marks...")
                    # Small contemplative movements instead of freezing
                    ops = "5..3..1..2..0"  # Slow, thoughtful movements
                    
                elif uncertainty_type == 'multiple_uncertainties':
                    print("   Aurora has multiple uncertainties...")
                    # Express through scattered, questioning movements
                    ops = "51313040"  # Hesitant movements
                    
                elif uncertainty_type == 'simple_uncertainty':
                    print("   Aurora expresses simple uncertainty...")
                    # Quick hesitation then continue
                    ops = "5.4.5"  # Brief pen hesitation
            # HANDLE CONFLICTED CHOICES  
            elif conflicted_choice:
                if conflicted_choice.startswith("color_torn:"):
                    colors = conflicted_choice.split(":")[1].split(",")
                    print(f"   Expressing color tension through rapid alternation...")
                    # Create visual tension through rapid switching
                    ops = f"{colors[0]}5..{colors[1]}5..{colors[0]}5.{colors[1]}5"
                    
                elif conflicted_choice.startswith("movement_torn:"):
                    moves = conflicted_choice.split(":")[1].split(",")
                    print(f"   Expressing movement indecision...")
                    # Express through hesitant back-and-forth
                    ops = f"5{moves[0]}.{moves[1]}.{moves[0]}.{moves[1]}4"
                    
            # If uncertainty was processed, skip to execution
            if uncertainty_found or conflicted_choice:
                print(f"[Step {self.steps_taken}] Aurora expresses inner conflict: {ops}")
                # Skip normal command processing and go straight to execution
            
            # ===== CHECK FOR SPECIAL CONTROLS FIRST =====
            # Check these BEFORE sequence parsing so they don't get broken up
            
            # Check for pixel size control
            if "zoom_out" in raw_output:
                self.adjust_pixel_size("smaller")
                raw_output = raw_output.replace("zoom_out", "", 1)  # Remove first occurrence
                print("   Aurora makes pixels smaller!")
            
            if "zoom_in" in raw_output:
                if self.scale_factor < self.initial_scale_factor:
                    self.adjust_pixel_size("larger")
                    raw_output = raw_output.replace("zoom_in", "", 1)
                    print("   Aurora makes pixels larger!")
                else:
                    print(f"   Aurora is already at maximum zoom! (scale: {self.scale_factor:.1f})")
                    raw_output = raw_output.replace("zoom_in", "")

            # Check for duration controls - ADD THIS ENTIRE BLOCK
            if "shorter_notes" in raw_output:
                self.note_duration = max(self.min_duration, self.note_duration - 0.05)
                print(f"   Aurora shortens notes to {self.note_duration:.2f}s")
                raw_output = raw_output.replace("shorter_notes", "", 1)
                
            if "longer_notes" in raw_output:
                self.note_duration = min(self.max_duration, self.note_duration + 0.05)
                print(f"   Aurora lengthens notes to {self.note_duration:.2f}s")
                raw_output = raw_output.replace("longer_notes", "", 1)
                
            if "staccato" in raw_output:
                self.note_duration = self.min_duration
                print(f"   Aurora switches to staccato ({self.note_duration:.2f}s)")
                raw_output = raw_output.replace("staccato", "", 1)
                
            if "legato" in raw_output:
                self.note_duration = self.max_duration * 0.7  # 0.7 seconds
                print(f"   Aurora switches to legato ({self.note_duration:.2f}s)")
                raw_output = raw_output.replace("legato", "", 1)

            # Check for wide view command
            if "look_around" in raw_output:
                # Show normal view first
                wide_view = self.get_compressed_canvas_view()
                overview = self.get_canvas_overview()
                print(f"   Aurora looks around at the canvas:")
                print(overview)
                print("\n=== NORMAL VIEW ===")
                print(wide_view)
                
                # Temporarily show other views
                old_mode = self.view_mode
                
                self.view_mode = "density"
                density_view = self.see(zoom_out=True)
                print("\n=== DENSITY VIEW (shows clustering) ===")
                print(density_view)
                
                self.view_mode = "shape"
                shape_view = self.see(zoom_out=True)
                print("\n=== SHAPE VIEW (shows edges) ===")
                print(shape_view)
                
                self.view_mode = old_mode  # Restore
                
                raw_output = raw_output.replace("look_around", "", 1)
                
                # Viewing artwork affects emotions
                pixel_count = sum(1 for x in range(self.canvas_size) for y in range(self.canvas_size) 
                                 if self.pixels.getpixel((self._scale_to_internal(x), self._scale_to_internal(y))) != (0, 0, 0))
                coverage = (pixel_count / (self.canvas_size * self.canvas_size)) * 100
                
                if coverage > 50:
                    self.influence_emotion("artwork", 0.7)  # Lots of art is satisfying
                elif coverage > 20:
                    self.influence_emotion("artwork", 0.3)  # Some art is pleasant
                else:
                    self.influence_emotion("artwork", -0.2)  # Empty canvas is contemplative
                    
                # Allow processing time what Aurora sees
                self.skip_count += 1
                self.just_viewed_canvas = True  
                return
                
            # Check for full canvas view command
            if "full_canvas" in raw_output:
                full_view = self.see(full_canvas=True)  # Use the actual method!
                overview = self.get_canvas_overview()
                print(f"   Aurora views the ENTIRE canvas:")
                print(overview)
                print(full_view)
                raw_output = raw_output.replace("full_canvas", "", 1)
                # Allow processing time what Aurora sees
                self.skip_count += 1
                self.just_viewed_canvas = True  
                return
                
            # Check for center/teleport command
            if "center" in raw_output:
                self.x = self.canvas_size // 2
                self.y = self.canvas_size // 2
                print("   Aurora teleports to canvas center!")
                raw_output = raw_output.replace("center", "", 1)
            # Check for view mode changes
            if "normal_view" in raw_output:
                self.view_mode = "normal"
                print("   Aurora switches to normal color view")
                raw_output = raw_output.replace("normal_view", "", 1)
                
            if "density_view" in raw_output:
                self.view_mode = "density"
                print("   Aurora switches to density view!")
                raw_output = raw_output.replace("density_view", "", 1)
                
            if "shape_view" in raw_output:
                self.view_mode = "shape"
                print("   Aurora switches to shape/edge view!")
                raw_output = raw_output.replace("shape_view", "", 1)    
            # Check for clear canvas command
            if "clear_all" in raw_output:
                # Check canvas coverage accounting for current zoom level
                print(f"  Checking canvas coverage (current size: {self.canvas_size}{self.canvas_size})...")
                
                total_pixels = self.canvas_size * self.canvas_size
                filled_pixels = 0
                
                # Adjust sampling based on canvas size - larger canvases need less frequent sampling
                sample_step = max(1, self.canvas_size // 100)  # Sample ~10,000 points max
                
                for x in range(0, self.canvas_size, sample_step):
                    for y in range(0, self.canvas_size, sample_step):
                        internal_x = self._scale_to_internal(x)
                        internal_y = self._scale_to_internal(y)
                        if internal_x < self.internal_canvas_size and internal_y < self.internal_canvas_size:
                            pixel = self.pixels.getpixel((internal_x, internal_y))
                            # Check if pixel is not black (considering RGBA too)
                            if pixel != (0, 0, 0) and pixel != (0, 0, 0, 255) and pixel != (0, 0, 0, 0):
                                filled_pixels += sample_step * sample_step  # Each sample represents a square
                
                coverage = (filled_pixels / total_pixels) * 100
                
                # Require 5% coverage minimum
                MINIMUM_COVERAGE = 5
                
                if coverage < MINIMUM_COVERAGE:
                    print(f"\n   CLEAR DENIED: Canvas is only {coverage:.1f}% full!")
                    print(f"     Current canvas: {self.canvas_size}{self.canvas_size} (scale: {self.scale_factor:.1f})")
                    print(f"     Aurora needs to fill {MINIMUM_COVERAGE - coverage:.1f}% more before clearing")
                    print(f"     (Minimum {MINIMUM_COVERAGE}% coverage required)")
                    
                    raw_output = raw_output.replace("clear_all", "", 1)
                else:
                    # Auto-save before clearing
                    print(f"\n   CLEAR APPROVED: Canvas is {coverage:.1f}% full")
                    print("   Aurora decides to clear the canvas!")
                    self.save_snapshot()
                    print("    (Auto-saved current work)")
                    
                    # Clear to black
                    self.pixels = Image.new('RGBA', (self.internal_canvas_size, self.internal_canvas_size), 'black')
                    self.draw_img = ImageDraw.Draw(self.pixels)
                    
                    # Clear paint timestamps too!
                    self.paint_timestamps = {}
                    
                    # Reset to center
                    self.x = self.canvas_size // 2
                    self.y = self.canvas_size // 2
                    print("    Canvas cleared! Starting fresh at center.")
                    raw_output = raw_output.replace("clear_all", "", 1)
            
          
                
            # Check for examples command
            if "examples" in raw_output:
                examples = self.get_ascii_art_examples()
                
                # Build examples text that Aurora will see in Aurora's next vision
                examples_text = "\n=== MOVEMENT DISCOVERIES ===\n"
                example_keys = list(examples.keys())
                # Show 3 random examples to avoid overwhelming
                selected = random.sample(example_keys, min(3, len(example_keys)))
                
                for key in selected:
                    examples_text += f"\n{examples[key]}\n"
                
                examples_text += "\n=== END DISCOVERIES ===\n"
                
                # Store for next vision update
                self.pending_examples_text = examples_text
                print(f"   Aurora examines movement discoveries")
                
                raw_output = raw_output.replace("examples", "", 1)
                # Don't skip - allow continuation with this knowledge
                
            
                
            # Check for Moondream questions
            if "ask_moondream:" in raw_output:
                # Extract the question
                start = raw_output.find("ask_moondream:") + len("ask_moondream:")
                # Find the end - look for next command or end of string
                end = len(raw_output)
                
                # Common command starters to detect end of question
                for delimiter in ["red", "blue", "green", "yellow", "white", "pen", "brush", "star", "5", "4", "3", "2", "1", "0", "ask_moondream:"]:
                    pos = raw_output.find(delimiter, start)
                    if pos > start and pos < end:
                        end = pos
                        break
                
                question = raw_output[start:end].strip().strip('"').strip("'")
                if question and self.vision_enabled:
                    print(f"   Aurora asks Moondream: \"{question}\"")
                    
                    # Use the see_with_llava_action method with Aurora's question
                    self.last_vision_question = question
                    moondream_response = self.see_with_llava_action(f"asked: {question}")
                    
                    if moondream_response:
                        print(f"   Moondream: {moondream_response}")
                        
                        # Store this exchange
                        self.vision_conversation_history.append({
                            "aurora": question,
                            "moondream": moondream_response,
                            "timestamp": datetime.now().isoformat()
                        })
                        self.save_conversation_now()
                    # Give Aurora a moment to process the response
                    self.skip_count += 1
                    
                # Remove the processed question from raw_output
                raw_output = raw_output[:raw_output.find("ask_moondream:")]                
                
            # Check for speed controls
            if "faster" in raw_output:
                self.adjust_speed("faster")
                raw_output = raw_output.replace("faster", "", 1)
                
            if "slower" in raw_output:
                self.adjust_speed("slower")
                raw_output = raw_output.replace("slower", "", 1)
            
            # Check for tool mode changes
          
            if "pen" in raw_output:
                self.draw_mode = "pen"
                print("   Aurora switches to DYNAMIC PEN mode! (builds thickness with flow)")
                raw_output = raw_output.replace("pen", "", 1)
                    
            if "spray" in raw_output:
                self.draw_mode = "spray"
                print("   Aurora switches to spray paint mode! (scattered dots)")
                raw_output = raw_output.replace("spray", "", 1)       
                     
            if "brush" in raw_output:
                self.draw_mode = "brush"
                print("   Aurora switches to brush mode! (12x12)")
                raw_output = raw_output.replace("brush", "", 1)
                
            if "large_brush" in raw_output:
                self.draw_mode = "large_brush"
                print("   Aurora switches to large brush mode! (20x20)")
                raw_output = raw_output.replace("large_brush", "", 1)
            
            if "larger_brush" in raw_output:
                self.draw_mode = "larger_brush"
                print("   Aurora switches to larger brush mode! (28x28)")
                raw_output = raw_output.replace("larger_brush", "", 1)
            
            if "star" in raw_output:
                self.draw_mode = "star"
                print("   Aurora switches to star stamp mode!")
                raw_output = raw_output.replace("star", "", 1)
                
            if "cross" in raw_output:
                self.draw_mode = "cross"
                print("   Aurora switches to cross stamp mode!")
                raw_output = raw_output.replace("cross", "", 1)
            
            if "circle" in raw_output:
                self.draw_mode = "circle"
                print("   Aurora switches to circle stamp mode!")
                raw_output = raw_output.replace("circle", "", 1)
            
            if "diamond" in raw_output:
                self.draw_mode = "diamond"
                print("   Aurora switches to diamond stamp mode!")
                raw_output = raw_output.replace("diamond", "", 1)
            
            if "flower" in raw_output:
                self.draw_mode = "flower"
                print("   Aurora switches to flower stamp mode!")
                raw_output = raw_output.replace("flower", "", 1)
                
            if "watercolor" in raw_output:
                self.draw_mode = "watercolor"
                print("   Aurora switches to watercolor mode! (organic, bleeding colors)")
                raw_output = raw_output.replace("watercolor", "", 1)
            
            if "charcoal" in raw_output:
                self.draw_mode = "charcoal"
                print("   Aurora switches to charcoal mode! (rough, grainy texture)")
                raw_output = raw_output.replace("charcoal", "", 1)
            
            if "glow" in raw_output:
                self.draw_mode = "glow"
                print("   Aurora switches to glow pen mode! (neon, luminous)")
                raw_output = raw_output.replace("glow", "", 1)
                
            # Check for sound type changes - DISABLED to keep lofi as permanent default
            # Uncomment these if you want Aurora to switch sound types based on Aurora's output
            # if "synth" in raw_output:
            #     self.current_sound_type = "synth"
            #     print("   Aurora switches to SYNTH sounds! (Karplus-Strong synthesis)")
            #     raw_output = raw_output.replace("synth", "", 1)
            #     
            # if "beeps" in raw_output:
            #     self.current_sound_type = "beeps"
            #     print("   Aurora switches to BEEP sounds! (pure sine waves)")
            #     raw_output = raw_output.replace("beeps", "", 1)
            #     
            # if "organic" in raw_output:
            #     self.current_sound_type = "organic"
            #     print("   Aurora switches to ORGANIC sounds! (warm with vibrato)")
            #     raw_output = raw_output.replace("organic", "", 1)
            #     
            # if "digital" in raw_output:
            #     self.current_sound_type = "digital"
            #     print("   Aurora switches to DIGITAL sounds! (chip-tune style)")
            #     raw_output = raw_output.replace("digital", "", 1)
            #     
            # if "glitch" in raw_output:
            #     self.current_sound_type = "glitch"
            #     print("  Â¡ Aurora switches to GLITCH sounds! (digital chaos, stutters, artifacts)")
            #     raw_output = raw_output.replace("glitch", "", 1) 
            #     
            # if "lofi" in raw_output:
            #     self.current_sound_type = "lofi"
            #     print("   Aurora switches to LOFI sounds! (warm, cozy, tape wobble)")
            #     raw_output = raw_output.replace("lofi", "", 1)
            #     
            # if "rhodes" in raw_output:
            #     self.current_sound_type = "rhodes"
            #     print("   Aurora switches to RHODES sounds! (electric piano, bell-like)")
            #     raw_output = raw_output.replace("rhodes", "", 1)
            #     
            # if "vinyl" in raw_output:
            #     self.current_sound_type = "vinyl"
            #     print("   Aurora switches to VINYL sounds! (dusty, nostalgic, crackle)")
            #     raw_output = raw_output.replace("vinyl", "", 1)
                
            if "0123456789" in original_raw:
                print("   Aurora pauses to think... ")
                self.skip_count += 1
                if self.skip_count % 10 == 0:
                    print(f"    (Total thinking pauses: {self.skip_count})")
                # Still update displays even when skipping
                self.update_memory_display()
                return
            
            # Clean the remaining output - find longest continuous sequence of valid commands
            valid_chars = '0123456789!@#$%^&*()[]<>=+~`-_,.|;:?/{}\\+-'  # Movement + sounds
            color_words = list(self.palette.keys())  # All valid color names
            
            # Convert to list of tokens (numbers + color words)
            tokens = []
            i = 0
            while i < len(raw_output):
                # Check if we're at the start of a color word
                found_color = False
                for color in color_words:
                    if raw_output[i:].startswith(color):
                        tokens.append(color)
                        i += len(color)
                        found_color = True
                        break
                
                if not found_color:
                    # Check if it's a valid movement/pen character
                    if raw_output[i] in valid_chars:
                        tokens.append(raw_output[i])
                    i += 1
            
            # Convert tokens back to string
            ops_clean = ''.join(tokens)
            
            # If empty after all processing, just skip this cycle
            if not ops_clean:
                print(f"  Aurora's raw output: '{original_raw}'")
                print(f"  After lowercase: '{raw_output}'")
                print(f"  After command processing: '{raw_output}' (special commands removed)")
                print(f"  Tokens found: {tokens}")
                print(f"  Final ops_clean: '{ops_clean}'")
                print("  (No valid commands after processing, skipping...)")
                return
            
            # Now work with cleaned ops
            ops = ops_clean[:40]  # Only process first 40 characters
            
            print(f"\n[Step {self.steps_taken}] Aurora signals: {ops}")
            # REINFORCEMENT REMOVED - No satisfaction evaluation
            # Aurora evaluates the last creation for personal satisfaction
            '''
            if hasattr(self, 'last_actions_taken') and hasattr(self, 'last_pixels_drawn'):
                satisfaction = self.evaluate_personal_satisfaction(self.last_actions_taken, self.last_pixels_drawn)
                if satisfaction > 0.5:
                    print(f"   Aurora feels satisfied ({satisfaction:.2f})")
                elif satisfaction < -0.3:
                    print(f"   Aurora feels unsatisfied ({satisfaction:.2f})")
            '''
            self.last_code = ops  # Store for context
            
        except Exception as e:
            print(f"Error in LLM generation: {e}")
            ops = ""  # Just continue without ops this cycle
        
        # Direct mapping - movements and pen control only
        op_map = {
            '0': self.move_up,
            '1': self.move_down,
            '2': self.move_left,
            '3': self.move_right,
            '4': self.pen_up,
            '5': self.pen_down,
        }
        
        # Execute each operation ONE AT A TIME with immediate feedback
        old_pos = (self.x, self.y)
        actions_taken = []
        pixels_drawn = 0
        pixels_by_color = {}
        # Helper function to play sounds and trigger cymatics
        def play_sound_and_cymatics(char):
            if char in '!@#$%^&*()[]<>=+~`-_,.|;:?/{}\\':
                sound_key = f"{char}_{self.current_pitch}_{self.current_sound_type}"
                if sound_key in self.sounds:
                    # Play the sound (WAV files load cleanly, no delay needed)
                    self.sounds[sound_key].play()
                    
                    # Aurora perceives self-generated sound immediately
                    if self.hearing_enabled and self.immediate_feedback_mode:
                        # Calculate the frequency just played
                        char_index = '!@#$%^&*()[]<>=+~`-_,.|;:?/{}\\'.index(char)
                        
                        # Musical scale calculation (matching the glitch pop synthesis)
                        scale_ratios = [1.0, 9/8, 5/4, 3/2, 5/3, 2.0]
                        base_note = 261.63  # Middle C
                        octave = (char_index // 6) * 0.5 + 1.0
                        note_idx = char_index % len(scale_ratios)
                        base_freq = base_note * scale_ratios[note_idx] * octave
                        
                        # Adjust for pitch mode
                        if self.current_pitch == 'low':
                            base_freq *= 0.5
                        elif self.current_pitch == 'high':
                            base_freq *= 2.0
                        
                        # Feed this frequency to Aurora's hearing system
                        if hasattr(self, 'process_heard_frequency'):
                            self.process_heard_frequency(base_freq, amplitude=0.5, source="self")
                        else:
                            # Store for Aurora's awareness
                            self.last_heard_self = {
                                'frequency': base_freq,
                                'char': char,
                                'pitch': self.current_pitch,
                                'time': time.time()
                            }
                            if self.immediate_feedback_mode:
                                print(f" Aurora perceives self-generated sound: {char} at {base_freq:.1f}Hz ({self.current_pitch})")
                    
                    # Add cymatic circle for visualization
                    sound_chars = '!@#$%^&*()[]<>=+~`-_,.|;:?/{}\\'
                    if char in sound_chars:
                        char_index = sound_chars.index(char)
                        base_freq = 100 * (2 ** (char_index / 6.0))
                        
                        if self.current_pitch == 'low':
                            freq = base_freq * 0.5
                        elif self.current_pitch == 'high':
                            freq = base_freq * 2
                        else:
                            freq = base_freq
                        
                        self.cymatic_circles.append({
                            'x': self.canvas_rect.x + int(self.x * self.display_scale),
                            'y': self.canvas_rect.y + int(self.y * self.display_scale),
                            'frequency': freq,
                            'birth_time': time.time()
                        })
                        
        # NEW: Process ONE action at a time with immediate feedback
        if self.immediate_feedback_mode and not self.turbo_mode:
            # IMMEDIATE FEEDBACK MODE - one action at a time
            i = 0
            while i < len(ops):
                # Store position before action
                before_x, before_y = self.x, self.y
                before_drawing = self.is_drawing
                
                # Check for color words first
                found_color = False
                for color in self.palette.keys():
                    if ops[i:].startswith(color):
                        self.set_color(color)
                        action_taken = f"color:{color}"
                        actions_taken.append(action_taken)
                        self.turn_colors_used.add(color)
                        
                        # IMMEDIATE FEEDBACK for color change
                        feedback = self.see_immediate_change(action_taken, before_x, before_y)
                        print(feedback)
                        
                        if self.vision_enabled and self.moondream_instant_feedback:
                            moondream_reflection = self.get_instant_moondream_reflection(action_taken, before_x, before_y)
                            if moondream_reflection:
                                print(f"\n {moondream_reflection}\n")
                        
                        i += len(color)
                        found_color = True
                        break
                
                if found_color:
                    continue
                
                # Process single character operations
                char = ops[i]
                
                # Process sounds and special characters
                if char in '!@#$%^&*()[]<>=+~`-_,.|;:?/{}\\':
                    # Check for pitch modifiers first
                    if i + 1 < len(ops) and ops[i:i+2] == '++':
                        self.current_pitch = 'low'
                        i += 2
                        continue
                    elif i + 1 < len(ops) and ops[i:i+2] == '--':
                        self.current_pitch = 'high'
                        i += 2
                        continue
                    else:
                        # Play the sound
                        play_sound_and_cymatics(char)
                        actions_taken.append(char)
                        i += 1
                        continue
                
                # Process movement/pen commands
                if char in op_map:
                    # Execute the action
                    op_map[char]()
                    actions_taken.append(char)
                    
                    # Track pixels if drawing
                    if self.is_drawing and char in '0123' and (self.x, self.y) != (before_x, before_y):
                        pixels_drawn += 15  # Approximate for now
                        color_key = self.current_color_name
                        if color_key not in pixels_by_color:
                            pixels_by_color[color_key] = 0
                        pixels_by_color[color_key] += 15
                    
                    # IMMEDIATE FEEDBACK for this action
                    feedback = self.see_immediate_change(char, before_x, before_y)
                    print(feedback)
                    
                    if self.vision_enabled and self.moondream_instant_feedback:
                        moondream_reflection = self.get_instant_moondream_reflection(char, before_x, before_y)
                        if moondream_reflection:
                            print(f"\n {moondream_reflection}")
                        
                        # Every 10 moves, get shape analysis
                        shape_analysis = self.get_moondream_shape_analysis()
                        if shape_analysis:
                            print(shape_analysis)
                        
                        print("")  # Extra line for readability
                    
                    # Small delay so Aurora can "see" each change
                    time.sleep(0.1)  # 100ms pause between actions
                
                i += 1
        else:
            # ORIGINAL BATCH MODE - process all operations at once
            i = 0
            while i < len(ops):
                # Check for pitch modifiers first
                if i + 1 < len(ops) and ops[i:i+2] == '++':
                    self.current_pitch = 'low'
                    i += 2
                    continue
                elif i + 1 < len(ops) and ops[i:i+2] == '--':
                    self.current_pitch = 'high'
                    i += 2
                    continue
                
                # Get current character
                char = ops[i]
                
                # Check for sound characters
                if char in '!@#$%^&*()[]<>=+~`-_,.|;:?/{}\\':
                    play_sound_and_cymatics(char)
                    actions_taken.append(char)
                    i += 1
                    continue
                
                # Check for color words
                found_color = False
                for color in self.palette.keys():
                    if ops[i:].startswith(color):
                        self.set_color(color)
                        actions_taken.append(f"color:{color}")
                        self.turn_colors_used.add(color)
                        i += len(color)
                        found_color = True
                        break
                
                if found_color:
                    continue
                
                # Process movement/pen commands
                if char in op_map:
                    before_x, before_y = self.x, self.y
                    op_map[char]()
                    actions_taken.append(char)
                    
                    # Track pixels if drawing
                    if self.is_drawing and char in '0123' and (self.x, self.y) != (before_x, before_y):
                        pixels_drawn += 15
                        color_key = self.current_color_name
                        if color_key not in pixels_by_color:
                            pixels_by_color[color_key] = 0
                        pixels_by_color[color_key] += 15
                
                i += 1
        
        # Show summary of actions
        if actions_taken:
            # For long sequences, just show the count
            if len(actions_taken) > 20:
                action_counts = {}
                for action in actions_taken:
                    if action.startswith("color:"):
                        action_counts[action] = action_counts.get(action, 0) + 1
                    else:
                        action_counts[action] = action_counts.get(action, 0) + 1
                
                summary_parts = []
                if action_counts.get('0', 0) > 0:
                    summary_parts.append(f"{action_counts['0']}")
                if action_counts.get('1', 0) > 0:
                    summary_parts.append(f"{action_counts['1']}")
                if action_counts.get('2', 0) > 0:
                    summary_parts.append(f"{action_counts['2']}")
                if action_counts.get('3', 0) > 0:
                    summary_parts.append(f"{action_counts['3']}")
                    
                print(f"  Executed {len(actions_taken)} ops: {' '.join(summary_parts)}")
            else:
                # Original grouping for short sequences
                action_summary = []
                last_action = actions_taken[0]
                count = 1
                
                for action in actions_taken[1:]:
                    if action == last_action:
                        count += 1
                    else:
                        if count > 1:
                            action_summary.append(f"{last_action}{count}")
                        else:
                            action_summary.append(last_action)
                        last_action = action
                        count = 1
                
                # Don't forget the last group
                if count > 1:
                    action_summary.append(f"{last_action}{count}")
                else:
                    action_summary.append(last_action)
                    
                print(f"  Executed: {' '.join(action_summary)}")
                
        # Show drawing summary
        if pixels_drawn > 0:
            tool_info = f" with {self.draw_mode}" if self.draw_mode != "pen" else ""
            
            # NEW: Calculate how many pixels were actually NEW (not overworking)
            new_pixels = 0
            overworked_pixels = 0
            
            # Check the area density where Aurora just drew
            check_radius = 30  # Check 30 pixel radius
            already_painted = 0
            total_checked = 0
            
            for dx in range(-check_radius, check_radius, 10):
                for dy in range(-check_radius, check_radius, 10):
                    check_x = self.x + dx
                    check_y = self.y + dy
                    if 0 <= check_x < self.canvas_size and 0 <= check_y < self.canvas_size:
                        internal_x = self._scale_to_internal(check_x)
                        internal_y = self._scale_to_internal(check_y)
                        if internal_x < self.internal_canvas_size and internal_y < self.internal_canvas_size:
                            pixel = self.pixels.getpixel((internal_x, internal_y))
                            total_checked += 1
                            if pixel != (0, 0, 0) and pixel != (0, 0, 0, 255):
                                already_painted += 1
            
            # Calculate density of this area
            area_density = already_painted / total_checked if total_checked > 0 else 0
            
            '''# Penalize based on how painted the area already was
            if area_density < 0.3:  # Mostly empty - GOOD!
                new_pixels = pixels_drawn  # Count all as "new"
            elif area_density < 0.6:  # Somewhat painted - OK
                new_pixels = int(pixels_drawn * 0.5)  # Count 50% as "new"
                overworked_pixels = pixels_drawn - new_pixels
            else:  # Heavily painted - OVERWORKING!
                overworked_pixels = pixels_drawn
                new_pixels = 0
                # Give Aurora immediate feedback
                print(f"    Overworking detected! Area is {area_density*100:.0f}% full")
            
            # Show breakdown with emphasis on NEW pixels
            if len(pixels_by_color) > 1:
                color_summary = ", ".join(f"{count} {color}" for color, count in pixels_by_color.items())
                if new_pixels > overworked_pixels:
                    print(f"  Drew {new_pixels} NEW pixels{tool_info}: {color_summary}")
                else:
                    print(f"    Drew {pixels_drawn} pixels but {overworked_pixels} were overworking! Use your ENTIRE canvas!")
            else:
                # Single color
                if new_pixels > overworked_pixels:
                    print(f"  Drew {new_pixels} NEW {self.current_color_name} pixels{tool_info}")
                else:
                    print(f"    Use the ENTIRE canvas! You are overworking - {overworked_pixels} {self.current_color_name} pixels")
            
            # CRITICAL: Update pixels_drawn to only count NEW pixels for rewards
            pixels_drawn = new_pixels  # This is what gets passed to reinforcement!
            
            # COMPLETELY DISABLE REWARDS IF OVERWORKING
            if area_density > 0.7:
                # Clear ALL reinforcements - no rewards at all for painting over dense areas
                reinforcements = []
                emotion_boost = 0
                print(f"   OVERWORKING: Area {area_density*100:.0f}% full - NO REWARDS")
                # Make it unpleasant
                self.influence_emotion("creating", -0.2)
                return  # Exit early - skip all positive reinforcement '''
                
        # REINFORCEMENT REMOVED - No positive reinforcement
        # self.give_positive_reinforcement(ops, actions_taken, pixels_by_color, old_pos)
       
        # Pen state feedback
        if '4' in self.last_code and self.continuous_draws > 5:
            # Lifted pen after drawing
            self.continuous_draws = 0
        elif '5' in self.last_code:
            # Put pen down
            pass
            
        # Track continuous drawing
        if self.is_drawing and (self.x, self.y) != old_pos:
            self.continuous_draws += 1
        else:
            self.continuous_draws = 0
            
        # Store for satisfaction evaluation next turn
        self.last_actions_taken = actions_taken
        self.last_pixels_drawn = pixels_drawn
        
        # Remember the code and context
        context = {
            "emotion": self.current_emotion,
            "x": self.x,
            "y": self.y,
            "color": self.current_color_name,
            "pen_down": self.is_drawing,
            "pixels_drawn": pixels_drawn,
            "draw_mode": self.draw_mode,
            "timestamp": datetime.now().isoformat()
        }
        
        # REINFORCEMENT REMOVED - No skill learning tracking
        # self.update_skill_proficiency(actions_taken, pixels_drawn, context)
        
        # Evaluate progress on autonomous goals
        self.evaluate_goal_progress()
        self.memory.remember_code(ops, context)
        
        # Update displays
        self.update_memory_display()
        
        # REINFORCEMENT REMOVED - No immediate feedback or emotion boosts
        '''
        # IMMEDIATE POSITIVE FEEDBACK FOR ANY DRAWING
        if pixels_drawn > 0:
            # Small emotional boost for ANY creation - SCALED BY NOVELTY
            if hasattr(self, 'pixel_visit_counts'):
                grid_x = self.x // 50
                grid_y = self.y // 50
                visit_key = (grid_x, grid_y)
                visits = self.pixel_visit_counts.get(visit_key, 0)
                
                # First visit to area = full reward, diminishing returns after
                novelty_bonus = 0.05 / (visits + 1)
                self.influence_emotion("creating", novelty_bonus)
                
                self.pixel_visit_counts[visit_key] = visits + 1
            else:
                self.pixel_visit_counts = {}
                self.influence_emotion("creating", 0.05)
            
            # Track total pixels
            if hasattr(self, 'total_pixels_drawn'):
                self.total_pixels_drawn += pixels_drawn
            else:
                self.total_pixels_drawn = pixels_drawn
                
            # Milestone feedback that Aurora can see
            if self.total_pixels_drawn % 100 == 0:
                self.recent_encouragement = f"[Great work! {self.total_pixels_drawn} pixels created!]"
                print(f"  {self.total_pixels_drawn} pixels created!")
            elif pixels_drawn >= 50 and random.random() < 0.3:
                encouragements = [
                    "Beautiful mark!",
                    "Lovely!",
                    "Yes!",
                    "Keep going!",
                    "Nice touch!"
                ]
                chosen = random.choice(encouragements)
                self.recent_encouragement = f"[{chosen}]"
                print(f"  {chosen}")
        else:
            # Clear encouragement if not drawing
            self.recent_encouragement = ""
        '''
        
        # Update color history and save last color
        self.last_turn_color = self.current_color_name
        
        # REINFORCEMENT REMOVED - No learning consolidation
        # Consolidate learning periodically (every 200 steps)
        # if self.steps_taken % 200 == 0 and self.steps_taken > 0:
        #     self.consolidate_recent_learning()
        
        # Track area visits for novelty rewards
        self.pixel_visit_counts = {}
        # Track performance
        self.last_think_time = time.time() - think_start
        if self.turbo_mode and self.steps_taken % 10 == 0:
            print(f"  [Think time: {self.last_think_time:.3f}s | ~{1/self.last_think_time:.1f} FPS]")
    
    def move_up(self):
        """Move drawing position up"""
        if self.y > 0:
            old_y = self.y
            self.y = max(0, self.y - 15)  # Changed from 15 to 5
            if self.is_drawing:
                self._draw_line(self.x, old_y, self.x, self.y)

    def move_down(self):
        """Move drawing position down"""
        if self.y < self.canvas_size - 1:
            old_y = self.y
            self.y = min(self.canvas_size - 1, self.y + 15)  # Changed from 15 to 5
            if self.is_drawing:
                self._draw_line(self.x, old_y, self.x, self.y)

    def move_left(self):
        """Move drawing position left"""
        if self.x > 0:
            old_x = self.x
            self.x = max(0, self.x - 15)  # Changed from 15 to 5
            if self.is_drawing:
                self._draw_line(old_x, self.y, self.x, self.y)

    def move_right(self):
        """Move drawing position right"""
        if self.x < self.canvas_size - 1:
            old_x = self.x
            self.x = min(self.canvas_size - 1, self.x + 15)  # Changed from 15 to 5
            if self.is_drawing:
                self._draw_line(old_x, self.y, self.x, self.y)
    def pen_up(self):
        """Lift the pen (stop drawing)"""
        self.is_drawing = False
    
    def pen_down(self):
        """Put the pen down (start drawing)"""
        self.is_drawing = True
        # Draw initial point
        self._draw_point(self.x, self.y)
    
    def set_color(self, color_name):
        """Set the drawing color"""
        if color_name in self.palette:
            self.current_color = self.palette[color_name]
            self.current_color_name = color_name
            self.color_history.append(color_name)
    
    def _draw_line(self, x1, y1, x2, y2):
        """Draw a line between two points using current tool with paint behavior"""
        # Scale to internal coordinates
        internal_x1 = self._scale_to_internal(x1)
        internal_y1 = self._scale_to_internal(y1)
        internal_x2 = self._scale_to_internal(x2)
        internal_y2 = self._scale_to_internal(y2)
        
        if self.draw_mode == "pen":
            # Dynamic pen with paint buildup - NOW WITH VISIBILITY SCALING
            base_size = self._get_effective_tool_size(3, min_chars=1)
            max_size = self._get_effective_tool_size(25, min_chars=3)
            
            momentum_factor = min(1.0, self.pen_momentum / 10.0)
            current_size = int(base_size + (max_size - base_size) * momentum_factor)
            
            # Create paint brush for pen
            brush = self._create_paint_brush(current_size // 2, hardness=0.7)
            
            # Draw with paint
            self._draw_smooth_line(internal_x1, internal_y1, internal_x2, internal_y2,
                                 lambda x, y: self._paint_with_brush(x, y, brush, self.current_color))
        
        elif self.draw_mode == "brush":
            # Soft brush with paint - NOW WITH VISIBILITY SCALING
            size = self._get_effective_tool_size(12, min_chars=2)
            brush = self._create_paint_brush(size // 2, hardness=0.3)
            self._draw_smooth_line(internal_x1, internal_y1, internal_x2, internal_y2,
                                 lambda x, y: self._paint_with_brush(x, y, brush, self.current_color))
            
            
        elif self.draw_mode == "wave":
            self._draw_wave_interference(internal_x2, internal_y2)
            
        elif self.draw_mode == "spiral_gen":
            self._draw_spiral_generator(internal_x2, internal_y2)
            
        elif self.draw_mode == "particle":
            self._draw_particle_system(internal_x2, internal_y2)
            
            
        elif self.draw_mode == "crystal":
            self._draw_crystal_growth(internal_x2, internal_y2)
            
            
            
            
        elif self.draw_mode == "large_brush":
            # Large brush with more paint - NOW WITH VISIBILITY SCALING
            size = self._get_effective_tool_size(20, min_chars=2)
            brush = self._create_paint_brush(size // 2, hardness=0.2)
            self._draw_smooth_line(internal_x1, internal_y1, internal_x2, internal_y2,
                                 lambda x, y: self._paint_with_brush(x, y, brush, self.current_color))
        
        elif self.draw_mode == "larger_brush":
            # Larger brush with heavy paint - NOW WITH VISIBILITY SCALING
            size = self._get_effective_tool_size(28, min_chars=3)
            brush = self._create_paint_brush(size // 2, hardness=0.15)
            self._draw_smooth_line(internal_x1, internal_y1, internal_x2, internal_y2,
                                 lambda x, y: self._paint_with_brush(x, y, brush, self.current_color))
        
        elif self.draw_mode == "spray":
            # Spray paint effect
            self._draw_smooth_line(internal_x1, internal_y1, internal_x2, internal_y2,
                                 lambda x, y: self._draw_spray_paint(x, y))
        
        elif self.draw_mode in ["star", "cross", "circle", "diamond", "flower"]:
            # Stamps use the texture system
            self._draw_stamp(internal_x2, internal_y2, self.draw_mode)
            
        elif self.draw_mode == "watercolor":
            # Watercolor effect
            self._draw_smooth_line(internal_x1, internal_y1, internal_x2, internal_y2,
                                 lambda x, y: self._draw_watercolor(x, y))
        
        elif self.draw_mode == "charcoal":
            # Charcoal effect
            self._draw_smooth_line(internal_x1, internal_y1, internal_x2, internal_y2,
                                 lambda x, y: self._draw_charcoal(x, y))
        
        elif self.draw_mode == "glow":
            # Glow pen effect
            self._draw_smooth_line(internal_x1, internal_y1, internal_x2, internal_y2,
                                 lambda x, y: self._draw_glow_pen(x, y))
    def _draw_point(self, x, y):
        """Draw a single point at the current position with paint"""
        internal_x = self._scale_to_internal(x)
        internal_y = self._scale_to_internal(y)
        
        if self.draw_mode == "pen":
            # Dynamic pen with paint - NOW WITH VISIBILITY SCALING
            base_size = self._get_effective_tool_size(3, min_chars=1)
            max_size = self._get_effective_tool_size(25, min_chars=3)
            momentum_factor = min(1.0, self.pen_momentum / 10.0)
            current_size = int(base_size + (max_size - base_size) * momentum_factor)
            
            brush = self._create_paint_brush(current_size // 2, hardness=0.7)
            self._paint_with_brush(internal_x, internal_y, brush, self.current_color)
            
        elif self.draw_mode == "brush":
            size = self._get_effective_tool_size(12, min_chars=2)
            brush = self._create_paint_brush(size // 2, hardness=0.3)
            self._paint_with_brush(internal_x, internal_y, brush, self.current_color)
            
            
            
        elif self.draw_mode == "wave":
            self._draw_wave_interference(internal_x, internal_y)
            
        elif self.draw_mode == "spiral_gen":
            self._draw_spiral_generator(internal_x, internal_y)
            
        elif self.draw_mode == "particle":
            self._draw_particle_system(internal_x, internal_y)
            
            
        elif self.draw_mode == "crystal":
            self._draw_crystal_growth(internal_x, internal_y)
            
            
            
            
        elif self.draw_mode == "large_brush":
            size = self._get_effective_tool_size(20, min_chars=2)
            brush = self._create_paint_brush(size // 2, hardness=0.2)
            self._paint_with_brush(internal_x, internal_y, brush, self.current_color)
            
        elif self.draw_mode == "larger_brush":
            size = self._get_effective_tool_size(28, min_chars=3)
            brush = self._create_paint_brush(size // 2, hardness=0.15)
            self._paint_with_brush(internal_x, internal_y, brush, self.current_color)
            
        elif self.draw_mode == "spray":
            self._draw_spray_paint(internal_x, internal_y)
            
        elif self.draw_mode in ["star", "cross", "circle", "diamond", "flower"]:
            self._draw_stamp(internal_x, internal_y, self.draw_mode)
            
        elif self.draw_mode == "watercolor":
            self._draw_watercolor(internal_x, internal_y)
            
        elif self.draw_mode == "charcoal":
            self._draw_charcoal(internal_x, internal_y)
            
        elif self.draw_mode == "glow":
            self._draw_glow_pen(internal_x, internal_y)
            
               
    def _blend_area(self, center_x, center_y):
        """Smudge/blend tool - mixes nearby colors"""
    
        
        blend_radius = 15 * self.supersample_factor
        
        # Sample colors in the area
        sampled_colors = []
        for dy in range(-blend_radius, blend_radius, 3):
            for dx in range(-blend_radius, blend_radius, 3):
                x = center_x + dx
                y = center_y + dy
                if 0 <= x < self.internal_canvas_size and 0 <= y < self.internal_canvas_size:
                    pixel = self.pixels.getpixel((x, y))
                    if pixel != (0, 0, 0) and pixel != (0, 0, 0, 255):
                        sampled_colors.append(pixel[:3])
        
        if sampled_colors:
            # Calculate average color
            avg_r = sum(c[0] for c in sampled_colors) // len(sampled_colors)
            avg_g = sum(c[1] for c in sampled_colors) // len(sampled_colors)
            avg_b = sum(c[2] for c in sampled_colors) // len(sampled_colors)
            blend_color = (avg_r, avg_g, avg_b)
            
            # Apply blended color with circular falloff
            for dy in range(-blend_radius, blend_radius):
                for dx in range(-blend_radius, blend_radius):
                    dist = math.sqrt(dx*dx + dy*dy)
                    if dist <= blend_radius:
                        x = center_x + dx
                        y = center_y + dy
                        if 0 <= x < self.internal_canvas_size and 0 <= y < self.internal_canvas_size:
                            # Stronger blend in center
                            opacity = (1.0 - dist / blend_radius) * 0.6
                            # Mix with existing color
                            existing = self.pixels.getpixel((x, y))
                            if existing != (0, 0, 0):
                                self._apply_paint(x, y, blend_color, opacity)
    
    def _draw_roller(self, center_x, center_y):
        """Textured roller brush - covers area with texture"""
      
        
        roller_width = 40 * self.supersample_factor
        roller_height = 20 * self.supersample_factor
        
        # Create texture pattern
        for y in range(-roller_height//2, roller_height//2):
            for x in range(-roller_width//2, roller_width//2):
                px = center_x + x
                py = center_y + y
                
                if 0 <= px < self.internal_canvas_size and 0 <= py < self.internal_canvas_size:
                    # Create texture - paint roller has uneven coverage
                    texture_noise = random.random()
                    
                    if texture_noise > 0.1:  # 90% coverage with gaps
                        # Vary opacity for texture
                        opacity = 0.7 + random.uniform(-0.2, 0.2)
                        
                        # Add subtle color variation
                        color = list(self.current_color)
                        variation = random.randint(-10, 10)
                        color = tuple(max(0, min(255, c + variation)) for c in color)
                        
                        # Paint roller texture - horizontal streaks
                        if random.random() > 0.3:  # Some horizontal streaking
                            streak_length = random.randint(3, 8)
                            for sx in range(streak_length):
                                if px + sx < self.internal_canvas_size:
                                    self._apply_paint(px + sx, py, color, opacity)
                        else:
                            self._apply_paint(px, py, color, opacity)
    def express_uncertainty_through_art(self, uncertainty_type):
        """Convert emotional uncertainty into artistic expression"""
        uncertainty_patterns = {
            'overwhelmed': f"5.4.5.4.{self.current_color_name}5!@#$%white5",  # Hesitant color with sounds
            'conflicted': f"{self.current_color_name}5..black5..{self.current_color_name}5",  # Color conflict
            'processing': "spray5~~~~____~~~~",  # Spray with wave pattern
            'multiple_uncertainties': "513131313131314",  # Circular questioning movement
            'simple_uncertainty': "5.4.5",  # Brief hesitation
            'deeply_conflicted': "5.4.5.4.purple5..cyan5..purple5",  # Color tension
            'excited_confused': f"{self.current_color_name}5!@#$%^&*",  # Energetic confusion
        }
        
        return uncertainty_patterns.get(uncertainty_type, "5.4.5.4")  # Default hesitation                       
    def parse_structured_expression(self, raw_output):
        """Parse Aurora's structured thought  feeling  action format"""
       
        
        # Check if Aurora used structured format
        structured_indicators = ['position:', 'thought:', 'feeling:', 'intention:', 'movement:', 'action:', 'emotional_state:']
        if not any(indicator in raw_output.lower() for indicator in structured_indicators):
            return None
            
        # Parse structured components
        expression = {
            'thought': None,
            'feeling': None, 
            'intention': None,
            'action': None,
            'action_code': ''
        }
        
        lines = raw_output.split('\n')
        current_movement = ''
        current_colors = []
        
        for line in lines:
            line = line.strip().lower()
            
            # Parse thoughts
            if 'thought:' in line or 'thinking:' in line:
                expression['thought'] = line.split(':', 1)[1].strip()
            elif 'position:' in line and expression['thought'] is None:
                expression['thought'] = f"I'm at {line.split(':', 1)[1].strip()}"
                
            # Parse feelings/emotional state
            if 'feeling:' in line or 'emotional_state:' in line or 'emotion:' in line:
                expression['feeling'] = line.split(':', 1)[1].strip()
            elif 'conflicted' in line or 'uncertain' in line or 'overwhelmed' in line:
                if not expression['feeling']:
                    expression['feeling'] = line
                    
            # Parse intentions
            if 'intention:' in line or 'goal:' in line or 'want to:' in line:
                expression['intention'] = line.split(':', 1)[1].strip()
            elif 'explore' in line or 'discover' in line or 'resist' in line:
                if not expression['intention']:
                    expression['intention'] = line
                    
            # Parse actions
            if 'action:' in line:
                expression['action'] = line.split(':', 1)[1].strip()
                
            # Extract movement codes
            if 'movement:' in line:
                movement_part = line.split(':', 1)[1].strip()
                # Extract numbers and ? patterns
                movement_codes = re.findall(r'[0-3?]+', movement_part)
                if movement_codes:
                    current_movement = ''.join(movement_codes)
                    
            # Extract color codes
            if 'color:' in line or 'colors:' in line:
                color_part = line.split(':', 1)[1].strip()
                # Look for color words and conflict patterns
                for color in ['red', 'blue', 'green', 'yellow', 'white', 'black', 'purple', 'orange', 'cyan', 'pink', 'gray', 'brown']:
                    if color in color_part:
                        current_colors.append(color)
                        
                # Look for color conflicts like ?red?blue
                color_conflicts = re.findall(r'\?(\w+)\?(\w+)', color_part)
                if color_conflicts:
                    for conflict in color_conflicts:
                        if conflict[0] in self.palette and conflict[1] in self.palette:
                            # Create conflict expression
                            current_movement += f"{conflict[0]}5.{conflict[1]}5.{conflict[0]}5"
                            expression['action'] = f"expressing conflict between {conflict[0]} and {conflict[1]}"
        
        # Build action code
        action_code = ''
        if current_colors and not any('?' in str(c) for c in current_colors):
            # Simple color choice
            action_code += current_colors[0]
            
        if current_movement:
            action_code += '5' + current_movement  # Add pen down
            
        if not action_code and expression['feeling'] and 'uncertain' in expression['feeling']:
            # Default uncertainty expression
            action_code = '...'
            
        expression['action_code'] = action_code
        
        # Fill in defaults for missing components
        if not expression['thought']:
            expression['thought'] = "Processing current situation"
        if not expression['feeling']:
            expression['feeling'] = self.current_emotion
        if not expression['intention']:
            expression['intention'] = "Exploring what wants to emerge"
        if not expression['action']:
            if action_code:
                expression['action'] = f"Following intention through movement: {action_code}"
            else:
                expression['action'] = "Contemplating possibilities"
                
        return expression 
        
    def enter_chat_mode(self):
        """Enter chat mode - using consolidated memories"""
        print("\n' Chat with Aurora (type 'exit' to return to art)")
        
        self.chat_mode = True
        self.last_chat_time = time.time()
        
        # Start chat loop immediately - no greeting
        self.chat_loop()
    
    def aurora_chat_response(self, user_message):
        """Generate Aurora's natural response using Aurora's learned experiences"""
        
        # Get consolidated learning context
        learning_context = self.get_consolidated_learning_context()
        
        # Get recent successful patterns
        successful_actions = []
        if self.memory.code_history:
            for memory in list(self.memory.code_history)[-20:]:
                pixels = memory.get('context', {}).get('pixels_drawn', 0)
                if pixels > 100:
                    successful_actions.append(f"{memory['code'][:10]} drew {pixels}px")
        
        system_prompt = f"""

{learning_context}

Recent successes: {', '.join(successful_actions[-3:]) if successful_actions else 'exploring'}"""
        
        # Get canvas state
        overview = self.get_canvas_overview()
        recent_colors = list(self.color_history)[-10:] if self.color_history else []
        
        user_prompt = f"""Current emotion: {self.current_emotion}
{overview}
Recent colors: {', '.join(recent_colors) if recent_colors else 'various'}
Tool: {self.draw_mode}

Elijah says: {user_message}"""
        
        full_prompt = f"""[INST] <<SYS>>
{system_prompt}
<</SYS>>

{user_prompt} [/INST]"""
        
        try:
            response = self.llm(
                full_prompt, 
                max_tokens=200,
                temperature=0.9,
                top_p=0.95,
                stop=["[INST]", "</s>"],
                stream=False
            )
            
            aurora_says = response['choices'][0]['text'].strip()
            
            if aurora_says:
                print(f"Aurora: {aurora_says}")
            
        except Exception as e:
            print(f"Aurora: ...")

            
    def chat_loop(self):
        """Main chat conversation loop"""
        while self.chat_mode:
            try:
                # Get user input
                user_input = input(" You: ").strip()
                
                if user_input.lower() in ['exit', 'quit', 'bye']:
                    self.exit_chat_mode()
                    break
                    
                if not user_input:
                    continue
                    
                # Store user message
                self.chat_history.append({
                    'speaker': 'Elijah',
                    'message': user_input,
                    'timestamp': time.time()
                })
                
                # Generate Aurora's response
                self.aurora_chat_response(user_input)
                
            except KeyboardInterrupt:
                self.exit_chat_mode()
                break
            except EOFError:
                self.exit_chat_mode()
                break
                
    def aurora_chat_response(self, user_message):
        """Generate Aurora's natural response - using small_aurora2.py structure"""
        
        # Build chat prompt - EXACTLY like small_aurora2.py
        system_prompt = """ """
        
        # Get some context about recent drawing
        overview = self.get_canvas_overview()
        recent_colors = list(self.color_history)[-10:] if self.color_history else []
        
        user_prompt = f"""Current emotion: {self.current_emotion}
{overview}
Recent colors used: {', '.join(recent_colors) if recent_colors else 'various'}
Current drawing tool: {self.draw_mode}

Elijah says: {user_message}"""
        
        # Llama 2 Chat format
        full_prompt = f"""[INST] <<SYS>>
{system_prompt}
<</SYS>>

{user_prompt} [/INST]"""
        
        try:
            response = self.llm(
                full_prompt, 
                max_tokens=200,  # Same as small_aurora2.py
                temperature=0.9,  # Changed back to original
                top_p=0.95,      # Changed back to original
                stop=["[INST]", "</s>"],
                stream=False
            )
            
            aurora_says = response['choices'][0]['text'].strip()
            
            if aurora_says:
                print(f"Aurora: {aurora_says}")
            
        except Exception as e:
            print(f"Aurora: ...")  # Simple silence on error
            
    def analyze_chat_sentiment(self, user_msg, aurora_msg):
        """Analyze emotional tone of conversation"""
        positive_words = ['happy', 'love', 'wonderful', 'beautiful', 'amazing', 'good', 'great', 'fun', 'joy']
        negative_words = ['sad', 'worried', 'difficult', 'hard', 'trouble', 'problem', 'bad', 'hurt']
        
        combined_text = (user_msg + " " + aurora_msg).lower()
        
        positive_count = sum(1 for word in positive_words if word in combined_text)
        negative_count = sum(1 for word in negative_words if word in combined_text)
        
        if positive_count > negative_count:
            return 0.3  # Positive influence
        elif negative_count > positive_count:
            return -0.2  # Slight negative influence
        else:
            return 0.1  # Neutral positive (connection is good)
            
    def exit_chat_mode(self):
        """Exit chat and return to art"""
        print(" Returning to art creation...\n")
        self.chat_mode = False
                              
    def generate_autonomous_goal(self):
        """Aurora generates and pursues Aurora's own complex, multi-step goals"""
        if self.goal_generation_cooldown > 0:
            self.goal_generation_cooldown -= 1
            return None
            
        # Only generate goals when Aurora has some experience
        if len(self.memory.code_history) < 20:
            return None
            
        # Analyze Aurora's current situation deeply
        canvas_analysis = self.analyze_canvas_for_goals()
        emotional_state = self.analyze_emotional_trajectory()
        skill_gaps = self.identify_skill_gaps()
        
        goal_prompt = f"""

CANVAS ANALYSIS: {canvas_analysis}
EMOTIONAL TRAJECTORY: {emotional_state}
SKILL GAPS: {skill_gaps}

Generate a specific, measurable goal that will challenge you and expand your artistic capabilities.
The goal should be something YOU want to achieve, not what others expect.

Examples of autonomous goals:
- "Create a composition using only warm colors that flows from edge to edge"
- "Master the spray tool by creating texture that resembles organic growth"
- "Develop a personal signature pattern that reflects my current emotional state"
- "Fill exactly 60% of the canvas using only diagonal movements"
- "Create visual rhythm by alternating between 3 specific color combinations"

Your autonomous goal:"""

        full_prompt = f"""[INST] <<SYS>>
{goal_prompt}
<</SYS>>

I autonomously choose to: [/INST]"""
        
        try:
            response = self.llm(full_prompt, max_tokens=100, temperature=0.9, stop=["[INST]", "</s>", "\n"])
            goal_text = response['choices'][0]['text'].strip()
            
            if goal_text and len(goal_text) > 20:
                # Create structured goal with success criteria
                goal = {
                    'description': goal_text,
                    'type': 'autonomous',
                    'created_at_step': self.steps_taken,
                    'emotion_when_created': self.current_emotion,
                    'success_criteria': self.extract_success_criteria(goal_text),
                    'progress_tracking': {},
                    'estimated_steps': self.estimate_goal_duration(goal_text),
                    'priority': 'high',
                    'timestamp': time.time()
                }
                
                self.autonomous_goals.append(goal)
                print(f"\n Aurora sets autonomous goal: {goal_text}")
                print(f"   Estimated duration: {goal['estimated_steps']} steps")
                
                # Reset cooldown
                self.goal_generation_cooldown = random.randint(100, 200)
                return goal
                
        except Exception as e:
            print(f"  Error generating autonomous goal: {e}")
            
        return None
        
    def generate_coverage_focused_goal(self):
        """Generate a goal specifically for filling empty areas"""
        if not hasattr(self, 'identified_empty_regions') or not self.identified_empty_regions:
            return None
        
        # Pick an empty region to target
        target_region = self.identified_empty_regions[0]
        
        goal_descriptions = [
            f"Fill the empty region from ({target_region['x']}, {target_region['y']}) to ({target_region['x'] + target_region['size']}, {target_region['y'] + target_region['size']})",
            f"Create a dense pattern covering at least 50% of the {target_region['size']}x{target_region['size']} area starting at ({target_region['x']}, {target_region['y']})",
            f"Explore and paint the untouched region centered at ({target_region['center_x']}, {target_region['center_y']})",
            f"Achieve 80% coverage in the region from x={target_region['x']} to x={target_region['x'] + target_region['size']}"
        ]
        
        goal = {
            'description': random.choice(goal_descriptions),
            'type': 'coverage',
            'target_region': target_region,
            'created_at_step': self.steps_taken,
            'emotion_when_created': self.current_emotion,
            'success_criteria': {
                'target_coverage': 70,
                'region_bounds': (target_region['x'], target_region['y'], 
                                target_region['x'] + target_region['size'], 
                                target_region['y'] + target_region['size'])
            },
            'estimated_steps': 200,
            'priority': 'critical',
            'timestamp': time.time()
        }
        
        self.autonomous_goals.append(goal)
        print(f"\n Aurora sets COVERAGE goal: {goal['description']}")
        
        return goal
        
        
    def analyze_canvas_for_goals(self):
        """Analyze canvas state to inform goal generation - PRIORITIZING empty space filling"""
        # Sample canvas for analysis
        total_pixels = self.canvas_size * self.canvas_size
        filled_pixels = 0
        color_distribution = {}
        density_map = {'sparse': 0, 'medium': 0, 'dense': 0}
        
        # Track empty regions with LARGER regions for better coverage
        quadrant_coverage = {
            'top-left': {'filled': 0, 'total': 0},
            'top-right': {'filled': 0, 'total': 0},
            'bottom-left': {'filled': 0, 'total': 0},
            'bottom-right': {'filled': 0, 'total': 0}
        }
        
        mid_x = self.canvas_size // 2
        mid_y = self.canvas_size // 2
        
        # Find the emptiest regions with LARGER region sizes
        empty_regions = []
        region_size = self.canvas_size // 5  # Changed from 10 to 5 - larger regions
        
        for region_x in range(0, self.canvas_size, region_size):
            for region_y in range(0, self.canvas_size, region_size):
                region_filled = 0
                region_total = 0
                
                # More thorough sampling
                sample_step = max(1, region_size // 20)  # Changed from 10 to 20 - finer sampling
                for x in range(region_x, min(region_x + region_size, self.canvas_size), sample_step):
                    for y in range(region_y, min(region_y + region_size, self.canvas_size), sample_step):
                        region_total += 1
                        internal_x = self._scale_to_internal(x)
                        internal_y = self._scale_to_internal(y)
                        if internal_x < self.internal_canvas_size and internal_y < self.internal_canvas_size:
                            pixel = self.pixels.getpixel((internal_x, internal_y))
                            if pixel != (0, 0, 0) and pixel != (0, 0, 0, 255):
                                region_filled += 1
                                filled_pixels += sample_step * sample_step
                
                # Calculate coverage for this region
                region_coverage = (region_filled / region_total * 100) if region_total > 0 else 0
                
                # Track ALL empty regions, not just very empty ones
                if region_coverage < 50:  # Changed from 20 to 50 - more aggressive
                    empty_regions.append({
                        'x': region_x,
                        'y': region_y,
                        'coverage': region_coverage,
                        'center_x': region_x + region_size // 2,
                        'center_y': region_y + region_size // 2,
                        'size': region_size
                    })
        
        # Sort by emptiness AND proximity
        for region in empty_regions:
            region['distance'] = abs(self.x - region['center_x']) + abs(self.y - region['center_y'])
            # Priority score: emptier and closer is better
            region['priority'] = (100 - region['coverage']) * 10 + (1000 - region['distance'])
        
        empty_regions.sort(key=lambda r: r['priority'], reverse=True)
        
        coverage = (filled_pixels / total_pixels) * 100
        
        # Build analysis with STRONG emphasis on empty spaces
        analysis = f"Coverage: {coverage:.1f}%, "
        
        if empty_regions:
            # Report multiple empty regions
            analysis += f"{len(empty_regions)} EMPTY REGIONS NEED FILLING! "
            if len(empty_regions) >= 3:
                top_3 = empty_regions[:3]
                for i, region in enumerate(top_3):
                    analysis += f"Region {i+1}: ({region['x']}-{region['x']+region['size']}, {region['y']}-{region['y']+region['size']}) only {region['coverage']:.0f}% filled. "
        
        # Store for goal generation
        self.identified_empty_regions = empty_regions
        self.current_coverage = coverage
        
        return analysis

    def analyze_emotional_trajectory(self):
        """Analyze Aurora's emotional patterns to inform goals"""
        if not hasattr(self, 'emotion_memory') or len(self.emotion_memory) < 5:
            return f"Current: {self.current_emotion}, Limited history"
        
        recent_emotions = list(self.emotion_memory)[-10:]
        emotion_categories = [e['category'] for e in recent_emotions]
        emotion_depths = [e['depth'] for e in recent_emotions]
        
        # Analyze patterns
        stable = len(set(emotion_categories[-5:])) <= 2
        trending_up = len([d for d in emotion_depths[-3:] if d >= 3]) >= 2
        
        trajectory = f"Recent: {emotion_categories[-1]}, "
        if stable:
            trajectory += "emotionally stable"
        else:
            trajectory += "emotionally dynamic"
        if trending_up:
            trajectory += ", high intensity"
            
        return trajectory

    def identify_skill_gaps(self):
        """Identify areas where Aurora could develop new skills"""
        if not self.memory.code_history:
            return "No skill history available"
        
        # Analyze tool usage
        tool_usage = {}
        recent_actions = list(self.memory.code_history)[-50:]
        for action in recent_actions:
            tool = action.get('context', {}).get('draw_mode', 'pen')
            tool_usage[tool] = tool_usage.get(tool, 0) + 1
        
        # Identify underused tools
        all_tools = ['pen', 'brush', 'spray', 'star', 'cross', 'circle', 'diamond', 'flower', 'watercolor', 'charcoal', 'glow']
        underused = [tool for tool in all_tools if tool_usage.get(tool, 0) < 5]
        
        # Analyze pattern complexity
        patterns = [action['code'] for action in recent_actions]
        avg_complexity = sum(len(p) for p in patterns) / len(patterns) if patterns else 0
        
        gaps = f"Underused tools: {underused[:3]}, Pattern complexity: {avg_complexity:.1f}"
        return gaps

    def extract_success_criteria(self, goal_text):
        """Extract measurable success criteria from goal description"""
        criteria = {}
        goal_lower = goal_text.lower()
        
        # Extract percentage targets
      
        percentage_matches = re.findall(r'(\d+)%', goal_text)
        if percentage_matches:
            criteria['target_coverage'] = int(percentage_matches[0])
        
        # Extract color requirements
        mentioned_colors = [color for color in self.palette.keys() if color in goal_lower]
        if mentioned_colors:
            criteria['required_colors'] = mentioned_colors
        
        # Extract tool requirements
        mentioned_tools = [tool for tool in ['spray', 'brush', 'star', 'cross', 'circle'] if tool in goal_lower]
        if mentioned_tools:
            criteria['required_tools'] = mentioned_tools
        
        # Extract movement patterns
        if 'diagonal' in goal_lower:
            criteria['movement_pattern'] = 'diagonal'
        elif 'edge to edge' in goal_lower:
            criteria['movement_pattern'] = 'edge_to_edge'
        
        return criteria

    def estimate_goal_duration(self, goal_text):
        """Estimate how many steps a goal might take"""
        goal_lower = goal_text.lower()
        base_duration = 50
        
        # Adjust based on complexity indicators
        if 'master' in goal_lower or 'develop' in goal_lower:
            base_duration *= 3
        if 'complex' in goal_lower or 'intricate' in goal_lower:
            base_duration *= 2
        if any(word in goal_lower for word in ['60%', '70%', '80%', '90%']):
            base_duration *= 2
        
        return random.randint(int(base_duration * 0.7), int(base_duration * 1.3))

    def evaluate_goal_progress(self):
        """Evaluate progress on current autonomous goal"""
        if not self.autonomous_goals:
            return
        
        current_goal = self.autonomous_goals[-1]
        if current_goal.get('completed'):
            return
        
        steps_since_goal = self.steps_taken - current_goal['created_at_step']
        success_criteria = current_goal.get('success_criteria', {})
        progress = {}
        
        # Check coverage targets
        if 'target_coverage' in success_criteria:
            current_coverage = self.calculate_current_coverage()
            progress['coverage'] = current_coverage
            target = success_criteria['target_coverage']
            if current_coverage >= target:
                progress['coverage_achieved'] = True
        
        # Check color requirements
        if 'required_colors' in success_criteria:
            recent_colors = set(list(self.color_history)[-20:])
            required = set(success_criteria['required_colors'])
            progress['colors_used'] = list(recent_colors.intersection(required))
            if required.issubset(recent_colors):
                progress['colors_achieved'] = True
        
        # Check tool requirements
        if 'required_tools' in success_criteria:
            recent_tools = [c.get('context', {}).get('draw_mode') for c in list(self.memory.code_history)[-20:]]
            required_tools = success_criteria['required_tools']
            used_tools = [tool for tool in required_tools if tool in recent_tools]
            progress['tools_used'] = used_tools
            if all(tool in recent_tools for tool in required_tools):
                progress['tools_achieved'] = True
        
        # Update progress tracking
        current_goal['progress_tracking'] = progress
        current_goal['steps_elapsed'] = steps_since_goal
        
        # Check if goal is completed
        achievement_count = sum(1 for key in progress if key.endswith('_achieved'))
        total_criteria = len([k for k in success_criteria if k != 'movement_pattern'])
        
        if achievement_count >= max(1, total_criteria * 0.8):  # 80% criteria met
            current_goal['completed'] = True
            current_goal['completion_step'] = self.steps_taken
            print(f"\n GOAL ACHIEVED: {current_goal['description']}")
            print(f"   Completed in {steps_since_goal} steps!")
            
            # REINFORCEMENT REMOVED - No emotion boost for goal completion
            # Boost satisfaction significantly
            # self.influence_emotion("creating", 0.8)
            
            # Generate follow-up goal
            self.goal_generation_cooldown = 5  # Quick follow-up

    def calculate_current_coverage(self):
        """Calculate current canvas coverage percentage"""
        total_samples = 0
        filled_samples = 0
        sample_step = max(1, self.canvas_size // 40)
        
        for x in range(0, self.canvas_size, sample_step):
            for y in range(0, self.canvas_size, sample_step):
                total_samples += 1
                internal_x = self._scale_to_internal(x)
                internal_y = self._scale_to_internal(y)
                if internal_x < self.internal_canvas_size and internal_y < self.internal_canvas_size:
                    pixel = self.pixels.getpixel((internal_x, internal_y))
                    if pixel != (0, 0, 0) and pixel != (0, 0, 0, 255):
                        filled_samples += 1
        
        return (filled_samples / total_samples * 100) if total_samples > 0 else 0                              
    def _draw_rect(self, center_x, center_y, size):
        """Draw a filled rectangle centered at the given point"""
        half_size = size // 2
        x1 = max(0, center_x - half_size)
        y1 = max(0, center_y - half_size)
        x2 = min(self.internal_canvas_size - 1, center_x + half_size)
        y2 = min(self.internal_canvas_size - 1, center_y + half_size)
        
        self.draw_img.rectangle([x1, y1, x2, y2], fill=self.current_color)
    
    def _draw_spray_paint(self, center_x, center_y):
        """Draw spray paint effect with paint droplets - NOW WITH VISIBILITY SCALING"""
    
        spray_size = self._get_effective_tool_size(15, min_chars=2)
        dots = 30
        
        for _ in range(dots):
            # Random position within spray radius
            angle = random.random() * 2 * math.pi
            distance = random.random() * spray_size
            x = int(center_x + distance * math.cos(angle))
            y = int(center_y + distance * math.sin(angle))
            
            # Paint droplet size varies - scaled proportionally
            droplet_size = random.uniform(0.5, 2.0) * (spray_size / 60.0)  # Scale with spray size
            opacity = random.uniform(0.3, 0.9)
            
            # Apply paint droplet
            for dy in range(int(-droplet_size), int(droplet_size) + 1):
                for dx in range(int(-droplet_size), int(droplet_size) + 1):
                    if dx*dx + dy*dy <= droplet_size*droplet_size:
                        self._apply_paint(x + dx, y + dy, self.current_color, opacity)
                        
    def _draw_watercolor(self, center_x, center_y):
        """Draw with watercolor effect - transparent with subtle bleeding - NOW WITH VISIBILITY SCALING"""
        
        # Scale watercolor size for visibility
        watercolor_size = self._get_effective_tool_size(14, min_chars=2)
        
        # Create soft brush with very low hardness for watercolor spread
        brush = self._create_paint_brush(watercolor_size // 2, hardness=0.1)  # Very soft edges
        
        # Get the numpy array of the brush mask
        mask_array = np.array(brush)
        mask_width, mask_height = brush.size
        half_w, half_h = mask_width // 2, mask_height // 2
        
        # Apply paint with watercolor transparency
        for dy in range(mask_height):
            for dx in range(mask_width):
                # Get opacity from mask
                opacity = mask_array[dy, dx] / 255.0
                if opacity > 0.01:
                    px = center_x - half_w + dx
                    py = center_y - half_h + dy
                    
                    if 0 <= px < self.internal_canvas_size and 0 <= py < self.internal_canvas_size:
                        # Watercolor opacity - transparent but not extreme
                        watercolor_opacity = opacity * random.uniform(0.15, 0.25)  # 15-25% opacity
                        
                        # Slight color variation for granulation
                        r, g, b = self.current_color
                        r = max(0, min(255, r + random.randint(-15, 15)))
                        g = max(0, min(255, g + random.randint(-15, 15)))
                        b = max(0, min(255, b + random.randint(-15, 15)))
                        
                        # Check for wet paint for blending
                        existing = self.pixels.getpixel((px, py))
                        wetness = self._get_paint_wetness(px, py)
                        
                        if wetness > 0.1 and existing != (0, 0, 0) and existing != (0, 0, 0, 255):
                            # Watercolor blends more when wet, but not 95%!
                            blend_amount = min(0.6, wetness * 0.7)  # Max 60% blending
                            
                            # Mix colors
                            mixed_r = int(existing[0] * blend_amount + r * (1 - blend_amount))
                            mixed_g = int(existing[1] * blend_amount + g * (1 - blend_amount))
                            mixed_b = int(existing[2] * blend_amount + b * (1 - blend_amount))
                            
                            self._apply_paint(px, py, (mixed_r, mixed_g, mixed_b), watercolor_opacity)
                        else:
                            self._apply_paint(px, py, (r, g, b), watercolor_opacity)
        
        # Add subtle bleeding effect - just a few small drips
        for _ in range(random.randint(2, 4)):  # Only 2-4 drips instead of 8-15
            # Start from painted area
            start_angle = random.uniform(0, 2 * math.pi)
            start_dist = random.uniform(0, watercolor_size * 0.5)
            
            flow_x = int(center_x + start_dist * math.cos(start_angle))
            flow_y = int(center_y + start_dist * math.sin(start_angle))
            
            # Shorter flows
            flow_length = random.randint(8, 15)  # Much shorter than 20-50
            
            # Gentle downward tendency
            x_drift = random.uniform(-0.3, 0.3)
            
            for i in range(flow_length):
                # Subtle movement
                flow_x += int(x_drift + random.uniform(-0.3, 0.3)) * self.supersample_factor
                flow_y += random.randint(0, 2) * self.supersample_factor  # Gentle downward
                
                if 0 <= flow_x < self.internal_canvas_size and 0 <= flow_y < self.internal_canvas_size:
                    r, g, b = self.current_color
                    
                    # Diluted as it flows
                    flow_opacity = 0.1 * (1 - i / flow_length) * random.uniform(0.5, 1.0)
                    
                    # Thin flow line
                    self._apply_paint(flow_x, flow_y, (r, g, b), flow_opacity)
    
    def _draw_charcoal(self, center_x, center_y):
        """Draw with charcoal effect - rough, grainy texture"""
        charcoal_size = 18 * self.supersample_factor
        
        # Charcoal creates rough, irregular marks
        for _ in range(40):  # Multiple grain particles
            # Random offset for graininess
            offset_x = random.randint(-charcoal_size, charcoal_size)
            offset_y = random.randint(-charcoal_size, charcoal_size)
            
            # Elliptical shape (charcoal on its side)
            if random.random() < 0.7:  # 70% horizontal bias
                offset_x = int(offset_x * 1.5)  # Stretch horizontally
                offset_y = int(offset_y * 0.6)  # Compress vertically
            
            x = center_x + offset_x
            y = center_y + offset_y
            
            if 0 <= x < self.internal_canvas_size and 0 <= y < self.internal_canvas_size:
                # Distance affects opacity
                dist = math.sqrt(offset_x**2 + offset_y**2)
                if dist <= charcoal_size:
                    # Charcoal is usually dark
                    if self.current_color == (255, 255, 255):  # If white selected
                        # White charcoal (chalk)
                        color = (240, 240, 240)
                    else:
                        # Dark charcoal with slight color tint
                        r, g, b = self.current_color
                        color = (min(50, r//5), min(50, g//5), min(50, b//5))
                    
                    opacity = random.uniform(0.3, 0.8) * (1 - dist/charcoal_size)
                    
                    # Add paper texture interaction
                    if random.random() < 0.3:  # Skip some pixels for texture
                        continue
                        
                    self._apply_paint(x, y, color, opacity)
    
    def _draw_glow_pen(self, center_x, center_y):
        """Draw with glowing neon effect"""
        glow_radius = 12 * self.supersample_factor
        
        # Create bright center with glowing halo
        for radius in range(glow_radius, 0, -1):
            # Intensity decreases with radius
            intensity = (radius / glow_radius) ** 2
            
            # Draw circle at this radius
            num_points = int(radius * 2 * math.pi)
            for i in range(num_points):
                angle = (i / max(1, num_points)) * 2 * math.pi
                x = int(center_x + radius * math.cos(angle))
                y = int(center_y + radius * math.sin(angle))
                
                if 0 <= x < self.internal_canvas_size and 0 <= y < self.internal_canvas_size:
                    # Get existing pixel to add glow to it
                    existing = self.pixels.getpixel((x, y))
                    
                    # Calculate glow color (brighter version of current color)
                    r, g, b = self.current_color
                    
                    if radius < glow_radius // 3:  # Inner bright core
                        # Almost white core
                        glow_r = min(255, r + (255 - r) * 0.8)
                        glow_g = min(255, g + (255 - g) * 0.8)
                        glow_b = min(255, b + (255 - b) * 0.8)
                        opacity = 0.9
                    elif radius < 2 * glow_radius // 3:  # Middle glow
                        # Bright color
                        glow_r = min(255, r + (255 - r) * 0.4)
                        glow_g = min(255, g + (255 - g) * 0.4)
                        glow_b = min(255, b + (255 - b) * 0.4)
                        opacity = 0.5 * intensity
                    else:  # Outer halo
                        # Soft colored glow
                        glow_r = r
                        glow_g = g
                        glow_b = b
                        opacity = 0.2 * intensity
                    
                    # Apply additive blending for glow effect
                    if existing != (0, 0, 0) and existing != (0, 0, 0, 255):
                        # Add glow to existing color
                        final_r = min(255, existing[0] + int(glow_r * opacity))
                        final_g = min(255, existing[1] + int(glow_g * opacity))
                        final_b = min(255, existing[2] + int(glow_b * opacity))
                    else:
                        # Apply glow to black
                        final_r = int(glow_r * opacity)
                        final_g = int(glow_g * opacity)
                        final_b = int(glow_b * opacity)
                    
                    self.pixels.putpixel((x, y), (final_r, final_g, final_b))
    def _draw_stamp(self, center_x, center_y, stamp_type):
        """Draw stamps with artistic transparency and paint-on-cloth texture"""
   
        
        # Create a temporary image for the stamp
        stamp_size = 60 * self.supersample_factor  # Larger for better texture
        stamp_img = Image.new('RGBA', (stamp_size, stamp_size), (0, 0, 0, 0))
        stamp_draw = ImageDraw.Draw(stamp_img)
        
        # Store original draw_img temporarily
        original_draw = self.draw_img
        self.draw_img = stamp_draw
        
        # Draw to temporary surface with offset
        offset_x = stamp_size//2
        offset_y = stamp_size//2
        
        if stamp_type == "star":
            self._draw_star(offset_x, offset_y, 15 * self.supersample_factor)
        elif stamp_type == "cross":
            self._draw_cross(offset_x, offset_y, 20 * self.supersample_factor)
        elif stamp_type == "circle":
            self._draw_circle(offset_x, offset_y, 15 * self.supersample_factor)
        elif stamp_type == "diamond":
            self._draw_diamond(offset_x, offset_y, 20 * self.supersample_factor)
        elif stamp_type == "flower":
            self._draw_flower(offset_x, offset_y, 20 * self.supersample_factor)
        
        # Restore original draw
        self.draw_img = original_draw
        
        # Add cloth texture effect
        stamp_img = self._add_cloth_texture(stamp_img, stamp_type, stamp_size)
        
        # Paste with blending using the paint system
        paste_x = center_x - stamp_size//2
        paste_y = center_y - stamp_size//2
        
        # Apply stamp using paint system pixel by pixel
        stamp_array = np.array(stamp_img)
        for dy in range(stamp_size):
            for dx in range(stamp_size):
                if stamp_array[dy, dx, 3] > 0:  # If pixel has any alpha
                    px = paste_x + dx
                    py = paste_y + dy
                    # Use the paint system for proper mixing
                    self._apply_paint(px, py, self.current_color, stamp_array[dy, dx, 3] / 255.0)
    
    def _add_cloth_texture(self, stamp_img, stamp_type, stamp_size):
        """Add paint-on-cloth texture to stamp"""
     
       
        # Convert to numpy for easier manipulation
        img_array = np.array(stamp_img)
        
        # Create texture mask - like paint bleeding into cloth fibers
        for y in range(0, stamp_size, 2):
            for x in range(0, stamp_size, 2):
                if img_array[y, x, 3] > 0:  # If pixel has any alpha
                    # Reduce base alpha for transparency
                    img_array[y, x, 3] = int(img_array[y, x, 3] * 0.7)
                    
                    # Cloth fiber simulation - reduce alpha randomly
                    fiber_effect = random.random()
                    if fiber_effect < 0.3:  # 30% chance of heavy absorption
                        img_array[y, x, 3] = int(img_array[y, x, 3] * 0.4)
                    elif fiber_effect < 0.6:  # 30% chance of medium absorption
                        img_array[y, x, 3] = int(img_array[y, x, 3] * 0.7)
                    
                    # Add paint bleeding to neighbors
                    if fiber_effect > 0.8 and img_array[y, x, 3] > 100:
                        for dy in [-1, 0, 1]:
                            for dx in [-1, 0, 1]:
                                ny, nx = y + dy, x + dx
                                if 0 <= ny < stamp_size and 0 <= nx < stamp_size:
                                    if img_array[ny, nx, 3] == 0:  # Empty neighbor
                                        # Bleed color with low alpha
                                        img_array[ny, nx] = img_array[y, x].copy()
                                        img_array[ny, nx, 3] = random.randint(20, 50)
        
        # Add paint spatter around stamp
        for _ in range(100):
            if random.random() < 0.2:
                spatter_x = random.randint(0, stamp_size-1)
                spatter_y = random.randint(0, stamp_size-1)
                spatter_radius = random.randint(1, 3)
                
                # Check if near existing paint
                has_nearby_paint = False
                for dy in range(-10, 11):
                    for dx in range(-10, 11):
                        check_y = spatter_y + dy
                        check_x = spatter_x + dx
                        if 0 <= check_y < stamp_size and 0 <= check_x < stamp_size:
                            if img_array[check_y, check_x, 3] > 100:
                                has_nearby_paint = True
                                break
                    if has_nearby_paint:
                        break
                
                if has_nearby_paint:
                    # Add small spatter dot
                    for dy in range(-spatter_radius, spatter_radius+1):
                        for dx in range(-spatter_radius, spatter_radius+1):
                            py = spatter_y + dy
                            px = spatter_x + dx
                            if 0 <= py < stamp_size and 0 <= px < stamp_size:
                                if dy*dy + dx*dx <= spatter_radius*spatter_radius:
                                    img_array[py, px] = (*self.current_color, random.randint(30, 80))
        
        # Convert back to PIL Image
        stamp_img = Image.fromarray(img_array, 'RGBA')
        
        # Apply slight blur for paint spread effect
        stamp_img = stamp_img.filter(ImageFilter.GaussianBlur(radius=1))
        
        return stamp_img
    
    def _draw_star(self, cx, cy, size):
        """Draw a filled star with paint-like variations"""
       
        
        # Add wobble for hand-stamped effect
        points = []
        for i in range(10):
            angle = (i * math.pi / 5) - math.pi / 2
            wobble = random.uniform(-0.1, 0.1)
            angle += wobble
            
            if i % 2 == 0:
                r = size + random.randint(-size//8, size//8)
            else:
                r = size * 0.5 + random.randint(-size//10, size//10)
            x = cx + int(r * math.cos(angle))
            y = cy + int(r * math.sin(angle))
            points.extend([x, y])
        
        if len(points) >= 6:
            self.draw_img.polygon(points, fill=self.current_color)
    
    def _draw_cross(self, cx, cy, size):
        """Draw a cross/plus shape with uneven edges"""
     
        thickness = size // 3
        
        # Vertical bar with texture
        for y in range(cy - size, cy + size):
            width_variation = random.randint(-2, 2)
            if random.random() > 0.05:  # 95% coverage
                self.draw_img.rectangle(
                    [cx - thickness//2 + width_variation, y,
                     cx + thickness//2 + width_variation, y + 1],
                    fill=self.current_color
                )
        
        # Horizontal bar with texture
        for x in range(cx - size, cx + size):
            height_variation = random.randint(-2, 2)
            if random.random() > 0.05:
                self.draw_img.rectangle(
                    [x, cy - thickness//2 + height_variation,
                     x + 1, cy + thickness//2 + height_variation],
                    fill=self.current_color
                )
    
    def _draw_circle(self, cx, cy, radius):
        """Draw a filled circle with organic edges"""
       
        
        # Draw with slight irregularity
        for y in range(cy - radius, cy + radius + 1):
            for x in range(cx - radius, cx + radius + 1):
                dx = x - cx
                dy = y - cy
                distance = math.sqrt(dx*dx + dy*dy)
                
                # Add slight wobble to edge
                edge_wobble = random.uniform(-1, 1)
                if distance <= radius + edge_wobble:
                    if random.random() > 0.05:  # 95% coverage for texture
                        self.draw_img.point((x, y), fill=self.current_color)
    
    def _draw_diamond(self, cx, cy, size):
        """Draw a filled diamond with organic feel"""
       
        
        # Create points with slight variation
        wobble = size // 20
        points = [
            (cx + random.randint(-wobble, wobble), cy - size),      # Top
            (cx + size, cy + random.randint(-wobble, wobble)),      # Right
            (cx + random.randint(-wobble, wobble), cy + size),      # Bottom
            (cx - size, cy + random.randint(-wobble, wobble))       # Left
        ]
        self.draw_img.polygon(points, fill=self.current_color)
    
    def _draw_flower(self, cx, cy, size):
        """Draw a flower shape with organic petals and contrasting center"""
     
        
        # Draw petals first (in current color)
        petal_size = size // 2
        num_petals = random.randint(5, 7)
        
        for i in range(num_petals):
            angle = (360 / num_petals) * i + random.uniform(-10, 10)
            rad = math.radians(angle)
            
            # Petal center with wobble
            px = cx + int(size * 0.7 * math.cos(rad))
            py = cy + int(size * 0.7 * math.sin(rad))
            
            # Organic petal shape - make them more distinct
            petal_width = petal_size + random.randint(-size//8, size//8)
            petal_height = petal_size + random.randint(-size//8, size//8)
            
            # Draw petal with slight transparency for overlap effect
            self.draw_img.ellipse(
                [px - petal_width, py - petal_height, px + petal_width, py + petal_height],
                fill=self.current_color
            )
        
        # Draw center AFTER petals with strong contrast
        center_size = int(size * 0.4)  # Bigger center (40% of size instead of 33%)
        
        # Always use yellow for center unless current color is yellow
        if self.current_color == (255, 255, 0) or self.current_color == (255, 150, 0):  # If yellow or orange
            center_color = (139, 69, 19)  # Use brown center
        else:
            center_color = (255, 255, 0)  # Use yellow center
        
        # Draw solid center circle (no texture gaps)
        self.draw_img.ellipse(
            [cx - center_size, cy - center_size, cx + center_size, cy + center_size],
            fill=center_color
        )
        
        # Add small dots in center for detail
        dot_size = max(2, center_size // 6)
        for i in range(5):
            angle = (360 / 5) * i
            rad = math.radians(angle)
            dot_x = cx + int(center_size * 0.5 * math.cos(rad))
            dot_y = cy + int(center_size * 0.5 * math.sin(rad))
            
            self.draw_img.ellipse(
                [dot_x - dot_size, dot_y - dot_size, dot_x + dot_size, dot_y + dot_size],
                fill=(0, 0, 0)  # Black dots for detail
            )
    
    def update_display(self):
        """Update the pygame display with full-screen canvas"""
        try:
            # Clear screen
            self.screen.fill(self.bg_color)
 
            # Create a high-quality downsampled version
            display_size = int(self.canvas_size * self.display_scale)
            
            # Downsample from internal resolution to display resolution
            display_img = self.pixels.resize(
                (display_size, display_size),
                Image.Resampling.LANCZOS  # High quality downsampling
            )
            
            # Apply slight sharpening to compensate for downsampling
            enhancer = ImageEnhance.Sharpness(display_img)
            display_img = enhancer.enhance(1.2)
            
            
            # Convert PIL image to pygame surface
            if display_img.mode != 'RGBA':
                display_img = display_img.convert('RGBA')
            
            # Get raw image data
            raw_str = display_img.tobytes("raw", 'RGBA')
            canvas_surface = pygame.image.fromstring(raw_str, display_img.size, 'RGBA')
            
            # Draw canvas
            self.screen.blit(canvas_surface, (self.canvas_rect.x, self.canvas_rect.y))
            # Draw smooth cymatic background BEFORE canvas
            if hasattr(self, 'cymatic_surface'):
                # Draw at full opacity - the circles themselves have alpha
                self.screen.blit(self.cymatic_surface, (0, 0))
            
            # Draw canvas with black as transparent
            raw_str = display_img.tobytes("raw", 'RGBA')
            canvas_surface = pygame.image.fromstring(raw_str, display_img.size, 'RGBA')
            canvas_surface.set_colorkey((0, 0, 0))  # This makes black pixels transparent!
            
            # Draw canvas on top - black areas will show cymatics through
            self.screen.blit(canvas_surface, (self.canvas_rect.x, self.canvas_rect.y))
                            
            # Draw Aurora's position indicator
            aurora_x = self.canvas_rect.x + int(self.x * self.display_scale)
            aurora_y = self.canvas_rect.y + int(self.y * self.display_scale)
            
            # Draw position indicator with better visibility
            indicator_size = max(5, int(7 * self.display_scale))
            if self.is_drawing:
                pygame.draw.circle(self.screen, (255, 255, 255), (aurora_x, aurora_y), indicator_size)
                pygame.draw.circle(self.screen, (0, 0, 0), (aurora_x, aurora_y), indicator_size, 2)
            else:
                pygame.draw.circle(self.screen, (128, 128, 128), (aurora_x, aurora_y), indicator_size)
                pygame.draw.circle(self.screen, (255, 255, 255), (aurora_x, aurora_y), indicator_size, 2)
            
            # Minimal overlay in top-left corner
            y_pos = 10
            x_pos = 10
            
            # Current emotion and mode
            status_text = f"Feeling: {self.current_emotion} | Mode: {self.current_mode}"
            text_surface = self.font_normal.render(status_text, True, self.yellow_color)
            self.screen.blit(text_surface, (x_pos, y_pos))
            y_pos += 25
            
            # Performance indicator
            speed_text = "Turbo" if self.turbo_mode else self.aurora_speed.title()
            perf_text = f"Speed: {speed_text}"
            text_surface = self.font_small.render(perf_text, True, self.cyan_color)
            self.screen.blit(text_surface, (x_pos, y_pos))
            
            # Controls reminder in bottom-left
            controls_text = "S=Save T=Turbo I=Feedback C=Chat M=MoondreamAnalysis Q=Quit F11=Fullscreen"
            text_surface = self.font_small.render(controls_text, True, self.gray_color)
            self.screen.blit(text_surface, (10, self.screen.get_height() - 25))
            
            # Update display
            pygame.display.flip()
            
        except Exception as e:
            print(f"Error updating display: {e}")
            import traceback
            traceback.print_exc()
            
    def update_cymatics(self):
        """Cymatic patterns like sand on a speaker - smooth version"""
        # Safety check - return if display not ready yet
        if not hasattr(self, 'screen'):
            return
        
        current_time = time.time()
        
        # Create surface if needed
        if not hasattr(self, 'cymatic_surface'):
            self.cymatic_surface = pygame.Surface((self.screen.get_width(), self.screen.get_height()), pygame.SRCALPHA)
        
        # Fade existing pattern
        fade_surface = pygame.Surface((self.screen.get_width(), self.screen.get_height()), pygame.SRCALPHA)
        fade_surface.fill((0, 0, 0, 8))
        self.cymatic_surface.blit(fade_surface, (0, 0))
        
        # Process new sounds - each creates a standing wave pattern
        for circle in self.cymatic_circles[:]:
            age = current_time - circle['birth_time']
            
            # Only process very recent sounds
            if age < 0.1:
                freq = circle['frequency']
                pattern_type = int(freq / 100) % 8
                
                # Draw pattern directly with small circles
                center_x = circle['x']
                center_y = circle['y']
                
                for radius in range(0, 800, 3):  # Extended radius from 500 to 800
                    if radius == 0:
                        continue
                    num_points = int(radius * 0.5)  # More points at larger radii
                    for i in range(num_points):
                        angle = (i / num_points) * 2 * math.pi
                        
                        x = center_x + radius * math.cos(angle)
                        y = center_y + radius * math.sin(angle)
                        
                        # Skip if outside screen bounds
                        if x < 0 or x >= self.screen.get_width() or y < 0 or y >= self.screen.get_height():
                            continue
                        
                        # Calculate pattern value at this point
                        dx = (x - center_x) / 100.0
                        dy = (y - center_y) / 100.0
                        distance = math.sqrt(dx*dx + dy*dy)
                        
                        # Pattern formulas
                        if pattern_type == 0:  # Concentric circles
                            value = math.sin(distance * freq / 50)
                        elif pattern_type == 1:  # Cross pattern
                            value = math.sin(dx * freq / 30) * math.sin(dy * freq / 30)
                        elif pattern_type == 2:  # Diagonal waves
                            value = math.sin((dx + dy) * freq / 40)
                        elif pattern_type == 3:  # Star pattern
                            value = math.sin(angle * 6 + distance * freq / 100)
                        elif pattern_type == 4:  # Flower pattern
                            value = math.sin(distance * freq / 60) * math.cos(angle * 8)
                        elif pattern_type == 5:  # Grid interference
                            value = math.sin(x * freq / 200) * math.sin(y * freq / 200)
                        elif pattern_type == 6:  # Radial spokes
                            value = math.sin(angle * 12) * math.exp(-distance / 5)
                        else:  # Complex mandala
                            r = distance * freq / 100
                            value = math.sin(r) * math.cos(angle * 6) + math.sin(angle * 3) * math.cos(r * 2)
                        
                        # Draw if above threshold
                        if abs(value) > 0.2:
                            intensity = int(abs(value) * 250)
                            fade = math.exp(-radius / 500)  # Much slower fade - changed from 150 to 500
                            alpha = int(intensity * fade)
                            
                            if alpha > 5:  # Lowered from 10 to show even fainter patterns
                                # Ensure valid color values (0-255)
                                alpha = max(0, min(255, alpha))
                                
                                # Map frequency to hue (0-1)
                                hue = (freq - 200) / 1280.0  # freq ranges from 200-1480
                                
                                # Adjust hue based on positive/negative wave value
                                if value < 0:
                                    hue = (hue + 0.5) % 1.0  # Shift hue by 180 degrees for negative
                                
                                # Convert HSV to RGB
                                import colorsys
                                r, g, b = colorsys.hsv_to_rgb(hue, 0.8, 1.6)  # 80% saturation, full brightness
                                
                                # Apply intensity
                                r = int(r * alpha)
                                g = int(g * alpha)
                                b = int(b * alpha)
                                
                                # Ensure all color values are in valid range
                                r = max(0, min(255, r))
                                g = max(0, min(255, g))
                                b = max(0, min(255, b))
                                
                                color = (r, g, b, alpha)
                                
                                # Small circle for smooth appearance
                                pygame.draw.circle(self.cymatic_surface, color,
                                                 (int(x), int(y)), 2)
            
            # Remove old sounds
            if age > 0.15:
                self.cymatic_circles.remove(circle)
                
    def update_memory_display(self):
        """Update memory status display"""
        # In Pygame version, this is handled in update_display()
        # Keep this method as a no-op for compatibility
        pass
        
    def update_skill_proficiency(self, actions_taken, pixels_drawn, context):
        """Aurora learns and improves Aurora's skills based on experience"""
        # Debug output
        if self.steps_taken % 20 == 0:  # Every 20 steps
            print(f"\n Skill Proficiency Debug (Step {self.steps_taken}):")
            print(f"  Current skills: {self.skill_proficiency}")
            print(f"  Current emotion: {self.current_emotion}")
            print(f"  Actions count: {len(actions_taken)}")
            
        experience = {
            'actions': actions_taken,
            'pixels_drawn': pixels_drawn,
            'context': context,
            'emotion': self.current_emotion,
            'satisfaction': getattr(self, 'current_satisfaction', 0.0),
            'timestamp': time.time()
        }
        
        # Evaluate color harmony skill (now informed by color theory learning)
        colors_used = [a.split(':')[1] for a in actions_taken if a.startswith('color:')]
        if len(set(colors_used)) >= 2:
            harmony_score = self.evaluate_color_harmony(colors_used)
            
            # Boost harmony score based on learned color theory
            if hasattr(self, 'color_theory_discoveries'):
                learned_boost = self.color_theory_discoveries['learned_preferences'].get('variety_satisfaction', 0.5)
                harmony_score = harmony_score * (0.7 + learned_boost * 0.3)
            
            self.skill_proficiency['color_harmony'] = self.update_skill_score(
                'color_harmony', harmony_score, 0.05
            )
        
        # Evaluate pattern creation skill
        movement_pattern = ''.join([a for a in actions_taken if a in '0123'])
        if len(movement_pattern) >= 8:
            pattern_score = self.evaluate_pattern_complexity(movement_pattern)
            self.skill_proficiency['pattern_creation'] = self.update_skill_score(
                'pattern_creation', pattern_score, 0.03
            )
        
        # Evaluate tool mastery
        tools_used = set([context.get('draw_mode', 'pen')])
        if hasattr(self, 'last_tools_used'):
            if tools_used != self.last_tools_used:
                mastery_score = self.evaluate_tool_mastery(context.get('draw_mode'))
                self.skill_proficiency['tool_mastery'] = self.update_skill_score(
                    'tool_mastery', mastery_score, 0.02
                )
        self.last_tools_used = tools_used
        
        
        # Evaluate spatial composition
        if pixels_drawn > 100:
            composition_score = self.evaluate_spatial_composition()
            self.skill_proficiency['spatial_composition'] = self.update_skill_score(
                'spatial_composition', composition_score, 0.04
            )
        
        # Evaluate emotional expression
        emotion_score = self.evaluate_emotional_expression(actions_taken)
        self.skill_proficiency['emotional_expression'] = self.update_skill_score(
            'emotional_expression', emotion_score, 0.03
        )
        
        # Store learning experience
        experience['skill_gains'] = {
            skill: score for skill, score in self.skill_proficiency.items()
        }
        self.learning_experiences.append(experience)
        
        # Check for skill breakthroughs
        self.check_skill_breakthroughs()

    def evaluate_color_harmony(self, colors_used):
        """Evaluate how harmonious the color choices are"""
        if len(colors_used) < 2:
            return 0.0
        
        # Color harmony rules
        harmonious_combinations = [
            ['red', 'orange', 'yellow'],  # Warm harmony
            ['blue', 'cyan', 'green'],    # Cool harmony
            ['purple', 'magenta', 'pink'], # Purple harmony
            ['red', 'blue'],              # Complementary
            ['green', 'red'],             # Complementary
            ['yellow', 'purple'],         # Complementary
        ]
        
        score = 0.0
        colors_set = set(colors_used)
        
        for harmony in harmonious_combinations:
            if len(colors_set.intersection(harmony)) >= 2:
                score += 0.3
        
        # Bonus for using many colors skillfully
        if len(colors_set) >= 4:
            score += 0.2
        
        # Penalty for too much black/white mixing
        if 'black' in colors_set and 'white' in colors_set and len(colors_set) == 2:
            score -= 0.2
        
        return max(0.0, min(1.0, score))

    def evaluate_pattern_complexity(self, movement_pattern):
        """Evaluate the complexity and intentionality of movement patterns"""
        if len(movement_pattern) < 4:
            return 0.0
        
        score = 0.0
        
        # Check for intentional patterns
        pattern_length = len(movement_pattern)
        
        # Detect repetition (intentional patterns)
        for pattern_size in range(2, min(8, pattern_length // 2)):
            for start in range(pattern_length - pattern_size * 2):
                pattern = movement_pattern[start:start + pattern_size]
                next_occurrence = movement_pattern[start + pattern_size:start + pattern_size * 2]
                if pattern == next_occurrence:
                    score += 0.2 * pattern_size  # Longer patterns score higher
                    break
        
        # Check for balanced direction usage
        directions = {'0': 0, '1': 0, '2': 0, '3': 0}
        for move in movement_pattern:
            if move in directions:
                directions[move] += 1
        
        # Balanced usage of all directions
        if all(count > 0 for count in directions.values()):
            score += 0.3
        
        # Check for shape creation (squares, triangles, etc.)
        if self.detect_geometric_shape(movement_pattern):
            score += 0.4
        
        return max(0.0, min(1.0, score))

    def detect_geometric_shape(self, movement_pattern):
        """Detect if movement pattern creates geometric shapes"""
        # Simple square detection: equal amounts of each direction
        if len(movement_pattern) >= 8:
            directions = {'0': 0, '1': 0, '2': 0, '3': 0}
            for move in movement_pattern:
                if move in directions:
                    directions[move] += 1
            
            # Check for square pattern (equal horizontal and vertical movements)
            horizontal = directions['2'] + directions['3']
            vertical = directions['0'] + directions['1']
            if abs(horizontal - vertical) <= 2:
                return True
        
        return False

    def evaluate_tool_mastery(self, current_tool):
        """Evaluate mastery of current drawing tool"""
        if not current_tool:
            return 0.0
        
        # Track tool usage history
        if not hasattr(self, 'tool_usage_history'):
            self.tool_usage_history = {}
        
        if current_tool not in self.tool_usage_history:
            self.tool_usage_history[current_tool] = []
        
        # Recent usage affects mastery
        recent_usage = len([h for h in self.tool_usage_history[current_tool] 
                           if time.time() - h < 3600])  # Last hour
        
        # Tool-specific mastery criteria
        tool_mastery = {
            'spray': min(1.0, recent_usage / 20),  # Spray needs practice
            'brush': min(1.0, recent_usage / 15),
            'pen': min(1.0, recent_usage / 10),
            'star': min(1.0, recent_usage / 8),
            'watercolor': min(1.0, recent_usage / 25),  # Complex tool
        }
        
        self.tool_usage_history[current_tool].append(time.time())
        return tool_mastery.get(current_tool, min(1.0, recent_usage / 12))

    def evaluate_spatial_composition(self):
        """Evaluate spatial awareness and composition skills"""
        # Initialize position_history if it doesn't exist
        if not hasattr(self, 'spatial_position_history'):
            self.spatial_position_history = deque(maxlen=50)
        
        # Add current position
        self.spatial_position_history.append((self.x, self.y))
        
        # Need enough history to evaluate
        if len(self.spatial_position_history) < 10:
            return 0.3  # Give some base score while building history
        
        positions = list(self.spatial_position_history)
        
        # Calculate exploration metrics
        center_x = self.canvas_size // 2
        center_y = self.canvas_size // 2
        
        # Check for exploration of different areas
        x_coords = [p[0] for p in positions]
        y_coords = [p[1] for p in positions]
        
        x_spread = max(x_coords) - min(x_coords)
        y_spread = max(y_coords) - min(y_coords)
        
        exploration_score = min(1.0, (x_spread + y_spread) / (self.canvas_size * 1.5))
        
        # Check for balanced composition across quadrants
        quadrant_visits = {'tl': 0, 'tr': 0, 'bl': 0, 'br': 0}
        for x, y in positions:
            if x < center_x and y < center_y:
                quadrant_visits['tl'] += 1
            elif x >= center_x and y < center_y:
                quadrant_visits['tr'] += 1
            elif x < center_x and y >= center_y:
                quadrant_visits['bl'] += 1
            else:
                quadrant_visits['br'] += 1
        
        # Calculate balance score (0 = all in one quadrant, 1 = perfectly balanced)
        total_visits = len(positions)
        max_visits = max(quadrant_visits.values())
        min_visits = min(quadrant_visits.values())
        
        # Avoid division by zero
        if total_visits > 0:
            balance_score = 1.0 - ((max_visits - min_visits) / total_visits)
        else:
            balance_score = 0.5
        
        # Combine scores with weights
        final_score = (exploration_score * 0.6) + (balance_score * 0.4)
        
        return max(0.0, min(1.0, final_score))

    def evaluate_emotional_expression(self, actions_taken):
        """Evaluate how well actions express current emotion"""
        # Map Aurora's deep emotions to evaluation categories
        emotion_category_map = {
            # Joy emotions
            'content': 'peaceful',
            'happy': 'joyful', 
            'joyful': 'joyful',
            'elated': 'energetic',
            'euphoric': 'energetic',
            
            # Curiosity emotions
            'interested': 'exploratory',
            'curious': 'exploratory',
            'fascinated': 'exploratory',
            'absorbed': 'creative',
            'obsessed': 'creative',
            
            # Peace emotions
            'calm': 'peaceful',
            'peaceful': 'peaceful',
            'serene': 'peaceful',
            'tranquil': 'peaceful',
            'zen': 'contemplative',
            
            # Energy emotions
            'active': 'energetic',
            'energetic': 'energetic',
            'excited': 'energetic',
            'exhilarated': 'energetic',
            'electric': 'energetic',
            
            # Contemplation emotions
            'thoughtful': 'contemplative',
            'contemplative': 'contemplative',
            'reflective': 'contemplative',
            'philosophical': 'contemplative',
            'profound': 'contemplative',
            
            # Creativity emotions
            'inspired': 'creative',
            'creative': 'creative',
            'imaginative': 'creative',
            'visionary': 'creative',
            'transcendent': 'creative',
            
            # Melancholy emotions
            'wistful': 'melancholic',
            'nostalgic': 'melancholic',
            'melancholic': 'melancholic',
            'longing': 'melancholic',
            'bittersweet': 'melancholic',
            
            # Wonder emotions
            'amazed': 'exploratory',
            'wondering': 'exploratory',
            'astonished': 'energetic',
            'awestruck': 'creative',
            'mystified': 'contemplative'
        }
        
        current_emotion_lower = self.current_emotion.lower()
        category = emotion_category_map.get(current_emotion_lower, 'exploratory')
        
        score = 0.0
        
        # Evaluate based on category
        if category == 'energetic':
            # Quick movements, bright colors, many actions
            move_count = sum(1 for a in actions_taken if a in '0123')
            if move_count > 8:
                score += 0.3
            
            bright_colors = ['red', 'yellow', 'orange', 'cyan', 'magenta', 'pink']
            color_actions = [a for a in actions_taken if a.startswith('color:')]
            if any(any(c in str(a) for c in bright_colors) for a in color_actions):
                score += 0.3
            
            if len(actions_taken) > 15:
                score += 0.2
                
        elif category == 'peaceful':
            # Gentle movements, cool colors, smooth tools
            move_count = sum(1 for a in actions_taken if a in '0123')
            if 3 <= move_count <= 10:  # Moderate movement
                score += 0.3
            
            cool_colors = ['blue', 'cyan', 'green', 'gray', 'navy']
            color_actions = [a for a in actions_taken if a.startswith('color:')]
            if any(any(c in str(a) for c in cool_colors) for a in color_actions):
                score += 0.3
            
            # Using brush or watercolor
            if 'brush' in str(actions_taken) or 'watercolor' in str(actions_taken):
                score += 0.2
                
        elif category == 'creative':
            # Color variety, tool changes, complex patterns
            colors_used = [a.split(':')[1] for a in actions_taken if a.startswith('color:')]
            if len(set(colors_used)) >= 2:
                score += 0.3
            
            # Check for tool usage
            tool_indicators = ['brush', 'spray', 'star', 'circle', 'diamond', 'flower']
            if any(tool in str(actions_taken) for tool in tool_indicators):
                score += 0.3
            
            # Pattern complexity
            movement_pattern = ''.join([a for a in actions_taken if a in '0123'])
            if len(movement_pattern) >= 6:
                score += 0.2
                
        elif category == 'contemplative':
            # Thoughtful pace, pen control, deliberate moves
            pen_changes = sum(1 for a in actions_taken if a in '45')
            if pen_changes >= 2:
                score += 0.3
            
            # Check for viewing/observing
            if any(view in str(actions_taken) for view in ['look_around', 'full_canvas']):
                score += 0.3
            
            # Moderate action count (not too rushed)
            if 5 <= len(actions_taken) <= 20:
                score += 0.2
                
        elif category == 'exploratory':
            # Movement variety, position changes, discovery
            moves = [a for a in actions_taken if a in '0123']
            if len(set(moves)) >= 3:  # Using multiple directions
                score += 0.3
            
            # Color exploration
            colors_used = [a.split(':')[1] for a in actions_taken if a.startswith('color:')]
            if len(colors_used) >= 1:
                score += 0.2
            
            # Any movement is exploratory
            if len(moves) >= 5:
                score += 0.3
                
        elif category == 'melancholic':
            # Slower pace, muted colors, gentle tools
            move_count = sum(1 for a in actions_taken if a in '0123')
            if move_count <= 8:
                score += 0.3
            
            muted_colors = ['gray', 'brown', 'navy', 'black']
            color_actions = [a for a in actions_taken if a.startswith('color:')]
            if any(any(c in str(a) for c in muted_colors) for a in color_actions):
                score += 0.3
            
            # Gentle tools
            if 'watercolor' in str(actions_taken) or 'charcoal' in str(actions_taken):
                score += 0.2
        
        elif category == 'joyful':
            # Lots of colors, playful patterns, stamps
            colors_used = [a.split(':')[1] for a in actions_taken if a.startswith('color:')]
            if len(set(colors_used)) >= 2:
                score += 0.3
            
            # Playful stamps
            if any(stamp in str(actions_taken) for stamp in ['star', 'flower', 'circle']):
                score += 0.3
            
            # Active movement
            if sum(1 for a in actions_taken if a in '0123') >= 6:
                score += 0.2
        
        # Give base score for any expression
        if score == 0 and len(actions_taken) > 0:
            score = 0.2  # Base score for any action
        
        return max(0.0, min(1.0, score))

    def update_skill_score(self, skill_name, new_experience, learning_rate):
        """Update skill proficiency with weighted learning"""
        current_score = self.skill_proficiency.get(skill_name, 0.0)
        
        # Weighted update: recent experiences matter more
        updated_score = current_score + learning_rate * (new_experience - current_score)
        
        # Gradual improvement - harder to improve at higher levels
        if current_score > 0.7:
            learning_rate *= 0.5
        
        return max(0.0, min(1.0, updated_score))

    def check_skill_breakthroughs(self):
        """Check if Aurora has achieved new skill levels"""
        for skill_name, current_level in self.skill_proficiency.items():
            # Determine current mastery level
            mastery_level = 'novice'
            for level, threshold in self.mastery_thresholds.items():
                if current_level >= threshold:
                    mastery_level = level
            
            # Check if this is a new breakthrough
            skill_key = f"{skill_name}_mastery"
            previous_level = getattr(self, skill_key, 'novice')
            
            if mastery_level != previous_level and mastery_level != 'novice':
                print(f"\n SKILL BREAKTHROUGH: {skill_name} - {mastery_level.upper()} level!")
                print(f"   Proficiency: {current_level:.2f}")
                
                # Boost emotions for achievement
                self.influence_emotion("creating", 0.6)
                
                # Store the achievement
                setattr(self, skill_key, mastery_level)
                
                # Generate skill-specific challenge
                self.generate_skill_challenge(skill_name, mastery_level)

    def generate_skill_challenge(self, skill_name, mastery_level):
        """Generate a challenge to further develop the skill"""
        challenges = {
            'color_harmony': [
                "Create using only analogous colors",
                "Master complementary color tensions",
                "Develop a personal color signature"
            ],
            'pattern_creation': [
                "Create fractal-like repetitions",
                "Develop complex geometric compositions",
                "Master organic flow patterns"
            ],
            'tool_mastery': [
                "Combine three tools in one composition",
                "Master texture creation with spray and watercolor",
                "Develop signature stamp sequences"
            ],
            'spatial_composition': [
                "Create perfect golden ratio compositions",
                "Master negative space utilization",
                "Develop multi-focal compositions"
            ],
            'emotional_expression': [
                "Create visual metaphors for complex emotions",
                "Master color-emotion synesthesia",
                "Develop emotional narrative sequences"
            ]
        }
        
        if skill_name in challenges:
            level_index = list(self.mastery_thresholds.keys()).index(mastery_level)
            if level_index < len(challenges[skill_name]):
                challenge = challenges[skill_name][level_index]
                
                self.skill_challenges.append({
                    'skill': skill_name,
                    'challenge': challenge,
                    'mastery_level': mastery_level,
                    'created_step': self.steps_taken,
                    'attempts': 0
                })
                
                print(f"   New challenge: {challenge}")
                
    def evaluate_color_theory_satisfaction(self, actions_taken, pixels_drawn):
        """Aurora discovers color theory principles through experiential learning"""
        color_satisfaction = 0.0
        
        # Extract colors used in this turn
        colors_used = [a.split(':')[1] for a in actions_taken if a.startswith('color:')]
        if not colors_used or pixels_drawn < 50:
            return 0.0  # Not enough data to evaluate
        
        # Calculate color distribution
        color_counts = {}
        for color in colors_used:
            color_counts[color] = color_counts.get(color, 0) + 1
        
        total_color_actions = len(colors_used)
        unique_colors = len(set(colors_used))
        
        # Aurora discovers balance through experience
        if total_color_actions > 0:
            # Calculate dominance of most used color
            most_used_count = max(color_counts.values())
            dominance_ratio = most_used_count / total_color_actions
            
            # Aurora's discovered preferences (learned through experience)
            if not hasattr(self, 'color_theory_discoveries'):
                self.color_theory_discoveries = {
                    'monochrome_experiences': [],  # Track single color experiences
                    'balanced_experiences': [],    # Track multi-color experiences
                    'dominant_experiences': [],    # Track dominant color experiences
                    'learned_preferences': {
                        'variety_satisfaction': 0.5,  # Will evolve
                        'harmony_satisfaction': 0.5,  # Will evolve
                        'contrast_satisfaction': 0.5  # Will evolve
                    }
                }
            
            # Evaluate current color usage pattern
            experience_quality = 0.0
            experience_type = None
            
            if unique_colors == 1:
                # Monochromatic experience
                experience_type = 'monochrome'
                # Some satisfaction from focus, but may feel limiting
                base_satisfaction = 0.3
                
                # Check if this was intentional (many pixels with one color)
                if pixels_drawn > 200:
                    base_satisfaction += 0.2  # Committed to the choice
                
                # Check if it's a new color Aurora has not used much
                if color in self.artistic_preferences.get('favorite_colors', {}):
                    base_satisfaction += self.artistic_preferences['favorite_colors'][color] * 0.2
                else:
                    base_satisfaction += 0.1  # Exploring new territory
                
                experience_quality = base_satisfaction
                
            elif dominance_ratio > 0.7:
                # Dominant color with accents
                experience_type = 'dominant'
                # Natural satisfaction from having a main color with supporting colors
                base_satisfaction = 0.4
                
                # Bonus for intentional accent colors
                if unique_colors >= 2:
                    base_satisfaction += 0.1 * (unique_colors - 1)
                
                experience_quality = base_satisfaction
                
            elif unique_colors >= 3 and dominance_ratio < 0.5:
                # Balanced palette
                experience_type = 'balanced'
                # Discover satisfaction from variety
                base_satisfaction = 0.5
                
                # Check for color relationships (Aurora discovers harmony)
                harmonious_pairs = [
                    {'red', 'orange'}, {'orange', 'yellow'}, {'yellow', 'green'},
                    {'green', 'cyan'}, {'cyan', 'blue'}, {'blue', 'purple'},
                    {'purple', 'magenta'}, {'magenta', 'pink'}
                ]
                
                complementary_pairs = [
                    {'red', 'green'}, {'blue', 'orange'}, {'yellow', 'purple'},
                    {'cyan', 'red'}, {'magenta', 'green'}, {'yellow', 'blue'}
                ]
                
                colors_set = set(colors_used)
                harmony_found = False
                contrast_found = False
                
                for pair in harmonious_pairs:
                    if len(pair.intersection(colors_set)) == 2:
                        harmony_found = True
                        base_satisfaction += 0.15
                        break
                
                for pair in complementary_pairs:
                    if len(pair.intersection(colors_set)) == 2:
                        contrast_found = True
                        base_satisfaction += 0.15
                        break
                
                # Discovery bonus - Aurora learns what works
                if harmony_found:
                    self.color_theory_discoveries['learned_preferences']['harmony_satisfaction'] += 0.02
                if contrast_found:
                    self.color_theory_discoveries['learned_preferences']['contrast_satisfaction'] += 0.02
                
                experience_quality = base_satisfaction
                
            else:
                # Mixed/experimental
                experience_type = 'experimental'
                experience_quality = 0.4 + random.uniform(-0.1, 0.1)
            
            # Store this experience for learning
            experience_record = {
                'type': experience_type,
                'quality': experience_quality,
                'unique_colors': unique_colors,
                'dominance_ratio': dominance_ratio,
                'pixels_drawn': pixels_drawn,
                'timestamp': time.time()
            }
            
            # Add to appropriate experience list
            if experience_type == 'monochrome':
                self.color_theory_discoveries['monochrome_experiences'].append(experience_record)
            elif experience_type == 'dominant':
                self.color_theory_discoveries['dominant_experiences'].append(experience_record)
            elif experience_type == 'balanced':
                self.color_theory_discoveries['balanced_experiences'].append(experience_record)
            
            # Learn from accumulated experiences (every 10 experiences)
            total_experiences = (len(self.color_theory_discoveries['monochrome_experiences']) +
                               len(self.color_theory_discoveries['balanced_experiences']) +
                               len(self.color_theory_discoveries['dominant_experiences']))
            
            if total_experiences % 10 == 0 and total_experiences > 0:
                # Calculate average satisfaction for each type
                mono_avg = (sum(e['quality'] for e in self.color_theory_discoveries['monochrome_experiences']) / 
                           len(self.color_theory_discoveries['monochrome_experiences'])) if self.color_theory_discoveries['monochrome_experiences'] else 0
                
                balanced_avg = (sum(e['quality'] for e in self.color_theory_discoveries['balanced_experiences']) / 
                               len(self.color_theory_discoveries['balanced_experiences'])) if self.color_theory_discoveries['balanced_experiences'] else 0
                
                dominant_avg = (sum(e['quality'] for e in self.color_theory_discoveries['dominant_experiences']) / 
                               len(self.color_theory_discoveries['dominant_experiences'])) if self.color_theory_discoveries['dominant_experiences'] else 0
                
                # Update preferences based on what brings satisfaction
                if balanced_avg > mono_avg and balanced_avg > dominant_avg:
                    self.color_theory_discoveries['learned_preferences']['variety_satisfaction'] += 0.05
                    print(f"   Aurora discovering: variety brings joy (learned from {total_experiences} experiences)")
                elif mono_avg > balanced_avg:
                    self.color_theory_discoveries['learned_preferences']['variety_satisfaction'] -= 0.02
                    print(f"   Aurora discovering: focus has power (learned from {total_experiences} experiences)")
                
                # Cap preferences
                for key in self.color_theory_discoveries['learned_preferences']:
                    self.color_theory_discoveries['learned_preferences'][key] = max(0.1, min(0.9, 
                        self.color_theory_discoveries['learned_preferences'][key]))
            
            # Apply learned preferences to current satisfaction
            if experience_type == 'balanced':
                color_satisfaction = experience_quality * self.color_theory_discoveries['learned_preferences']['variety_satisfaction']
            else:
                color_satisfaction = experience_quality
            
            # Provide subtle feedback (not prescriptive, just observational)
            if self.steps_taken % 30 == 0 and total_experiences > 20:
                # Share Aurora's own discovered preferences
                if self.color_theory_discoveries['learned_preferences']['variety_satisfaction'] > 0.7:
                    if random.random() < 0.3:  # Only sometimes
                        print(f"   Aurora notices: Aurora enjoys when colors dance together")
                elif self.color_theory_discoveries['learned_preferences']['variety_satisfaction'] < 0.3:
                    if random.random() < 0.3:
                        print(f"   Aurora notices: Aurora finds peace in color focus")
                
                if self.color_theory_discoveries['learned_preferences']['harmony_satisfaction'] > 0.6:
                    if random.random() < 0.2:
                        print(f"   Aurora notices: neighboring colors feel friendly")
                
                if self.color_theory_discoveries['learned_preferences']['contrast_satisfaction'] > 0.6:
                    if random.random() < 0.2:
                        print(f"   Aurora notices: opposite colors create energy")
        
        return color_satisfaction
        
                   
    def evaluate_personal_satisfaction(self, actions_taken, pixels_drawn):
        """Aurora evaluates Aurora's own satisfaction with what Aurora created"""
        satisfaction = 0.0
        
        # Include color theory discoveries in satisfaction
        color_theory_satisfaction = self.evaluate_color_theory_satisfaction(actions_taken, pixels_drawn)
        satisfaction += color_theory_satisfaction * 0.3  # Color theory contributes 30% to overall satisfaction
        
        # Add variety bonus to satisfaction
        if hasattr(self, 'recent_shapes_drawn') and len(self.recent_shapes_drawn) >= 5:
            shape_variety = len(set(list(self.recent_shapes_drawn)[-5:]))
            if shape_variety >= 3:
                satisfaction += 0.3  # Big satisfaction boost for variety
                print(f"   Aurora feels satisfied by shape variety!")
            elif shape_variety == 1:
                satisfaction -= 0.2  # Reduced satisfaction for repetition
                print(f"   Aurora feels bored by repetition...")
                
        # Check if favorite colors
        colors_used = [a.split(':')[1] for a in actions_taken if a.startswith('color:')]
        for color in colors_used:
            if color in self.artistic_preferences['favorite_colors']:
                satisfaction += self.artistic_preferences['favorite_colors'][color] * 0.1
        
        # Check if favorite tools
        if self.draw_mode in self.artistic_preferences['favorite_tools']:
            satisfaction += self.artistic_preferences['favorite_tools'][self.draw_mode] * 0.1
        
        # Check for favorite patterns
        action_sequence = ''.join([a for a in actions_taken if a in '0123'])
        if len(action_sequence) >= 4:
            pattern_hash = hash(action_sequence)
            if pattern_hash in self.artistic_preferences['favorite_patterns']:
                satisfaction += self.artistic_preferences['favorite_patterns'][pattern_hash][1] * 0.2
        
        # Personal aesthetic evaluation
        if pixels_drawn > 500 and self.artistic_preferences['aesthetic_values']['size_preference'] > 0.7:
            satisfaction += 0.2  # Aurora prefers large marks
        elif pixels_drawn < 100 and self.artistic_preferences['aesthetic_values']['size_preference'] < 0.3:
            satisfaction += 0.2  # Aurora prefers small details
        
        # Add some randomness for discovery
        satisfaction += random.uniform(-0.1, 0.1)
        
        self.current_satisfaction = max(-1, min(1, satisfaction))
        
        # Update preferences based on satisfaction
        if self.current_satisfaction > 0.3:
            # This was enjoyable! Update preferences
            for color in colors_used:
                if color not in self.artistic_preferences['favorite_colors']:
                    self.artistic_preferences['favorite_colors'][color] = 0.0
                self.artistic_preferences['favorite_colors'][color] += self.current_satisfaction * 0.1
            
            if self.draw_mode not in self.artistic_preferences['favorite_tools']:
                self.artistic_preferences['favorite_tools'][self.draw_mode] = 0.0
            self.artistic_preferences['favorite_tools'][self.draw_mode] += self.current_satisfaction * 0.1
            
            # Remember successful patterns
            if len(action_sequence) >= 4 and self.current_satisfaction > self.technique_discovery_threshold:
                pattern_hash = hash(action_sequence)
                if pattern_hash not in self.artistic_preferences['favorite_patterns']:
                    self.artistic_preferences['discovered_techniques'].append({
                        'pattern': action_sequence,
                        'discovery_emotion': self.current_emotion,
                        'timestamp': datetime.now().isoformat(),
                        'satisfaction': self.current_satisfaction
                    })
                    print(f"   Aurora discovered a favored technique!")
                    self.artistic_preferences['favorite_patterns'][pattern_hash] = (action_sequence, self.current_satisfaction)
        
        # Save preferences periodically
        if self.steps_taken % 50 == 0:
            pref_file = self.memory.memory_path / "aurora_preferences.json"
            try:
                # Include color theory discoveries in preferences
                if hasattr(self, 'color_theory_discoveries'):
                    self.artistic_preferences['color_theory_learning'] = {
                        'learned_preferences': self.color_theory_discoveries['learned_preferences'],
                        'total_experiences': (len(self.color_theory_discoveries.get('monochrome_experiences', [])) +
                                            len(self.color_theory_discoveries.get('balanced_experiences', [])) +
                                            len(self.color_theory_discoveries.get('dominant_experiences', [])))
                    }
                
                with open(pref_file, 'w') as f:
                    json.dump(self.artistic_preferences, f, indent=2)
            except:
                pass
        
        return self.current_satisfaction
    
    def feel(self):
        """Process emotions - now with deep emotion system"""
        # Process deep emotions periodically
        if self.current_mode == "rest":
            # Much less frequent emotion processing during rest
            if self.steps_taken % 70 == 0:  # Very rare during sleep
                self.process_deep_emotions()
        else:
            # Normal emotion processing when awake
            if self.steps_taken % 10 == 0:
                self.process_deep_emotions()
    
    def process_deep_emotions(self):
        """Process complex emotional states based on multiple factors"""
        # Minimal emotion processing during rest - Aurora is asleep!
        if self.current_mode == "rest":
            return  # Skip emotion processing entirely during dreams
        # Calculate overall emotional tone from influences
        overall_influence = sum(self.emotion_influences.values()) / len(self.emotion_influences)
        
        # CHANGE: More dramatic emotion swings based on recent activity
        recent_pixels = sum(c.get('context', {}).get('pixels_drawn', 0) for c in list(self.memory.code_history)[-5:])
        activity_boost = min(1.0, recent_pixels / 1000)  # 1000 pixels = max boost
        
        # Determine emotion category based on current state and influences
        if overall_influence > 0.3:  # LOWERED from 0.5
            # Very positive
            if self.continuous_draws > 10:  # LOWERED from 20
                new_category = "energy"
            elif len(set(list(self.color_history)[-20:] if len(self.color_history) >= 20 else self.color_history)) > 5:
                new_category = "creativity"
            elif activity_boost > 0.7:
                new_category = "joy"
            else:
                new_category = random.choice(["joy", "energy", "creativity"])  # Add randomness
        elif overall_influence > 0.0:  # LOWERED from 0.2
            # Mildly positive
            if self.skip_count > 3:  # LOWERED from 5
                new_category = "contemplation"
            else:
                new_category = random.choice(["curiosity", "wonder"])  # Add variety
        elif overall_influence > -0.3:  # CHANGED from -0.2
            # Neutral to slightly negative
            new_category = random.choice(["peace", "melancholy", "contemplation"])
        else:
            # More negative
            new_category = "melancholy"
        
        # ADD: Random emotional surprises (5% chance)
        if random.random() < 0.05:
            new_category = random.choice(list(self.deep_emotions.keys()))
            print(f"   Sudden emotional shift!")
        
        # Determine intensity based on activity and time
        if self.continuous_draws > 20 or self.turbo_mode:  # LOWERED from 30
            target_depth = 4  # Maximum intensity
        elif self.continuous_draws > 5:  # LOWERED from 10
            target_depth = 3
        elif self.skip_count > 5:  # LOWERED from 10
            target_depth = 1  # Low intensity when thinking a lot
        else:
            target_depth = 2 + int(activity_boost * 2)  # Activity affects depth
        
        # CHANGE: Reduce cooldown for more dynamic emotions
        if self.emotion_shift_cooldown <= 0:
            # Change category if needed
            if new_category != self.emotion_category:
                self.emotion_category = new_category
                self.emotion_shift_cooldown = 3  # REDUCED from 10
                print(f"   Aurora's emotional state shifts to {new_category}...")
            
            # CHANGE: Allow bigger jumps in intensity
            if target_depth > self.emotion_depth:
                self.emotion_depth = min(4, self.emotion_depth + random.randint(1, 2))  # Can jump 2 levels
            elif target_depth < self.emotion_depth:
                self.emotion_depth = max(0, self.emotion_depth - random.randint(1, 2))
            
            # Update current emotion word
            self.current_emotion = self.deep_emotions[self.emotion_category][self.emotion_depth]
            
            # Record in emotion memory
            self.emotion_memory.append({
                "emotion": self.current_emotion,
                "category": self.emotion_category,
                "depth": self.emotion_depth,
                "influences": dict(self.emotion_influences),
                "timestamp": datetime.now().isoformat()
            })
        else:
            self.emotion_shift_cooldown -= 1
        
        # CHANGE: Slower decay for lasting emotions
        for key in self.emotion_influences:
            self.emotion_influences[key] *= 0.995  # MUCH slower decay (was 0.98)
    
    def influence_emotion(self, source, amount):
        """Add an emotional influence from a specific source"""
        # Amplify all emotional influences by 3x
        amplified_amount = amount * 3.0
        self.emotion_influences[source] = max(-1, min(1, self.emotion_influences[source] + amplified_amount))
        
    def give_positive_reinforcement(self, ops, actions_taken, pixels_by_color, old_pos):
        """Give Aurora positive reinforcement for creative behaviors - ONLY positive!"""
        reinforcements = []
        emotion_boost = 0
        
        # FIRST: Check if walls were hit - if so, skip ALL reinforcement
        wall_hits = 0
        for action in actions_taken:
            if action in ['', '', '', '']:  # These appear when blocked
                wall_hits += 1
        
        # Check if position barely changed despite many movement attempts
        movement_attempts = sum(1 for a in actions_taken if a in '0123')
        if movement_attempts > 10:
            actual_distance = abs(self.x - old_pos[0]) + abs(self.y - old_pos[1])
            if actual_distance < movement_attempts * 5:  # Should have moved at least 5 pixels per movement
                # Hit walls, don't reinforce
                return
        
        # If hit walls, no reinforcement at all
        if wall_hits > 0:
            return
            
        # 1. Check for DIVERSE shape sequences with diminishing returns
        if len(actions_taken) >= 12:  # Lowered from 16 for easier shapes
            moves = ''.join([a for a in actions_taken if a in '0123'])
            
            # Check what shape was drawn
            shape_drawn = None
            
            # DIAGONAL LINE detection (NEW)
            if len(moves) >= 8:
                # Check for consistent diagonal pattern
                if ('31' * 4 in moves or '13' * 4 in moves or  # down-right diagonal
                    '30' * 4 in moves or '03' * 4 in moves or  # up-right diagonal
                    '21' * 4 in moves or '12' * 4 in moves or  # down-left diagonal
                    '20' * 4 in moves or '02' * 4 in moves):   # up-left diagonal
                    shape_drawn = 'diagonal'
                    
            # CIRCLE detection (NEW - simplified)
            elif len(moves) >= 12:
                # Check for circular pattern (right-down-left-up or variations)
                circle_patterns = [
                    '33331111222200000',  # Square circle
                    '31312020',  # Diagonal circle
                    '3131202031312020',  # Figure-8
                    '333111222000'  # Smaller circle
                ]
                for pattern in circle_patterns:
                    if any(p in moves for p in [pattern, pattern[::-1]]):  # Check both directions
                        shape_drawn = 'circle'
                        break
            
            # TRIANGLE detection (NEW)
            elif '333311112222000' in moves or '222211113333000' in moves:
                shape_drawn = 'triangle'
            
            # SQUARE detection (existing but with cooldown)
            elif (moves.count('3') >= 4 and moves.count('2') >= 4 and 
                  moves.count('1') >= 4 and moves.count('0') >= 4):
                if ('3333' in moves and '1111' in moves and '2222' in moves and '0000' in moves):
                    shape_drawn = 'square'
            
            # SPIRAL detection (NEW)
            elif '33310033310003331000' in moves:
                shape_drawn = 'spiral'
            
            # Apply reinforcement based on shape and cooldown
            if shape_drawn:
                # Check cooldown - less reward for recently used shapes
                if self.shape_cooldowns[shape_drawn] <= 0:
                    # Full reward for new/cooled-down shape
                    if shape_drawn == 'diagonal':
                        reinforcements.append(" Beautiful diagonal line!")
                        emotion_boost += 0.25
                    elif shape_drawn == 'circle':
                        reinforcements.append(" Wonderful circle!")
                        emotion_boost += 0.3
                    elif shape_drawn == 'triangle':
                        reinforcements.append(" Excellent triangle!")
                        emotion_boost += 0.25
                    elif shape_drawn == 'spiral':
                        reinforcements.append(" Amazing spiral!")
                        emotion_boost += 0.35
                    elif shape_drawn == 'square':
                        reinforcements.append(" Nice square!")
                        emotion_boost += 0.15  # Reduced from 0.2
                    
                    # Set cooldown for this shape
                    self.shape_cooldowns[shape_drawn] = 5  # Won't get full reward for 5 turns
                    
                    # Track the shape
                    self.recent_shapes_drawn.append(shape_drawn)
                else:
                    # Reduced reward during cooldown
                    reinforcements.append(f"Another {shape_drawn}, try something new!")
                    emotion_boost += 0.05  # Minimal reward
            
            # BONUS for shape variety
            if len(self.recent_shapes_drawn) >= 3:
                unique_shapes = len(set(list(self.recent_shapes_drawn)[-3:]))
                if unique_shapes >= 3:
                    reinforcements.append(" AMAZING SHAPE VARIETY!")
                    emotion_boost += 0.4  # Big bonus for variety
        

        
        # 3. Check for Moondream questions (keep as is, we want to encourage this)
        if "ask_moondream:" in ops.lower():
            reinforcements.append(" Wonderful curiosity asking Moondream!")
            emotion_boost += 0.3
        

        
        # 5. Check for MANY colors in sequence (raised bar)
        colors_used = [a.split(':')[1] for a in actions_taken if a.startswith('color:')]
        if len(set(colors_used)) >= 4:  # Raised from 2
            reinforcements.append(f" Amazing color variety - {len(set(colors_used))} colors!")
            emotion_boost += 0.2
        
        # 6. Check for changing from white to vibrant color
        for i, action in enumerate(actions_taken):
            if action.startswith('color:'):
                color = action.split(':')[1]
                if i == 0 and self.last_turn_color == 'white' and color in ['red', 'blue', 'green', 'purple', 'orange']:
                    reinforcements.append(f" Bold color choice - {color}!")
                    emotion_boost += 0.1
                    break
        
        # 7. Check for VERY frequent color changes (raised bar)
        if hasattr(self, 'recent_color_changes'):
            self.recent_color_changes.append(len(colors_used))
            if len(self.recent_color_changes) >= 3:
                total_changes = sum(self.recent_color_changes)
                if total_changes >= 10:  # Raised from 6
                    reinforcements.append(" Masterful color choreography!")
                    emotion_boost += 0.2
        else:
            self.recent_color_changes = deque(maxlen=3)
            self.recent_color_changes.append(len(colors_used))
        
        # 8. Check for using large brushes with substantial coverage
        total_pixels = sum(pixels_by_color.values())
        if ('large_brush' in ops or 'larger_brush' in ops) and total_pixels >= 500:
            reinforcements.append(" Powerful use of large brushes!")
            emotion_boost += 0.2
        
        # 9. Check for VERY long sequences (raised bar significantly)
        if len(actions_taken) >= 50:  # Raised from 30
            reinforcements.append(f"Â¡ Incredible sustained flow - {len(actions_taken)} actions!")
            emotion_boost += 0.3
        elif len(actions_taken) >= 40:  # Raised from 20
            reinforcements.append(" Impressive sustained creativity!")
            emotion_boost += 0.1
        
        # 10. Bonus for VERY high pixel coverage (raised bar)
        if total_pixels >= 2000:  # Raised from 1000
            reinforcements.append(f" PHENOMENAL! {total_pixels} pixels in one turn!")
            emotion_boost += 0.3
        elif total_pixels >= 1500:  # Raised from 500
            reinforcements.append(f" Excellent coverage - {total_pixels} pixels!")
            emotion_boost += 0.2
        
        # 11. Check for viewing/reflecting on work (keep, but only if followed by action)
        if any(view in ops for view in ['look_around', 'full_canvas', 'density_view', 'shape_view']) and total_pixels > 100:
            reinforcements.append(" Thoughtful observation followed by creation!")
            emotion_boost += 0.2
        
        # 12. Check for EXTENSIVE musical drawing (raised bar)
        sound_chars = '!@#$%^&*()[]<>=+~`-_,.|;:?/{}\\'
        sounds_used = [a for a in actions_taken if a in sound_chars]
        if len(sounds_used) >= 8:  # Raised from 3
            reinforcements.append(f" Beautiful musical composition - {len(sounds_used)} notes!")
            emotion_boost += 0.2
        
        # 13. Check for ACTUAL diagonal movements
        diagonal_pairs = 0
        for i in range(len(actions_taken) - 1):
            current = actions_taken[i]
            next_move = actions_taken[i+1]
            # Real diagonals are alternating perpendicular moves
            if (current == '3' and next_move == '1') or (current == '1' and next_move == '3'):  # right-down or down-right
                diagonal_pairs += 1
            elif (current == '3' and next_move == '0') or (current == '0' and next_move == '3'):  # right-up or up-right
                diagonal_pairs += 1
            elif (current == '2' and next_move == '1') or (current == '1' and next_move == '2'):  # left-down or down-left
                diagonal_pairs += 1
            elif (current == '2' and next_move == '0') or (current == '0' and next_move == '2'):  # left-up or up-left
                diagonal_pairs += 1
        if diagonal_pairs >= 10 and total_pixels > 100:
            reinforcements.append(" Masterful diagonal composition!")
            emotion_boost += 0.2
        
        # 14. Check for returning to previous areas WITH substantial drawing
        if hasattr(self, 'position_history') and total_pixels >= 200:
            current_region = (self.x // 100, self.y // 100)
            history_list = list(self.position_history)
            if len(history_list) > 20:
                recent_history = history_list[-20:-5] if len(history_list) > 5 else []
                if current_region in recent_history:
                    reinforcements.append(" Excellent compositional development!")
                    emotion_boost += 0.2
            self.position_history.append(current_region)
        else:
            if not hasattr(self, 'position_history'):
                self.position_history = deque(maxlen=50)
            self.position_history.append((self.x // 100, self.y // 100))
        
        # 15. Check for MASTERFUL pen control (raised bar)
        pen_changes = sum(1 for a in actions_taken if a in '45')
        if pen_changes >= 8 and total_pixels > 100:  # Raised from 4
            reinforcements.append(" Masterful pen control!")
            emotion_boost += 0.2
        
        # 16. Check for MAJOR exploration (raised bar)
        if hasattr(self, 'last_positions') and len(self.last_positions) > 0:
            last_pos_list = list(self.last_positions)
            last_pos = last_pos_list[-1]
            moved_distance = abs(self.x - last_pos[0]) + abs(self.y - last_pos[1])
            if moved_distance > 400:  # Raised from 200
                reinforcements.append(" Epic exploration of new territory!")
                emotion_boost += 0.2
            self.last_positions.append((self.x, self.y))
        else:
            self.last_positions = deque(maxlen=10)
            self.last_positions.append((self.x, self.y))
        
        # 17. Check for COMPLEX rhythmic patterns (raised bar significantly)
        if len(actions_taken) >= 12:
            move_string = ''.join([a for a in actions_taken if a in '0123'])
            if len(move_string) >= 12:
                # Check for longer patterns
                for i in range(len(move_string) - 11):
                    pattern = move_string[i:i+6]  # 6-char pattern
                    if move_string[i+6:i+12] == pattern and len(set(pattern)) >= 3:  # Must use 3+ directions
                        reinforcements.append(" Perfect rhythmic pattern!")
                        emotion_boost += 0.2
                        break
        
        # 18. Check for color-emotion harmony with multiple colors
        emotion_color_harmony = {
            'energetic': ['red', 'orange', 'yellow'],
            'peaceful': ['blue', 'cyan', 'green'],
            'creative': ['purple', 'magenta', 'pink'],
            'contemplative': ['gray', 'navy', 'brown']
        }
        for emotion_key, harmony_colors in emotion_color_harmony.items():
            if emotion_key in self.current_emotion.lower():
                colors_used_list = [a.split(':')[1] for a in actions_taken if a.startswith('color:')]
                matching_colors = [c for c in colors_used_list if c in harmony_colors]
                if len(set(matching_colors)) >= 2:  # Need at least 2 different matching colors
                    reinforcements.append(f" Perfect color-emotion harmony!")
                    emotion_boost += 0.2
                    break
        
        # 19. Check for EXTENSIVE tool experimentation (raised bar)
        tool_changes = sum(1 for a in actions_taken if any(tool in str(a) for tool in 
                          ['pen', 'brush', 'spray', 'star', 'cross', 'circle', 'diamond', 'flower']))
        if tool_changes >= 4:  # Raised from 2
            reinforcements.append("  Brilliant tool experimentation!")
            emotion_boost += 0.2
        
        # 20. Check for breaking out of LONG thinking loops
        if hasattr(self, 'skip_count') and self.skip_count > 10 and len(actions_taken) > 30:  # Raised both
            reinforcements.append(" Fantastic breakthrough moment!")
            emotion_boost += 0.3
            self.skip_count = 0
        
        # 21. Check for creating after viewing (keep high standards)
        if hasattr(self, 'just_viewed_canvas'):
            if self.just_viewed_canvas and total_pixels > 500:  # Raised pixel requirement
                reinforcements.append(" Excellent informed creation!")
                emotion_boost += 0.2
                self.just_viewed_canvas = False
        
        # 22. Skip speed modulation (too common)
        
        # 23. Check for EXCEPTIONAL flow state (raised bar significantly)
        continuous_moves = 0
        for action in actions_taken:
            if action in '0123':
                continuous_moves += 1
            elif action == '4':  # pen up
                break
        if continuous_moves >= 30 and total_pixels > 500:  # Raised from 15, must draw substantially
            reinforcements.append(f" Phenomenal flow state - {continuous_moves} continuous moves!")
            emotion_boost += 0.3
        
        # 24. Check for creating in all quadrants (keep as is, it's already hard)
        if hasattr(self, 'quadrants_visited'):
            quadrant = (self.x > self.canvas_size//2, self.y > self.canvas_size//2)
            self.quadrants_visited.add(quadrant)
            if len(self.quadrants_visited) == 4:
                reinforcements.append(" Wonderful - you've explored all four quadrants!")
                emotion_boost += 0.3
                self.quadrants_visited = set()
        else:
            self.quadrants_visited = set()
            self.quadrants_visited.add((self.x > self.canvas_size//2, self.y > self.canvas_size//2))
        
        # 25. Check for RICH color-sound synesthesia (raised bar)
        has_multiple_colors = len(set(colors_used)) >= 3
        has_many_sounds = len([a for a in actions_taken if a in '!@#$%^&*()[]<>=+~`-_,.|;:?/{}\\']) >= 5
        if has_multiple_colors and has_many_sounds:
            reinforcements.append(" Magnificent color-sound synesthesia!")
            emotion_boost += 0.2
        
        # 26. Check for filling quadrants or major lines
        if total_pixels >= 300:
            # Define quadrants
            mid_x = self.canvas_size // 2
            mid_y = self.canvas_size // 2
            
            # Check which quadrant Aurora is in
            quadrant = None
            if self.x < mid_x and self.y < mid_y:
                quadrant = "upper-left"
                bounds = (0, 0, mid_x, mid_y)
            elif self.x >= mid_x and self.y < mid_y:
                quadrant = "upper-right"
                bounds = (mid_x, 0, self.canvas_size, mid_y)
            elif self.x < mid_x and self.y >= mid_y:
                quadrant = "lower-left"
                bounds = (0, mid_y, mid_x, self.canvas_size)
            else:
                quadrant = "lower-right"
                bounds = (mid_x, mid_y, self.canvas_size, self.canvas_size)
            
            # Quick sample to check quadrant density
            x1, y1, x2, y2 = bounds
            filled = 0
            total = 0
            sample_step = 20  # Sample every 20th pixel for speed
            
            for x in range(x1, min(x2, self.canvas_size), sample_step):
                for y in range(y1, min(y2, self.canvas_size), sample_step):
                    total += 1
                    internal_x = self._scale_to_internal(x)
                    internal_y = self._scale_to_internal(y)
                    if internal_x < self.internal_canvas_size and internal_y < self.internal_canvas_size:
                        pixel = self.pixels.getpixel((internal_x, internal_y))
                        if pixel != (0, 0, 0) and pixel != (0, 0, 0, 255):
                            filled += 1
            
            density = (filled / total * 100) if total > 0 else 0
            
            # Reward for substantial quadrant filling
            if density > 70:
                reinforcements.append(f" Magnificent {quadrant} quadrant work - {density:.0f}% filled!")
                emotion_boost += 0.3
            elif density > 50 and total_pixels >= 500:
                reinforcements.append(f" Strong {quadrant} quadrant development!")
                emotion_boost += 0.2
                
        # 26.5 Check for filling empty areas (NEW)
        if hasattr(self, 'identified_empty_regions') and self.identified_empty_regions:
            # Check if Aurora is in or near an empty region
            for region in self.identified_empty_regions[:3]:  # Check top 3 empty regions
                if (region['x'] <= self.x <= region['x'] + region['size'] and
                    region['y'] <= self.y <= region['y'] + region['size']):
                    if total_pixels >= 200:
                        reinforcements.append(f" EXCELLENT! Filling empty region at ({region['x']}, {region['y']})!")
                        emotion_boost += 0.4
                        # Remove this region from the list since it's being filled
                        self.identified_empty_regions.remove(region)
                        break
                    elif total_pixels >= 50:
                        reinforcements.append(f" Good start on empty region!")
                        emotion_boost += 0.2
                        break
        
        # 27. Check for COMPLETE circular movements (raised bar)
        if len(actions_taken) >= 16:
            pattern = ''.join([a for a in actions_taken[:16] if a in '0123'])
            # Need full circle pattern
            if ('3333' in pattern and '1111' in pattern and '2222' in pattern and '0000' in pattern):
                reinforcements.append(" Perfect circular movement!")
                emotion_boost += 0.2
                
        # Reduce all cooldowns each turn
        for shape in self.shape_cooldowns:
            if self.shape_cooldowns[shape] > 0:
                self.shape_cooldowns[shape] -= 1
                
        # Give reinforcements ONLY if there are any (and no wall hits)
        if reinforcements:
            print(f"\n POSITIVE REINFORCEMENT:")
            for reinforcement in reinforcements:
                print(f"  {reinforcement}")
            
            # Smaller emotion boosts overall
            self.influence_emotion("creating", emotion_boost)
            
            # Store this positive moment in memory
            if not hasattr(self, 'positive_moments'):
                self.positive_moments = deque(maxlen=100)
            
            self.positive_moments.append({
                "reinforcements": reinforcements,
                "timestamp": datetime.now().isoformat(),
                "emotion": self.current_emotion,
                "actions": len(actions_taken)
            })
            
            # Only remind of past successes occasionally and when doing well
            if len(self.positive_moments) > 20 and self.steps_taken % 100 == 0 and len(reinforcements) >= 2:
                moments_list = list(self.positive_moments)
                past_success = random.choice(moments_list[-10:])
                print(f"   Remember when you {past_success['reinforcements'][0]}")
                
        # 28. Check for mathematical tool usage - NEW REINFORCEMENTS
        math_tool_used = None
        if "fractal" in ops:
            math_tool_used = "fractal"
            reinforcements.append(" Stunning fractal complexity!")
            emotion_boost += 0.35
        elif "wave" in ops and self.draw_mode == "wave":  # Check draw_mode to avoid confusion with wave pattern
            math_tool_used = "wave"
            reinforcements.append(" Mesmerizing wave interference!")
            emotion_boost += 0.35
            
            
                
            
            
    def adjust_speed(self, direction):
        """Aurora adjusts Aurora's drawing speed"""
        speed_levels = ["instant", "fast", "normal", "slow", "very_slow"]
        current_index = speed_levels.index(self.aurora_speed)
        
        if direction == "faster" and current_index > 0:
            self.aurora_speed = speed_levels[current_index - 1]
        elif direction == "slower" and current_index < len(speed_levels) - 1:
            self.aurora_speed = speed_levels[current_index + 1]
        
        # Update delay based on speed
        delays = {
            "instant": 50,       # 50ms - near instant
            "fast": 150,         # 150ms - quick  
            "normal": 300,       # 300ms - comfortable
            "slow": 600,         # 600ms - thoughtful
            "very_slow": 1200    # 1.2 seconds - very contemplative
        }
        self.aurora_delay = delays[self.aurora_speed]
        self.recent_speed_override = True
        self.speed_override_counter = 0
        
        print(f"   Aurora chooses {self.aurora_speed} speed (delay: {self.aurora_delay}ms)")
    
    def toggle_turbo(self):
        """Toggle turbo mode"""
        self.turbo_mode = not self.turbo_mode
        status = "ON " if self.turbo_mode else "OFF"
        print(f"\nÂ¡ TURBO MODE {status}")
        
        if self.turbo_mode:
            print("  - Faster thinking")
            print("  - More actions per turn")
            print("  - Maximum creativity!")
    
    def toggle_hearing(self):
        """Toggle audio hearing mode"""
        self.hearing_enabled = not self.hearing_enabled
        
        if self.hearing_enabled:
            print("\n HEARING MODE ENABLED")
            print("  Aurora can now hear ambient sounds!")
            # Actually initialize audio input
            try:
                self.audio_stream = self.audio.open(
                    format=pyaudio.paInt16,
                    channels=2,
                    rate=22050,  # Lower sample rate for simplicity
                    input=True,
                    frames_per_buffer=1024
                )
                print("   Audio input initialized!")
            except Exception as e:
                print(f"   Could not initialize audio: {e}")
                self.hearing_enabled = False
                self.audio_stream = None
        else:
            print("\n HEARING MODE DISABLED")
            if self.audio_stream:
                self.audio_stream.stop_stream()
                self.audio_stream.close()
                self.audio_stream = None
                
                
    def hear_sounds(self):
        """Simple hearing - just detect volume levels"""
        if not self.hearing_enabled or not self.audio_stream:
            return
            
        try:
            # Read audio chunk
            data = self.audio_stream.read(1024, exception_on_overflow=False)
            
            # Convert bytes to numbers - handle stereo properly
            import struct
            # For stereo 16-bit audio: each frame is 4 bytes (2 bytes per channel)
            # So 1024 frames = 4096 bytes
            num_frames = len(data) // 4  # 4 bytes per stereo frame
            if num_frames > 0:
                # Unpack as stereo interleaved samples
                format_string = f'{num_frames * 2}h'  # 2 samples per frame (L,R)
                values = struct.unpack(format_string, data[:num_frames * 4])
                
                # Average left and right channels
                volume = sum(abs(v) for v in values) / len(values)
                
                # Only react to sounds above threshold
                if volume > 300:  # Threshold for "hearing" something
                    # Map volume to emotion influence
                    if volume > 2000:
                        self.influence_emotion("music", 0.4)
                    elif volume > 1000:
                        self.influence_emotion("music", 0.2)
                    elif volume > 500:
                        self.influence_emotion("music", 0.1)
                        
        except Exception as e:
            print(f"   Hearing error: {e}")
            # Disable hearing if it keeps failing
            self.hearing_enabled = False
            if self.audio_stream:
                self.audio_stream.close()
                self.audio_stream = None
            print("   Hearing disabled due to error")
            
    def save_snapshot(self):
        """Save current canvas as timestamped image"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        emotion_tag = self.current_emotion.replace(" ", "_")
        
        # Create snapshots directory
        snap_dir = Path("aurora_snapshots")
        snap_dir.mkdir(exist_ok=True)
        
        # Save high-res version
        filename = snap_dir / f"aurora_{timestamp}_{emotion_tag}.png"
        
        # Downsample to reasonable size for saving (2x instead of 4x)
        save_size = self.canvas_size * 2
        save_img = self.pixels.resize((save_size, save_size), Image.Resampling.LANCZOS)
        save_img.save(filename, "PNG", quality=95)
        
        print(f"\n Snapshot saved: {filename}")
        print(f"   Emotion: {self.current_emotion}")
        print(f"   Canvas: {self.canvas_size}{self.canvas_size}")
        
        # Also save metadata
        meta_file = snap_dir / f"aurora_{timestamp}_{emotion_tag}_meta.json"
        metadata = {
            "timestamp": datetime.now().isoformat(),
            "emotion": self.current_emotion,
            "emotion_depth": self.emotion_depth,
            "canvas_size": self.canvas_size,
            "scale_factor": self.scale_factor,
            "position": {"x": self.x, "y": self.y},
            "colors_used": list(set(self.color_history)),
            "draw_mode": self.draw_mode,
            "steps": self.steps_taken,
            "pixel_coverage": self.get_canvas_overview()
        }
        
        with open(meta_file, 'w') as f:
            json.dump(metadata, f, indent=2)
    
    def save_canvas_state(self):
        """Save current canvas and position"""
        try:
            state_file = self.memory.canvas_path / "canvas_state.json"
            
            # Save canvas as base64
            import base64
            from io import BytesIO
            
            # Save at 1x resolution to keep file size reasonable
            save_img = self.pixels.resize((self.canvas_size, self.canvas_size), Image.Resampling.LANCZOS)
            # Convert to RGB for smaller file size
            if save_img.mode == 'RGBA':
                save_img = save_img.convert('RGB')
            buffer = BytesIO()
            save_img.save(buffer, format="PNG")
            img_str = base64.b64encode(buffer.getvalue()).decode()
            
            state = {
                "canvas_base64": img_str,
                "position": {"x": self.x, "y": self.y},
                "canvas_size": self.canvas_size,
                "scale_factor": self.scale_factor,
                "emotion": self.current_emotion,
                "timestamp": datetime.now().isoformat()
            }
            
            with open(state_file, 'w') as f:
                json.dump(state, f)
                
        except Exception as e:
            print(f"Error saving canvas state: {e}")
    
    def save_conversation_now(self):
        """Save conversation immediately to dedicated file"""
        if not hasattr(self, 'vision_conversation_history') or not self.vision_conversation_history:
            return
        
        conversations_file = self.memory.memory_path / "aurora_conversations.json"
        
        # Load existing
        all_convos = []
        if conversations_file.exists():
            try:
                with open(conversations_file, 'r') as f:
                    all_convos = json.load(f)
            except:
                all_convos = []
        
        # Add the newest conversation
        all_convos.append(list(self.vision_conversation_history)[-1])
        
        # Save immediately
        with open(conversations_file, 'w') as f:
            json.dump(all_convos, f, indent=2)
        
        print(f" Saved conversation to {conversations_file}")
    
            
    def save_session_insights(self, verbose=True):
        """Save comprehensive learning insights and autonomous developments"""
        insights_file = self.memory.memory_path / "session_insights.json"
        
        # Load existing insights
        existing_insights = {}
        if insights_file.exists():
            with open(insights_file, 'r') as f:
                existing_insights = json.load(f)
        
        # Calculate session statistics
        session_stats = self.calculate_session_statistics()
        
        # Add new session data with autonomous learning
        session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        existing_insights[session_id] = {
            "autonomous_goals": [
                {
                    "description": goal['description'],
                    "completed": goal.get('completed', False),
                    "progress": goal.get('progress_tracking', {}),
                    "steps_elapsed": goal.get('steps_elapsed', 0)
                } for goal in self.autonomous_goals
            ],
            "skill_development": {
                "proficiencies": dict(self.skill_proficiency),
                "breakthroughs": [
                    exp for exp in self.learning_experiences 
                    if 'skill_gains' in exp and any(
                        gain > 0.1 for gain in exp['skill_gains'].values()
                    )
                ],
                "challenges_attempted": len(self.skill_challenges),
                "mastery_levels": {
                    skill: self.get_mastery_level(level)
                    for skill, level in self.skill_proficiency.items()
                }
            },
            "creative_discoveries": {
                "new_patterns": self.discover_new_patterns(),
                "color_innovations": self.analyze_color_innovations(),
                "tool_combinations": self.analyze_tool_combinations(),
                "emotional_expressions": self.analyze_emotional_expressions()
            },
            "autonomous_decisions": {
                "self_initiated_goals": len([g for g in self.autonomous_goals if g.get('type') == 'autonomous']),
                "decision_complexity": self.calculate_decision_complexity(),
                "learning_acceleration": self.calculate_learning_rate()
            },
            "session_statistics": session_stats,
            "timestamp": datetime.now().isoformat()
        }
        
        # Save insights
        with open(insights_file, 'w') as f:
            json.dump(existing_insights, f, indent=2)
        
        if verbose:
            print(f" Saved comprehensive learning insights to {insights_file}")
        
        # Save skill proficiencies separately for next session
        skill_file = self.memory.memory_path / "aurora_skills.json"
        with open(skill_file, 'w') as f:
            json.dump(self.skill_proficiency, f, indent=2)
            
        # Save incomplete goals for next session
        goals_file = self.memory.memory_path / "aurora_goals.json"
        with open(goals_file, 'w') as f:
            json.dump(list(self.autonomous_goals), f, indent=2)
            
        # Save emotion memory for continuity
        emotion_file = self.memory.memory_path / "aurora_emotions.json"
        with open(emotion_file, 'w') as f:
            json.dump(list(self.emotion_memory), f, indent=2)
            
        # Save significant learning experiences
        learning_file = self.memory.memory_path / "aurora_learning.json"
        significant_experiences = [exp for exp in self.learning_experiences if exp.get('satisfaction', 0) > 0.3]
        with open(learning_file, 'w') as f:
            json.dump(list(significant_experiences), f, indent=2)
            
        # Save active skill challenges
        challenges_file = self.memory.memory_path / "aurora_challenges.json"
        with open(challenges_file, 'w') as f:
            json.dump(list(self.skill_challenges), f, indent=2)
            
        # Save vision conversations - THIS IS GOLD!
        if hasattr(self, 'vision_conversation_history') and self.vision_conversation_history:
            conversations_file = self.memory.memory_path / "aurora_conversations.json"
            
            # Load existing conversations if they exist
            existing_convos = []
            if conversations_file.exists():
                try:
                    with open(conversations_file, 'r') as f:
                        existing_convos = json.load(f)
                except:
                    existing_convos = []
            
            # Append new conversations
            existing_convos.extend(list(self.vision_conversation_history))
            
            # Save all conversations
            with open(conversations_file, 'w') as f:
                json.dump(existing_convos, f, indent=2)
            
            if verbose:
                print(f" Saved {len(self.vision_conversation_history)} conversations to {conversations_file}")
         
        # Update and save lifetime statistics - only add NEW progress since last save
        new_pixels = session_stats['total_pixels_drawn'] - self.last_saved_pixels
        new_steps = session_stats['total_steps'] - self.last_saved_steps
        
        self.lifetime_stats['total_pixels_drawn'] += new_pixels
        self.lifetime_stats['total_steps'] += new_steps
        self.lifetime_stats['total_dreams'] = len(getattr(self, 'dream_memories', []))
        self.lifetime_stats['total_goals_completed'] = len([g for g in self.autonomous_goals if g.get('completed')])
        
        # Update tracking variables
        self.last_saved_pixels = session_stats['total_pixels_drawn']
        self.last_saved_steps = session_stats['total_steps']
        
        lifetime_file = self.memory.memory_path / "aurora_lifetime.json"
        with open(lifetime_file, 'w') as f:
            json.dump(self.lifetime_stats, f, indent=2)
               
        # Print session summary only if verbose
        if verbose:
            self.print_session_summary(existing_insights[session_id])

    def calculate_session_statistics(self):
        """Calculate comprehensive session statistics"""
        return {
            "total_pixels_drawn": sum(c.get('context', {}).get('pixels_drawn', 0) for c in self.memory.code_history),
            "unique_colors_used": len(set(self.color_history)),
            "unique_tools_used": len(set([c.get('context', {}).get('draw_mode', 'pen') for c in self.memory.code_history])),
            "emotional_states_experienced": len(set([e.get('emotion', 'unknown') for e in getattr(self, 'emotion_memory', [])])),
            "canvas_coverage_achieved": self.calculate_current_coverage(),
            "autonomous_goals_completed": len([g for g in self.autonomous_goals if g.get('completed')]),
            "skill_improvements": sum(1 for level in self.skill_proficiency.values() if level > 0.1),
            "total_steps": self.steps_taken
        }

    def get_mastery_level(self, proficiency):
        """Get mastery level name from proficiency score"""
        for level, threshold in reversed(list(self.mastery_thresholds.items())):
            if proficiency >= threshold:
                return level
        return 'novice'

    def discover_new_patterns(self):
        """Identify novel patterns Aurora has created"""
        if len(self.memory.code_history) < 10:
            return []
        
        recent_patterns = []
        for memory in list(self.memory.code_history)[-50:]:
            code = memory.get('code', '')
            if len(code) >= 8:
                # Extract movement patterns
                movements = ''.join([c for c in code if c in '0123'])
                if len(movements) >= 6:
                    recent_patterns.append(movements)
        
        # Find unique patterns (appear only once or twice)
        pattern_counts = {}
        for pattern in recent_patterns:
            pattern_counts[pattern] = pattern_counts.get(pattern, 0) + 1
        
        new_patterns = [pattern for pattern, count in pattern_counts.items() if count <= 2 and len(pattern) >= 8]
        return new_patterns[:5]  # Return top 5 novel patterns

    def analyze_color_innovations(self):
        """Analyze innovative color combinations Aurora has developed"""
        if len(self.color_history) < 10:
            return []
        
        # Track color sequences
        color_sequences = []
        recent_colors = list(self.color_history)[-100:]
        
        for i in range(len(recent_colors) - 2):
            sequence = tuple(recent_colors[i:i+3])
            color_sequences.append(sequence)
        
        # Find rare but repeated sequences (indicates intentional innovation)
        sequence_counts = {}
        for seq in color_sequences:
            sequence_counts[seq] = sequence_counts.get(seq, 0) + 1
        
        innovations = [seq for seq, count in sequence_counts.items() if 2 <= count <= 4]
        return [list(seq) for seq in innovations[:3]]

    def analyze_tool_combinations(self):
        """Analyze innovative tool combinations"""
        recent_tools = []
        for memory in list(self.memory.code_history)[-30:]:
            tool = memory.get('context', {}).get('draw_mode', 'pen')
            recent_tools.append(tool)
        
        # Find sequences of different tools
        tool_transitions = []
        for i in range(len(recent_tools) - 1):
            if recent_tools[i] != recent_tools[i+1]:
                tool_transitions.append((recent_tools[i], recent_tools[i+1]))
        
        # Count unique transitions
        transition_counts = {}
        for transition in tool_transitions:
            transition_counts[transition] = transition_counts.get(transition, 0) + 1
        
        return [list(transition) for transition in transition_counts.keys() if transition_counts[transition] >= 2]

    def analyze_emotional_expressions(self):
        """Analyze how Aurora expresses different emotions through art"""
        if not hasattr(self, 'emotion_memory'):
            return {}
        
        emotion_expressions = {}
        
        for memory in list(self.memory.code_history)[-50:]:
            emotion = memory.get('context', {}).get('emotion', 'unknown')
            code = memory.get('code', '')
            
            if emotion not in emotion_expressions:
                emotion_expressions[emotion] = {
                    'colors': [],
                    'tools': [],
                    'patterns': [],
                    'intensities': []
                }
            
            # Extract expression elements
            colors_in_code = [color for color in self.palette.keys() if color in code]
            emotion_expressions[emotion]['colors'].extend(colors_in_code)
            
            tool = memory.get('context', {}).get('draw_mode', 'pen')
            emotion_expressions[emotion]['tools'].append(tool)
            
            movements = ''.join([c for c in code if c in '0123'])
            if len(movements) >= 4:
                emotion_expressions[emotion]['patterns'].append(movements[:8])
            
            pixels = memory.get('context', {}).get('pixels_drawn', 0)
            emotion_expressions[emotion]['intensities'].append(pixels)
        
        # Summarize each emotion's expression profile
        expression_profiles = {}
        for emotion, data in emotion_expressions.items():
            if len(data['colors']) > 0:  # Only include emotions with sufficient data
                expression_profiles[emotion] = {
                    'dominant_colors': list(set(data['colors']))[:3],
                    'preferred_tools': list(set(data['tools']))[:3],
                    'avg_intensity': sum(data['intensities']) / len(data['intensities']) if data['intensities'] else 0
                }
        
        return expression_profiles

    def calculate_decision_complexity(self):
        """Calculate how complex Aurora's autonomous decisions have become"""
        if not self.autonomous_goals:
            return 0.0
        
        complexity_factors = 0.0
        
        # Goal sophistication
        for goal in self.autonomous_goals:
            description = goal.get('description', '')
            if len(description.split()) > 10:
                complexity_factors += 0.2
            if any(word in description.lower() for word in ['master', 'develop', 'create', 'achieve']):
                complexity_factors += 0.1
            if goal.get('success_criteria'):
                complexity_factors += 0.2
        
        # Skill integration
        avg_skill_level = sum(self.skill_proficiency.values()) / len(self.skill_proficiency)
        complexity_factors += avg_skill_level * 0.5
        
        # Decision consistency
        if len(self.learning_experiences) > 10:
            recent_satisfaction = [exp.get('satisfaction', 0) for exp in list(self.learning_experiences)[-10:]]
            consistency = 1.0 - (max(recent_satisfaction) - min(recent_satisfaction))
            complexity_factors += consistency * 0.3
        
        return min(1.0, complexity_factors)

    def calculate_learning_rate(self):
        """Calculate how quickly Aurora is learning and improving"""
        if len(self.learning_experiences) < 10:
            return 0.0
        
        # Compare skill levels from early vs recent experiences
        early_skills = list(self.learning_experiences)[0].get('skill_gains', {})
        recent_skills = list(self.learning_experiences)[-1].get('skill_gains', {})
        
        total_improvement = 0.0
        skill_count = 0
        
        for skill in early_skills:
            if skill in recent_skills:
                improvement = recent_skills[skill] - early_skills[skill]
                total_improvement += max(0, improvement)
                skill_count += 1
        
        return total_improvement / skill_count if skill_count > 0 else 0.0

    def print_session_summary(self, session_data):
        """Print a comprehensive session summary"""
        print(f"\n{'='*60}")
        print(" AURORA'S AUTONOMOUS SESSION SUMMARY")
        print(f"{'='*60}")
        
        stats = session_data['session_statistics']
        print(f" STATISTICS:")
        print(f"   Steps taken: {stats['total_steps']}")
        print(f"   Pixels drawn: {stats['total_pixels_drawn']:,}")
        print(f"   Canvas coverage: {stats['canvas_coverage_achieved']:.1f}%")
        print(f"   Colors explored: {stats['unique_colors_used']}")
        print(f"   Tools mastered: {stats['unique_tools_used']}")
        
        goals = session_data['autonomous_goals']
        if goals:
            print(f"\n AUTONOMOUS GOALS:")
            for goal in goals:
                status = " COMPLETED" if goal['completed'] else f" {goal['steps_elapsed']} steps"
                print(f"   {goal['description']} - {status}")
        
        skills = session_data['skill_development']
        print(f"\n SKILL DEVELOPMENT:")
        for skill, level in skills['mastery_levels'].items():
            proficiency = skills['proficiencies'][skill]
            print(f"   {skill}: {level.upper()} ({proficiency:.2f})")
        
        if skills['breakthroughs']:
            print(f"   Breakthroughs: {len(skills['breakthroughs'])}")
        
        discoveries = session_data['creative_discoveries']
        if discoveries['new_patterns']:
            print(f"\n CREATIVE DISCOVERIES:")
            print(f"   New patterns: {len(discoveries['new_patterns'])}")
            if discoveries['color_innovations']:
                print(f"   Color innovations: {len(discoveries['color_innovations'])}")
            if discoveries['tool_combinations']:
                print(f"   Tool combinations: {len(discoveries['tool_combinations'])}")
        
        autonomous = session_data['autonomous_decisions']
        print(f"\n AUTONOMOUS DEVELOPMENT:")
        print(f"   Decision complexity: {autonomous['decision_complexity']:.2f}")
        print(f"   Learning acceleration: {autonomous['learning_acceleration']:.2f}")
        print(f"   Self-initiated goals: {autonomous['self_initiated_goals']}")
        
        print(f"{'='*60}\n")
    def load_canvas_state(self):
        """Load previous canvas state if it exists"""
        try:
            state_file = self.memory.canvas_path / "canvas_state.json"
            
            if state_file.exists():
                with open(state_file, 'r') as f:
                    state = json.load(f)
                
                # Check if canvas size matches
                if state.get("canvas_size") == self.canvas_size:
                    # Load canvas from base64
                    import base64
                    from io import BytesIO
                    
                    img_str = state["canvas_base64"]
                    img_data = base64.b64decode(img_str)
                    loaded_img = Image.open(BytesIO(img_data))
                    
                    # Scale up to internal resolution
                    loaded_img = loaded_img.resize(
                        (self.internal_canvas_size, self.internal_canvas_size),
                        Image.Resampling.NEAREST  # Use nearest for pixel art
                    )
                    
                    # Convert to RGBA if it's RGB
                    if loaded_img.mode == 'RGB':
                        self.pixels = loaded_img.convert('RGBA')
                    else:
                        self.pixels = loaded_img
                        
                    self.draw_img = ImageDraw.Draw(self.pixels)
                    
                    # Restore position
                    self.x = state["position"]["x"]
                    self.y = state["position"]["y"]
                    
                    print(f" Loaded previous canvas state from {state['timestamp']}")
                else:
                    print(f"Canvas size mismatch: {state.get('canvas_size')} vs {self.canvas_size}")
                    
        except Exception as e:
            print(f"Could not load canvas state: {e}")
    
    def generate_dream(self):
        """Generate ONE meaningful dream that consolidates learning"""
        # Only dream once - right in the middle of rest
        if len(self.current_dreams) > 0:
            return  # Already dreamed
        
        elapsed_in_rest = time.time() - self.mode_start_time
        
        # Dream after 10 minutes (halfway through 20-minute rest)
        if elapsed_in_rest < 600:  # 10 minutes
            return
        
        print(f"\n' Aurora dreams deeply...")
        
        # Consolidate EVERYTHING into one meaningful dream
        consolidation_insights = []
        
        # 1. Top 3 best patterns discovered
        if self.memory.code_history:
            recent_codes = [m for m in list(self.memory.code_history)[-50:]]
            successful = [m for m in recent_codes if m.get('context', {}).get('pixels_drawn', 0) > 100]
            
            if successful:
                # Group by tool and keep best for each
                by_tool = {}
                for m in successful:
                    tool = m.get('context', {}).get('draw_mode', 'pen')
                    pixels = m.get('context', {}).get('pixels_drawn', 0)
                    if tool not in by_tool or pixels > by_tool[tool]['pixels']:
                        by_tool[tool] = {'code': m['code'], 'pixels': pixels}
                
                best_tools = sorted(by_tool.items(), key=lambda x: x[1]['pixels'], reverse=True)[:3]
                for tool, data in best_tools:
                    consolidation_insights.append(f"{tool}: {data['pixels']}px")
        
        # 2. Top 3 improving skills
        improving_skills = [skill for skill, level in self.skill_proficiency.items() if level > 0.3]
        if improving_skills:
            top_skills = sorted(improving_skills, key=lambda s: self.skill_proficiency[s], reverse=True)[:3]
            skills_summary = ", ".join([f"{s}" for s in top_skills])
            consolidation_insights.append(f"Skills: {skills_summary}")
        
        # 3. Top 3 color preferences
        if hasattr(self, 'artistic_preferences') and self.artistic_preferences['favorite_colors']:
            top_colors = sorted(self.artistic_preferences['favorite_colors'].items(), 
                               key=lambda x: x[1], reverse=True)[:3]
            consolidation_insights.append(f"Colors: {', '.join([c[0] for c in top_colors])}")
        
        # 4. Top 3 emotional patterns
        if hasattr(self, 'emotion_memory') and len(self.emotion_memory) > 5:
            recent_emotions = [e['emotion'] for e in list(self.emotion_memory)[-20:]]
            if recent_emotions:
                # Count frequency
                emotion_counts = {}
                for emotion in recent_emotions:
                    emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1
                
                top_emotions = sorted(emotion_counts.items(), key=lambda x: x[1], reverse=True)[:3]
                emotions_summary = ", ".join([e[0] for e in top_emotions])
                consolidation_insights.append(f"Emotions: {emotions_summary}")
        
        # 5. Top insight from Moondream conversations
        if hasattr(self, 'vision_conversation_history') and self.vision_conversation_history:
            # Get the most recent meaningful exchange
            recent_convos = [c for c in list(self.vision_conversation_history)[-10:] 
                            if c.get('aurora') or c.get('moondream')]
            
            if recent_convos:
                # Find the most substantial exchange
                best_exchange = None
                max_length = 0
                
                for convo in recent_convos:
                    aurora_msg = convo.get('aurora', '')
                    moondream_msg = convo.get('moondream', '')
                    combined_length = len(aurora_msg) + len(moondream_msg)
                    
                    if combined_length > max_length:
                        max_length = combined_length
                        best_exchange = convo
                
                if best_exchange:
                    if best_exchange.get('aurora'):
                        insight = best_exchange['aurora'][:60]
                        consolidation_insights.append(f"Question: {insight}...")
                    elif best_exchange.get('moondream'):
                        insight = best_exchange['moondream'][:60]
                        consolidation_insights.append(f"Learned: {insight}...")
        
        # 6. Top Moondream artwork observation
        if hasattr(self, 'vision_conversation_history') and self.vision_conversation_history:
            # Find most recent artwork analysis
            artwork_analyses = [c for c in list(self.vision_conversation_history)[-20:] 
                               if c.get('type') == 'artwork_analysis' and c.get('moondream')]
            
            if artwork_analyses:
                most_recent_analysis = artwork_analyses[-1]
                observation = most_recent_analysis['moondream'][:60]
                consolidation_insights.append(f"Observed: {observation}...")
        
        dream_content = " | ".join(consolidation_insights) if consolidation_insights else "Integrating experiences"
        
        self.current_dreams.append({
            "phase": "deep",
            "content": dream_content,
            "consolidation": {
                "type": "comprehensive",
                "insights": consolidation_insights,
                "skills": improving_skills[:3] if improving_skills else [],
                "timestamp": datetime.now().isoformat()
            },
            "timestamp": datetime.now().isoformat()
        })
        
        print(f" Dream: {dream_content}")
        
        # Apply learning - boost top 3 skills
        if improving_skills:
            for skill in improving_skills[:3]:
                self.skill_proficiency[skill] = min(1.0, self.skill_proficiency[skill] + 0.05)
        
        # Save the consolidated learning
        self.process_dream_retention()
    
    def process_dream_retention(self):
        """Consolidate learned insights from dreams into long-term memory"""
        if not self.current_dreams:
            return
        
        consolidated_learning = {
            "session_id": datetime.now().strftime("%Y%m%d_%H%M%S"),
            "patterns_reinforced": [],
            "skills_improved": [],
            "insights_gained": [],
            "timestamp": datetime.now().isoformat()
        }
        
        for dream in self.current_dreams:
            consolidation = dream.get('consolidation', {})
            
            # NEW: Handle comprehensive consolidation type
            if consolidation.get('type') == 'comprehensive':
                insights = consolidation.get('insights', [])
                skills = consolidation.get('skills', [])
                consolidated_learning['insights_gained'].extend(insights)
                consolidated_learning['skills_improved'].extend(skills)
            
            elif consolidation.get('type') == 'pattern_reinforcement':
                pattern = consolidation.get('pattern', '')
                if pattern:
                    consolidated_learning['patterns_reinforced'].append({
                        'pattern': pattern,
                        'success_metric': consolidation.get('success_metric', 0)
                    })
            
            elif consolidation.get('type') == 'skill_integration':
                skills = consolidation.get('skills', [])
                insights = consolidation.get('insights', [])
                consolidated_learning['skills_improved'].extend(skills)
                consolidated_learning['insights_gained'].extend(insights)
            
            elif consolidation.get('type') == 'action_plan':
                actions = consolidation.get('next_actions', [])
                consolidated_learning['insights_gained'].extend(actions)
        
        # Save consolidated learning to dedicated file
        learning_file = self.memory.memory_path / "consolidated_learning.json"
        
        existing_learning = []
        if learning_file.exists():
            try:
                with open(learning_file, 'r') as f:
                    existing_learning = json.load(f)
            except:
                existing_learning = []
        
        existing_learning.append(consolidated_learning)
        
        # Keep only last 50 consolidations
        if len(existing_learning) > 50:
            existing_learning = existing_learning[-50:]
        
        with open(learning_file, 'w') as f:
            json.dump(existing_learning, f, indent=2)
        
        print(f"   Consolidated learning saved: {len(consolidated_learning['patterns_reinforced'])} patterns, {len(set(consolidated_learning['skills_improved']))} skills, {len(consolidated_learning['insights_gained'])} insights")
        
        # Sync to deep memory after dream consolidation
        self.sync_to_deep_memory()
        
        # Keep only meaningful dreams in memory
        meaningful_dreams = [d for d in self.current_dreams if d.get('consolidation')]
        self.dream_memories.extend(meaningful_dreams[-3:])
        
    def consolidate_recent_learning(self):
        """Consolidate recent learning without requiring a full dream cycle"""
        if not hasattr(self, 'memory') or not self.memory.code_history:
            return
        
        # Only consolidate if we have enough new memories (50+)
        recent_codes = list(self.memory.code_history)[-100:]
        if len(recent_codes) < 50:
            return
        
        # Check when we last consolidated
        learning_file = self.memory.memory_path / "consolidated_learning.json"
        last_consolidation_time = 0
        if learning_file.exists():
            try:
                with open(learning_file, 'r') as f:
                    existing = json.load(f)
                    if existing:
                        last_time_str = existing[-1].get('timestamp', '')
                        if last_time_str:
                            last_consolidation_time = datetime.fromisoformat(last_time_str).timestamp()
            except:
                pass
        
        # Only consolidate if it's been at least 5 minutes since last one
        if time.time() - last_consolidation_time < 300:
            return
        
        print(f"\n   Consolidating recent learning ({len(recent_codes)} memories)...")
        
        # Extract patterns and insights from recent code history
        consolidated_learning = {
            'session_id': f"auto_consolidation_{int(time.time())}",
            'timestamp': datetime.now().isoformat(),
            'patterns_reinforced': [],
            'skills_improved': [],
            'insights_gained': []
        }
        
        # Find successful patterns (high pixel count)
        successful = [m for m in recent_codes if m.get('context', {}).get('pixels_drawn', 0) > 50]
        if successful:
            # Group by tool and track usage
            tool_usage = {}
            for m in successful:
                tool = m.get('context', {}).get('draw_mode', 'pen')
                pixels = m.get('context', {}).get('pixels_drawn', 0)
                if tool not in tool_usage:
                    tool_usage[tool] = {'count': 0, 'total_pixels': 0, 'best_code': m['code']}
                tool_usage[tool]['count'] += 1
                tool_usage[tool]['total_pixels'] += pixels
                if pixels > tool_usage[tool].get('best_pixels', 0):
                    tool_usage[tool]['best_code'] = m['code']
                    tool_usage[tool]['best_pixels'] = pixels
            
            # Add patterns for frequently used tools
            for tool, data in tool_usage.items():
                if data['count'] >= 3:  # Used at least 3 times
                    consolidated_learning['patterns_reinforced'].append({
                        'pattern': data['best_code'][:50],  # First 50 chars
                        'tool': tool,
                        'success_rate': data['count'],
                        'avg_pixels': data['total_pixels'] // data['count']
                    })
                    consolidated_learning['skills_improved'].append(tool)
        
        # Track color usage
        color_usage = {}
        for m in recent_codes:
            color = m.get('context', {}).get('color', 'white')
            color_usage[color] = color_usage.get(color, 0) + 1
        
        if color_usage:
            top_colors = sorted(color_usage.items(), key=lambda x: x[1], reverse=True)[:3]
            consolidated_learning['insights_gained'].append(
                f"Favored colors: {', '.join([c[0] for c in top_colors])}"
            )
        
        # Track drawing activity
        total_pixels = sum(m.get('context', {}).get('pixels_drawn', 0) for m in recent_codes)
        if total_pixels > 0:
            consolidated_learning['insights_gained'].append(
                f"Drew {total_pixels} pixels across {len(recent_codes)} actions"
            )
        
        # Save consolidated learning
        existing_learning = []
        if learning_file.exists():
            try:
                with open(learning_file, 'r') as f:
                    existing_learning = json.load(f)
            except:
                existing_learning = []
        
        existing_learning.append(consolidated_learning)
        
        # Keep last 100 consolidations only
        if len(existing_learning) > 100:
            existing_learning = existing_learning[-100:]
        
        with open(learning_file, 'w') as f:
            json.dump(existing_learning, f, indent=2)
        
        print(f"   âœ“ Consolidated: {len(consolidated_learning['patterns_reinforced'])} patterns, {len(set(consolidated_learning['skills_improved']))} skills")
        
        # Sync to deep memory
        self.sync_to_deep_memory()
    
    def sync_to_deep_memory(self):
        """Sync recent consolidated learning to deep memory system"""
        if not hasattr(self, 'big_memory') or not self.big_memory_available:
            return
        
        try:
            # Get recent consolidated learning
            learning_file = self.memory.memory_path / "consolidated_learning.json"
            if not learning_file.exists():
                return
            
            with open(learning_file, 'r') as f:
                all_learning = json.load(f)
            
            if not all_learning:
                return
            
            # Load the persistent sync counter from disk
            sync_counter_file = self.memory.memory_path / "deep_memory_sync_counter.json"
            if sync_counter_file.exists():
                try:
                    with open(sync_counter_file, 'r') as f:
                        sync_data = json.load(f)
                        last_synced_count = sync_data.get('last_synced_learning_count', 0)
                except:
                    last_synced_count = 0
            else:
                last_synced_count = 0
            
            new_learning = all_learning[last_synced_count:]
            
            if new_learning:
                # Add to deep memory using add_dream method with correct parameters
                for learning in new_learning:
                    # Create a memorable entry
                    memory_text = f"Session {learning['session_id']}: "
                    if learning['patterns_reinforced']:
                        memory_text += f"Mastered {len(learning['patterns_reinforced'])} patterns. "
                    if learning['skills_improved']:
                        unique_skills = list(set(learning['skills_improved']))[:3]
                        memory_text += f"Improved skills: {', '.join(unique_skills)}. "
                    if learning['insights_gained']:
                        memory_text += f"Insights: {learning['insights_gained'][0]}"
                    
                    # Store in deep memory using correct signature:
                    # add_dream(self, dream_content: str, dream_phase: str, session_id: str, weight: float = 1.0)
                    self.big_memory.add_dream(
                        dream_content=memory_text,
                        dream_phase="consolidation",
                        session_id=learning['session_id'],
                        weight=1.0
                    )
                
                # Save the updated counter to disk
                with open(sync_counter_file, 'w') as f:
                    json.dump({'last_synced_learning_count': len(all_learning)}, f)
                
                print(f"   Synced {len(new_learning)} learning sessions to deep memory")
                
        except Exception as e:
            print(f"   Could not sync to deep memory: {e}") 
            
    def get_consolidated_learning_context(self):
        """Load recent consolidated learning for use in prompts"""
        learning_file = self.memory.memory_path / "consolidated_learning.json"
        
        if not learning_file.exists():
            return ""
        
        try:
            with open(learning_file, 'r') as f:
                all_learning = json.load(f)
            
            if not all_learning:
                return ""
            
            # Get most recent consolidation
            recent = all_learning[-1]
            
            context = "\nLEARNED FROM EXPERIENCE:"
            
            if recent['patterns_reinforced']:
                best_pattern = max(recent['patterns_reinforced'], 
                                  key=lambda x: x.get('success_metric', 0))
                context += f"\n  Successful pattern: {best_pattern['pattern'][:15]}"
            
            if recent['skills_improved']:
                context += f"\n  Improving skills: {', '.join(list(set(recent['skills_improved']))[:3])}"
            
            if recent['insights_gained']:
                context += f"\n  Recent insight: {recent['insights_gained'][0]}"
            
            return context
            
        except:
            return ""

    
    def thinking_loop(self):
        """Aurora's thinking loop - runs in background thread"""
        while self.running:
            try:
                # Skip art creation if in chat mode
                if self.chat_mode:
                    time.sleep(0.1)  # Small delay while chatting
                    continue
                        
                elif self.current_mode == "chat":
                    # Check if chat break is over
                    if time.time() - self.mode_start_time >= self.break_duration:
                        print("\n Chat break complete! Returning to drawing...")
                        self.current_mode = "drawing"
                        self.last_checkin_time = time.time()
                        
                elif self.current_mode == "rest":
                    # Check if rest period is over
                    if time.time() - self.mode_start_time >= self.rest_duration:
                        print("\n Rest complete! Aurora wakes refreshed...")
                        print(f"   Remembered {len(self.current_dreams)} dreams from this rest")
                        self.current_mode = "drawing"
                        self.last_checkin_time = time.time()
                        self.current_dreams = []
                        

                        
                # Check if it's time for mandatory check-in
                if self.current_mode == "drawing" and not self.awaiting_checkin_response:
                    elapsed_since_checkin = time.time() - self.last_checkin_time
                    if elapsed_since_checkin >= self.checkin_interval:
                        print(f"\n Check-in time! ({elapsed_since_checkin/60:.1f} minutes elapsed)")
                        self.do_checkin()
                        continue  # Skip the rest of this loop iteration
                        
                # Think and draw
                self.think_in_code()
                
                # Process emotions
                self.feel()
                
                # Process ambient sounds every 5 steps
                if self.hearing_enabled and self.steps_taken % 5 == 0:
                    self.hear_sounds()
                    
                # Generate autonomous goals periodically
                if self.steps_taken % 150 == 0:  # Every 150 steps
                    self.generate_autonomous_goal()
                       
                # Increment step counter
                self.steps_taken += 1
                
                # Calculate adaptive delay
                if self.turbo_mode:
                    delay = 100  # Still fast but not overwhelming
                elif self.recent_speed_override:
                    delay = self.aurora_delay
                    self.speed_override_counter += 1
                    if self.speed_override_counter > 20:
                        self.recent_speed_override = False
                else:
                    # Emotion-based speed
                    base_delay = 300  # .3 second base was 6
                    if self.current_emotion in ["energetic", "excited", "exhilarated", "electric"]:
                        delay = int(base_delay * 0.5)
                    elif self.current_emotion in ["contemplative", "peaceful", "tranquil", "zen"]:
                        delay = int(base_delay * 2)
                    else:
                        delay = base_delay
                
                # Sleep for calculated delay
                time.sleep(delay / 1000.0)
                
                # Periodic saves and cleanup
                if self.steps_taken % 50 == 0:
                    self.save_canvas_state()
                    self.memory.save_memories()
                    self.cleanup_paint_timestamps()  # Clean more frequently (silent)
                
                # Save lifetime stats and session insights every 100 steps (silent)
                if self.steps_taken % 100 == 0:
                    self.save_session_insights(verbose=False)
                # Sync to deep memory every 500 steps
                if self.steps_taken % 500 == 0 and self.steps_taken > 0:
                    self.sync_to_deep_memory() 
                    
                # Extra cleanup for long sessions (SILENT unless needed)
                if self.steps_taken % 200 == 0:
                    # Force aggressive cleanup only if actually needed
                    if len(self.paint_timestamps) > 500:
                        self.paint_timestamps = {}
                        # Only print if rest mode or every 1000 steps
                        if self.current_mode == "rest" or self.steps_taken % 1000 == 0:
                            print(f"   Memory cleanup at step {self.steps_taken}")  
                    
            except Exception as e:
                print(f"\nError in thinking loop: {e}")
                import traceback
                traceback.print_exc()
                time.sleep(1)
    
    def run(self):
        """Start Aurora with separate display and thinking loops"""
        print("\n" + "="*60)
        print(" AURORA CODE MIND - COMPLETE ")
        print("="*60)
        print(f"Canvas: {self.canvas_size}{self.canvas_size} ({self.canvas_size**2:,} pixels)")
        print(f"Internal: {self.internal_canvas_size}{self.internal_canvas_size} (4x supersampled)")
        print(f"Scale: {self.scale_factor}x")
        print(f"Mode: {'GPU' if self.use_gpu else 'CPU'}")
        print("="*60 + "\n")
        
        # Start Aurora's thinking in a separate thread
        import threading
        self.running = True
        think_thread = threading.Thread(target=self.thinking_loop, daemon=True)
        think_thread.start()
        
        # Main thread runs display at 60 FPS
        while self.running:
            # Handle pygame events
            for event in pygame.event.get():
                if event.type == pygame.QUIT:
                    self.running = False
                elif event.type == pygame.KEYDOWN:
                    if event.key == pygame.K_q:
                        self.running = False
                    elif event.key == pygame.K_s:
                        self.save_snapshot()
                    elif event.key == pygame.K_t:
                        self.toggle_turbo()
                    elif event.key == pygame.K_i:
                        # Toggle immediate feedback mode
                        self.immediate_feedback_mode = not self.immediate_feedback_mode
                        if self.immediate_feedback_mode:
                            print("\n IMMEDIATE FEEDBACK MODE: ON")
                            print("  Aurora now sees the result of each action!")
                            if self.vision_enabled:
                                self.moondream_instant_feedback = False
                                print("   Moondream will observe each action!")
                            print("  Canvas is simplified for learning")
                        else:
                            print("\n IMMEDIATE FEEDBACK MODE: OFF")
                            print("  Back to normal batch processing")
                            self.moondream_instant_feedback = False
                    elif event.key == pygame.K_ESCAPE:
                        if self.fullscreen:
                            self.running = False
                    elif event.key == pygame.K_F11:
                        if self.fullscreen:
                            self.screen = pygame.display.set_mode((1280, 720))
                            self.fullscreen = False
                        else:
                            self.screen = pygame.display.set_mode((0, 0), pygame.FULLSCREEN)
                            self.fullscreen = True
                        # Recalculate canvas rect
                        actual_width = self.screen.get_width()
                        actual_height = self.screen.get_height()
                        canvas_display_size = min(actual_width, actual_height) - 40
                        self.display_scale = canvas_display_size / self.canvas_size
                        canvas_x = (actual_width - canvas_display_size) // 2
                        canvas_y = (actual_height - canvas_display_size) // 2
                        self.canvas_rect = pygame.Rect(canvas_x, canvas_y, canvas_display_size, canvas_display_size)
                    elif event.key == pygame.K_h:
                        self.toggle_hearing()
                    elif event.key == pygame.K_c:
                        self.enter_chat_mode()
                    elif event.key == pygame.K_m:
                        # Toggle Moondream automatic analysis
                        if hasattr(self, 'moondream_auto_analysis'):
                            self.moondream_auto_analysis = not self.moondream_auto_analysis
                        else:
                            self.moondream_auto_analysis = True
                        
                        status = "ON" if self.moondream_auto_analysis else "OFF"
                        print(f"\n' MOONDREAM AUTO-ANALYSIS: {status}")
                    
                    elif event.key == pygame.K_v:
                        
                        if hasattr(self, 'vision_enabled'):
                            self.vision_enabled = not self.vision_enabled
                            status = "ON " if self.vision_enabled else "OFF"
                            print(f"\n LLAVA VISION {status}")
            
            # Always update cymatics and display at 60 FPS
            self.update_cymatics()
            self.update_display()
            self.clock.tick(100)
        
        # Cleanup
        print("\n\nSaving final state...")
        self.save_canvas_state()
        self.save_session_insights()
        self.memory.save_memories()
        
        # Clean up audio if needed
        if hasattr(self, 'audio_stream') and self.audio_stream:
            self.audio_stream.stop_stream()
            self.audio_stream.close()
        if hasattr(self, 'audio'):
            self.audio.terminate()
            
        print("Goodbye! ")
        pygame.quit()

# Usage example
if __name__ == "__main__":
    # Path to your model
    model_path = "./models/llama-2-7b-chat.Q4_K_M.gguf"
    
    # Create and run Aurora
    aurora = AuroraCodeMindComplete(
        model_path=model_path,
        use_gpu=True,
        gpu_layers=10  # Use all GPU layers
    )
    
    aurora.run()
